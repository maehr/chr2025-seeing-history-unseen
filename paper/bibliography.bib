@online{a11ychecklist,
  title = {Checklist},
  author = {Project, The A11Y},
  year = {\bibstring{nodate}},
  url = {https://www.a11yproject.com/checklist/}
}

@article{cecilia2023a,
  title = {Creating Accessible Digital Images for Vision Impaired Audiences and Researchers},
  author = {Cecilia, Rafie and Moussouri, Theano and Fraser, John},
  date = {2023},
  journaltitle = {Curator},
  volume = {66},
  number = {1},
  pages = {5--8},
  doi = {10.1111/cura.12536}
}

@article{cecilia2023b,
  title = {{{AltText}}: {{An}} Institutional Tool for Change},
  author = {Cecilia, Rafie and Moussouri, Theano and Fraser, John},
  date = {2023},
  journaltitle = {Curator},
  volume = {66},
  number = {2},
  pages = {225--231},
  doi = {10.1111/cura.12551}
}

@incollection{fickers2022,
  title = {Digital Hermeneutics: {{The}} Reflexive Turn in Digital Public History?},
  booktitle = {Handbook of Digital Public History},
  author = {Fickers, Andreas},
  editor = {Noiret, Serge and Schafer, Valérie and Zaagsma, Gerben},
  date = {2022},
  pages = {139--148},
  publisher = {De Gruyter},
  doi = {10.1515/9783110430295-012}
}

@online{gavrikov2025,
  title = {Can {{We Talk Models Into Seeing}} the {{World Differently}}?},
  author = {Gavrikov, Paul and Lukasik, Jovita and Jung, Steffen and Geirhos, Robert and Mirza, M. Jehanzeb and Keuper, Margret and Keuper, Janis},
  date = {2025-03-05},
  eprint = {2403.09193},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2403.09193},
  url = {http://arxiv.org/abs/2403.09193},
  urldate = {2025-10-27},
  abstract = {Unlike traditional vision-only models, vision language models (VLMs) offer an intuitive way to access visual content through language prompting by combining a large language model (LLM) with a vision encoder. However, both the LLM and the vision encoder come with their own set of biases, cue preferences, and shortcuts, which have been rigorously studied in uni-modal models. A timely question is how such (potentially misaligned) biases and cue preferences behave under multi-modal fusion in VLMs. As a first step towards a better understanding, we investigate a particularly well-studied vision-only bias - the texture vs. shape bias and the dominance of local over global information. As expected, we find that VLMs inherit this bias to some extent from their vision encoders. Surprisingly, the multi-modality alone proves to have important effects on the model behavior, i.e., the joint training and the language querying change the way visual cues are processed. While this direct impact of language-informed training on a model's visual perception is intriguing, it raises further questions on our ability to actively steer a model's output so that its prediction is based on particular visual cues of the user's choice. Interestingly, VLMs have an inherent tendency to recognize objects based on shape information, which is different from what a plain vision encoder would do. Further active steering towards shape-based classifications through language prompts is however limited. In contrast, active VLM steering towards texture-based decisions through simple natural language prompts is often more successful. URL: https://github.com/paulgavrikov/vlm\_shapebias},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition}
}

@inproceedings{hanley2021,
  title = {Computer Vision and Conflicting Values: {{Describing}} People with Automated Alt Text},
  booktitle = {Proceedings of the {{ACM}} Conference on {{AI}}, Ethics, and Society ({{AIES}})},
  author = {Hanley, Margot and Barocas, Solon and Levy, Karen and Azenkot, Shiri and Nissenbaum, Helen},
  date = {2021},
  eprint = {2105.12754},
  eprinttype = {arXiv}
}

@online{holmes2020,
  title = {Disability Language Style Guide},
  author = {Holmes, Katie},
  date = {2020},
  url = {https://ncdj.org/style-guide/}
}

@misc{maehr_2022,
  type = {Keynote},
  title = {Research {{Data Management}} in ({{Public}}) {{History}}},
  author = {Mähr, Moritz},
  date = {2022-06},
  location = {Istituto Svizzero di Roma},
  url = {https://doi.org/10.5281/zenodo.6637118},
  abstract = {The production of a comprehensive history of the city of Basel has been underway at the University of Basel since 2018. More than 70 authors are investigating the past 2200 years and preparing 10 volumes for print. In 2017, the Stadt.Geschichte.Basel Foundation, which is carrying out the project, made a commitment to the canton of Basel-Stadt to make the research data available to the public via an online portal. We interpret this commitment as meaning that the research data will be collected, processed and secured for later use and long-term archiving in a first step, before being made available to the general public in a second step. This raises the question - and this is a general problem in historical research - of what research data are and, following on from this, how they can be made accessible. A field report striving to think about epistemological and methodological consequences.},
  copyright = {Creative Commons Attribution 4.0 International, Open Access},
  langid = {english},
  keywords = {Digital History,Open Access,Open Source,Public History,Research Data Management,Stadt.Geschichte.Basel,Survey}
}

@misc{maehrHandbuchZurErstellung2024,
  title = {Handbuch zur Erstellung diskriminierungsfreier Metadaten für historische Quellen und Forschungsdaten: Erfahrungen aus dem geschichtswissenschaftlichen Forschungsprojekt Stadt.Geschichte.Basel},
  shorttitle = {Handbuch zur Erstellung diskriminierungsfreier Metadaten für historische Quellen und Forschungsdaten},
  author = {Mähr, Moritz and Schnegg, Noëlle},
  date = {2024-06},
  location = {Basel},
  doi = {10.5281/ZENODO.11124720},
  abstract = {This manual is a guide to creating non-discriminatory metadata for historical sources and research data, developed as part of the Stadt.Geschichte.Basel research project. It is aimed at professional historians, archivists, librarians and anyone involved in open research data in the field of history. The authors Moritz Mähr and Noëlle Schnegg guide readers through the practical aspects of creating metadata based on the FAIR principles to make research data discoverable, accessible, interoperable and reusable. Through practical instructions and illustrated case studies, the manual shows how machine-readable metadata can enrich research and teaching and influence the interpretation of historical sources. As a publicly accessible "living document", it is designed for continuous development by the community and is committed to an inclusive and non-discriminatory representation of historical content. The handbook is a fundamental resource for anyone interested in modern digital history and open research data. It is available in German.},
  archiveprefix = {Zenodo},
  copyright = {Creative Commons Attribution Share Alike 4.0 International},
  langid = {ngerman},
  organization = {Zenodo},
  keywords = {Code of Conduct,Diskriminierungsfreie Metadaten,Dublin Core,FAIR-Prinzipien,GenderOpen Schlagwortindex,Historische Quellen und Forschungsdaten,Open Research Data,Stadt.Geschichte.Basel}
}

@inproceedings{mohanbabu2024,
  title = {Context-{{Aware Image Descriptions}} for {{Web Accessibility}}},
  booktitle = {The 26th {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  author = {Mohanbabu, Ananya Gubbi and Pavel, Amy},
  date = {2024-10-27},
  eprint = {2409.03054},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {1--17},
  doi = {10.1145/3663548.3675658},
  url = {http://arxiv.org/abs/2409.03054},
  urldate = {2025-10-27},
  abstract = {Blind and low vision (BLV) internet users access images on the web via text descriptions. New vision-to-language models such as GPT-V, Gemini, and LLaVa can now provide detailed image descriptions on-demand. While prior research and guidelines state that BLV audiences' information preferences depend on the context of the image, existing tools for accessing vision-to-language models provide only context-free image descriptions by generating descriptions for the image alone without considering the surrounding webpage context. To explore how to integrate image context into image descriptions, we designed a Chrome Extension that automatically extracts webpage context to inform GPT-4V-generated image descriptions. We gained feedback from 12 BLV participants in a user study comparing typical context-free image descriptions to context-aware image descriptions. We then further evaluated our context-informed image descriptions with a technical evaluation. Our user evaluation demonstrated that BLV participants frequently prefer context-aware descriptions to context-free descriptions. BLV participants also rated context-aware descriptions significantly higher in quality, imaginability, relevance, and plausibility. All participants shared that they wanted to use context-aware descriptions in the future and highlighted the potential for use in online shopping, social media, news, and personal interest blogs.},
  keywords = {Computer Science - Human-Computer Interaction}
}

@online{reihanian2025,
  title = {A {{Review}} of {{Generative AI}} in {{Computer Science Education}}: {{Challenges}} and {{Opportunities}} in {{Accuracy}}, {{Authenticity}}, and {{Assessment}}},
  shorttitle = {A {{Review}} of {{Generative AI}} in {{Computer Science Education}}},
  author = {Reihanian, Iman and Hou, Yunfei and Chen, Yu and Zheng, Yifei},
  date = {2025-06-17},
  eprint = {2507.11543},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2507.11543},
  url = {http://arxiv.org/abs/2507.11543},
  urldate = {2025-10-27},
  abstract = {This paper surveys the use of Generative AI tools, such as ChatGPT and Claude, in computer science education, focusing on key aspects of accuracy, authenticity, and assessment. Through a literature review, we highlight both the challenges and opportunities these AI tools present. While Generative AI improves efficiency and supports creative student work, it raises concerns such as AI hallucinations, error propagation, bias, and blurred lines between AI-assisted and student-authored content. Human oversight is crucial for addressing these concerns. Existing literature recommends adopting hybrid assessment models that combine AI with human evaluation, developing bias detection frameworks, and promoting AI literacy for both students and educators. Our findings suggest that the successful integration of AI requires a balanced approach, considering ethical, pedagogical, and technical factors. Future research may explore enhancing AI accuracy, preserving academic integrity, and developing adaptive models that balance creativity with precision.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society}
}

@article{shen2024,
  title = {{{AltGen}}: {{AI-driven}} Alt Text Generation for Enhancing {{EPUB}} Accessibility},
  author = {Shen, Yixian and others},
  date = {2024},
  journaltitle = {arXiv preprint},
  eprint = {2501.00113},
  eprinttype = {arXiv}
}

@book{shew2023,
  title = {Against Technoableism: {{Rethinking}} Who Needs Improvement},
  author = {Shew, Ashley},
  date = {2023},
  publisher = {W. W. Norton},
  location = {New York}
}

@online{wcag2023,
  title = {Web Content Accessibility Guidelines ({{WCAG}}) 2.2},
  author = {{World Wide Web Consortium}},
  date = {2023},
  url = {https://www.w3.org/TR/WCAG22/}
}
