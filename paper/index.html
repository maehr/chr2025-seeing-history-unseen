<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head><meta charset="utf-8"><style>:where(img[jampack-sized]){max-width:100%;height:auto}</style>


<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=yes">

<meta name="author" content="Moritz Mähr">
<meta name="author" content="Moritz Twente">
<meta name="keywords" content="alt-text, vision-language models, accessibility, WCAG 2.2, digital heritage collections, historical accuracy, human-in-the-loop, ethical implications, metadata, disability justice">

<title>Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections – CHR 2025 - Seeing History Unseen</title>
<style>code{white-space:pre-wrap}span.smallcaps{font-variant:small-caps}div.columns{gap:min(4vw,1.5em);display:flex}div.column{flex:auto;overflow-x:auto}div.hanging-indent{text-indent:-1.5em;margin-left:1.5em}ul.task-list{list-style:none}ul.task-list li input[type=checkbox]{vertical-align:middle;width:.8em;margin:0 .8em .2em -1em}html{-webkit-text-size-adjust:100%}pre>code.sourceCode{white-space:pre;position:relative}pre>code.sourceCode>span{line-height:1.25;display:inline-block}pre>code.sourceCode>span:empty{height:1.2em}.sourceCode{overflow:visible}code.sourceCode>span{color:inherit;-webkit-text-decoration:inherit;text-decoration:inherit}div.sourceCode{margin:1em 0}pre.sourceCode{margin:0}@media screen{div.sourceCode{overflow:auto}}@media print{pre>code.sourceCode{white-space:pre-wrap}pre>code.sourceCode>span{text-indent:-5em;padding-left:5em}}pre.numberSource code{counter-reset:source-line 0}pre.numberSource code>span{counter-increment:source-line;position:relative;left:-4em}pre.numberSource code>span>a:first-child:before{content:counter(source-line);text-align:right;vertical-align:baseline;-webkit-touch-callout:none;-webkit-user-select:none;user-select:none;-khtml-user-select:none;border:none;width:4em;padding:0 4px;display:inline-block;position:relative;left:-1em}pre.numberSource{margin-left:3em;padding-left:4px}@media screen{pre>code.sourceCode>span>a:first-child:before{text-decoration:underline}}div.csl-entry{clear:both;margin-bottom:0}.hanging-indent div.csl-entry{text-indent:-2em;margin-left:2em}div.csl-left-margin{float:left;min-width:2em}div.csl-right-inline{margin-left:2em;padding-left:1em}div.csl-indent{margin-left:2em}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../abstract/index.html" rel="next">
<link href="../presentation/index.html" rel="prev">
<link href="../android-chrome-512x512.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-b758ccaa5987ceb1b75504551e579abf.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-b3b235ae6ba71d6e5c2a90c00144237d.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-b3b235ae6ba71d6e5c2a90c00144237d.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-b3b235ae6ba71d6e5c2a90c00144237d.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": true,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">const typesetMath=s=>{if(window.MathJax)window.MathJax.typeset([s]);else if(window.katex)for(var r=s.getElementsByClassName("math"),d=[],a=0;a<r.length;a++){var o=r[a].firstChild;r[a].tagName=="SPAN"&&o&&o.data&&window.katex.render(o.data,r[a],{displayMode:r[a].classList.contains("display"),throwOnError:!1,macros:d,fleqn:!1})}};window.Quarto={typesetMath};
</script>

<link rel="stylesheet" href="../styles.css">
<meta property="og:title" content="Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections – CHR 2025 - Seeing History Unseen">
<meta property="og:description" content="Digitized heritage collections remain partially inaccessible because images often lack descriptive alternative text (alt-text). We evaluate whether contemporary Vision-Language Models (VLMs) can assist in producing WCAG-compliant alt-text for heterogeneous historical materials. Using a 100-item dataset curated from the Stadt.Geschichte.Basel Open Research Data Platform—covering photographs, maps, drawings, objects, diagrams, and print ephemera across multiple eras—we generate candidate descriptions with four VLMs (Google Gemini 2.5 Flash Lite, Meta Llama 4 Maverick, OpenAI GPT-4o mini, Qwen 3 VL 8B Instruct). Our pipeline fixes WCAG and output constraints in the system prompt and injects concise, collection-specific metadata at the user turn to mitigate “lost-in-the-middle” effects. Feasibility benchmarks on a 20-item subset show 100 % coverage, latencies of ~2–4 s per item, and sub-cent costs per description. A rater study with 21 humanities scholars ranks per-image model outputs; Friedman and Wilcoxon tests reveal no statistically significant performance differences, while qualitative audits identify recurring errors: factual misrecognition, selective omission, and uncritical reproduction of harmful historical terminology. We argue that VLMs are operationally viable but epistemically fragile in heritage contexts. Effective adoption requires editorial policies, sensitivity filtering, and targeted human-in-the-loop review, especially for sensitive content and complex figures. The study contributes a transparent, reproducible workflow, a small but representative evaluation set, and an initial cost–quality baseline to inform GLAM institutions considering AI-assisted accessibility at scale.">
<meta property="og:image" content="https://maehr.github.io/chr2025-seeing-history-unseen/paper/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==">
<meta property="og:site_name" content="CHR 2025 - Seeing History Unseen">
<meta name="twitter:title" content="Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections – CHR 2025 - Seeing History Unseen">
<meta name="twitter:description" content="Digitized heritage collections remain partially inaccessible because images often lack descriptive alternative text (alt-text). We evaluate whether contemporary Vision-Language Models (VLMs) can assist in producing WCAG-compliant alt-text for heterogeneous historical materials. Using a 100-item dataset curated from the Stadt.Geschichte.Basel Open Research Data Platform—covering photographs, maps, drawings, objects, diagrams, and print ephemera across multiple eras—we generate candidate descriptions with four VLMs (Google Gemini 2.5 Flash Lite, Meta Llama 4 Maverick, OpenAI GPT-4o mini, Qwen 3 VL 8B Instruct). Our pipeline fixes WCAG and output constraints in the system prompt and injects concise, collection-specific metadata at the user turn to mitigate “lost-in-the-middle” effects. Feasibility benchmarks on a 20-item subset show 100 % coverage, latencies of ~2–4 s per item, and sub-cent costs per description. A rater study with 21 humanities scholars ranks per-image model outputs; Friedman and Wilcoxon tests reveal no statistically significant performance differences, while qualitative audits identify recurring errors: factual misrecognition, selective omission, and uncritical reproduction of harmful historical terminology. We argue that VLMs are operationally viable but epistemically fragile in heritage contexts. Effective adoption requires editorial policies, sensitivity filtering, and targeted human-in-the-loop review, especially for sensitive content and complex figures. The study contributes a transparent, reproducible workflow, a small but representative evaluation set, and an initial cost–quality baseline to inform GLAM institutions considering AI-assisted accessibility at scale.">
<meta name="twitter:image" content="https://maehr.github.io/chr2025-seeing-history-unseen/paper/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="floating nav-fixed nav-sidebar quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="fixed-top headroom">
    <nav class="navbar navbar-expand-lg" data-bs-theme="dark">
      <div class="container-fluid navbar-container">
      <div class="mx-auto navbar-brand-container">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../android-chrome-512x512.webp" alt="CHR 2025" class="navbar-logo light-content" fetchpriority="high" decoding="async" width="512" height="512" jampack-sized="true">
    <img src="../android-chrome-512x512.webp" alt="CHR 2025" class="navbar-logo dark-content" fetchpriority="high" decoding="async" width="512" height="512" jampack-sized="true">
    </a>
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">CHR 2025 - Seeing History Unseen</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="window.quartoToggleHeadroom&&window.quartoToggleHeadroom();
">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="me-auto navbar-nav navbar-nav-scroll">
  <li class="nav-item">
    <a class="nav-link active" href="../index.html" aria-current="page"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../paper/index.html"> 
<span class="menu-text">Paper</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../abstract/index.html"> 
<span class="menu-text">Abstract</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../survey/index.html"> 
<span class="menu-text">Survey</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../presentation/index.html"> 
<span class="menu-text">Presentation</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
    <a href="https://github.com/maehr/chr2025-seeing-history-unseen/" title="" class="px-1 quarto-navigation-tool" aria-label=""><i class="bi bi-github"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="btn quarto-btn-toggle" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="window.quartoToggleHeadroom&&window.quartoToggleHeadroom();
">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../presentation/index.html">Research</a></li><li class="breadcrumb-item"><a href="../paper/index.html">Paper</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="window.quartoToggleHeadroom&&window.quartoToggleHeadroom();
">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="page-columns page-layout-article page-navbar page-rows-contents quarto-container">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="collapse collapse-horizontal floating overflow-auto quarto-sidebar-collapse-item sidebar sidebar-navigation">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Research</span></a>
          <a class="text-start sidebar-item-toggle" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse depth1 list-unstyled show sidebar-section">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../presentation/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Presentation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../paper/index.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Paper</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../abstract/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Abstract</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Data &amp; Survey</span></a>
          <a class="text-start sidebar-item-toggle" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse depth1 list-unstyled show sidebar-section">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../survey/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Survey</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Run Artefacts</span></a>
          <a class="text-start sidebar-item-toggle" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse depth1 list-unstyled show sidebar-section">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../runs/20251021_233530/manifest.json" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2025-10-21 Manifest</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../runs/20251021_233530/alt_text_runs_20251021_233933_wide.csv" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2025-10-21 Wide CSV</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../runs/20251021_233530/alt_text_runs_20251021_233933_long.csv" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2025-10-21 Long CSV</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../runs/20251021_233530/alt_text_runs_20251021_233933_prompts.csv" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2025-10-21 Prompts</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Project</span></a>
          <a class="text-start sidebar-item-toggle" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse depth1 list-unstyled show sidebar-section">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../CHANGELOG.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Changelog</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../CONTRIBUTING.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Contributing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../CODE_OF_CONDUCT.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Code of Conduct</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../SECURITY.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Security</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Licenses</span></a>
          <a class="text-start sidebar-item-toggle" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse depth1 list-unstyled show sidebar-section">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../LICENSE-CCBY.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data (CC BY 4.0)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../LICENSE-AGPL.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Code (AGPL 3.0)</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="margin-sidebar sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#sec-introduction" id="toc-sec-introduction" class="nav-link active" data-scroll-target="#sec-introduction"><span class="header-section-number">1</span> Introduction</a>
  <ul class="collapse">
  <li><a href="#research-questions" id="toc-research-questions" class="nav-link" data-scroll-target="#research-questions"><span class="header-section-number">1.1</span> Research questions</a></li>
  </ul></li>
  <li><a href="#sec-data-methods" id="toc-sec-data-methods" class="nav-link" data-scroll-target="#sec-data-methods"><span class="header-section-number">2</span> Data &amp; Methodology</a>
  <ul class="collapse">
  <li><a href="#dataset-for-alt-text-generation-and-evaluation" id="toc-dataset-for-alt-text-generation-and-evaluation" class="nav-link" data-scroll-target="#dataset-for-alt-text-generation-and-evaluation"><span class="header-section-number">2.1</span> Dataset for Alt Text Generation and Evaluation</a></li>
  <li><a href="#dataset-limitations" id="toc-dataset-limitations" class="nav-link" data-scroll-target="#dataset-limitations"><span class="header-section-number">2.2</span> Dataset Limitations</a></li>
  <li><a href="#model-selection" id="toc-model-selection" class="nav-link" data-scroll-target="#model-selection"><span class="header-section-number">2.3</span> Model Selection</a></li>
  <li><a href="#prompt-engineering" id="toc-prompt-engineering" class="nav-link" data-scroll-target="#prompt-engineering"><span class="header-section-number">2.4</span> Prompt engineering</a></li>
  <li><a href="#alt-text-generation-and-post-processing" id="toc-alt-text-generation-and-post-processing" class="nav-link" data-scroll-target="#alt-text-generation-and-post-processing"><span class="header-section-number">2.5</span> Alt Text Generation and Post-processing</a></li>
  <li><a href="#survey" id="toc-survey" class="nav-link" data-scroll-target="#survey"><span class="header-section-number">2.6</span> Survey</a></li>
  <li><a href="#close-reading" id="toc-close-reading" class="nav-link" data-scroll-target="#close-reading"><span class="header-section-number">2.7</span> Close reading</a></li>
  </ul></li>
  <li><a href="#sec-results" id="toc-sec-results" class="nav-link" data-scroll-target="#sec-results"><span class="header-section-number">3</span> Results and Analysis</a>
  <ul class="collapse">
  <li><a href="#rq1-feasibility-coverage-throughput-and-unit-cost" id="toc-rq1-feasibility-coverage-throughput-and-unit-cost" class="nav-link" data-scroll-target="#rq1-feasibility-coverage-throughput-and-unit-cost"><span class="header-section-number">3.1</span> RQ1 Feasibility: Coverage, Throughput, and Unit Cost</a>
  <ul class="collapse">
  <li><a href="#coverage-and-reliability" id="toc-coverage-and-reliability" class="nav-link" data-scroll-target="#coverage-and-reliability"><span class="header-section-number">3.1.1</span> Coverage and reliability</a></li>
  <li><a href="#throughput-and-latency" id="toc-throughput-and-latency" class="nav-link" data-scroll-target="#throughput-and-latency"><span class="header-section-number">3.1.2</span> Throughput and latency</a></li>
  <li><a href="#cost-efficiency" id="toc-cost-efficiency" class="nav-link" data-scroll-target="#cost-efficiency"><span class="header-section-number">3.1.3</span> Cost efficiency</a></li>
  <li><a href="#summary-of-rq1" id="toc-summary-of-rq1" class="nav-link" data-scroll-target="#summary-of-rq1"><span class="header-section-number">3.1.4</span> Summary of RQ1</a></li>
  </ul></li>
  <li><a href="#rq2-relative-quality-expert-ranking-and-qualitative-assessment" id="toc-rq2-relative-quality-expert-ranking-and-qualitative-assessment" class="nav-link" data-scroll-target="#rq2-relative-quality-expert-ranking-and-qualitative-assessment"><span class="header-section-number">3.2</span> RQ2 Relative Quality: Expert Ranking and Qualitative Assessment</a>
  <ul class="collapse">
  <li><a href="#quantitative-ranking-analysis" id="toc-quantitative-ranking-analysis" class="nav-link" data-scroll-target="#quantitative-ranking-analysis"><span class="header-section-number">3.2.1</span> Quantitative ranking analysis</a></li>
  <li><a href="#descriptive-patterns" id="toc-descriptive-patterns" class="nav-link" data-scroll-target="#descriptive-patterns"><span class="header-section-number">3.2.2</span> Descriptive patterns</a></li>
  <li><a href="#sec-2-close-reading" id="toc-sec-2-close-reading" class="nav-link" data-scroll-target="#sec-2-close-reading"><span class="header-section-number">3.2.3</span> Qualitative evaluation of top-rated outputs</a></li>
  <li><a href="#interpretation" id="toc-interpretation" class="nav-link" data-scroll-target="#interpretation"><span class="header-section-number">3.2.4</span> Interpretation</a></li>
  </ul></li>
  <li><a href="#synthesis" id="toc-synthesis" class="nav-link" data-scroll-target="#synthesis"><span class="header-section-number">3.3</span> Synthesis</a></li>
  </ul></li>
  <li><a href="#sec-discussion" id="toc-sec-discussion" class="nav-link" data-scroll-target="#sec-discussion"><span class="header-section-number">4</span> Discussion</a></li>
  <li><a href="#sec-future-work" id="toc-sec-future-work" class="nav-link" data-scroll-target="#sec-future-work"><span class="header-section-number">5</span> Future Work</a></li>
  <li><a href="#acknowledgements" id="toc-acknowledgements" class="nav-link" data-scroll-target="#acknowledgements"><span class="header-section-number">6</span> Acknowledgements</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">7</span> References</a></li>
  
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/maehr/chr2025-seeing-history-unseen/edit/main/paper/index.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/maehr/chr2025-seeing-history-unseen/issues/new/choose" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div><div class="quarto-other-links"><h2>Other Links</h2><ul><li><a href="paper.pdf"><i class="bi bi-filetype-pdf"></i>Paper</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="default quarto-title-block"><nav class="quarto-page-breadcrumbs d-lg-block d-none quarto-title-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../presentation/index.html">Research</a></li><li class="breadcrumb-item"><a href="../paper/index.html">Paper</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections</h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Authors</div>
  <div class="quarto-title-meta-heading">Affiliations</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Moritz Mähr <a href="mailto:moritz.maehr@gmail.com" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0000-0002-1367-1618" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==" alt=""></a></p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Stadt.Geschichte.Basel, University of Basel, Switzerland
          </p>
        <p class="affiliation">
            Digital Humanities, University of Bern, Switzerland
          </p>
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Moritz Twente <a href="mailto:mtwente@protonmail.com" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0009-0005-7187-9774" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==" alt=""></a></p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Stadt.Geschichte.Basel, University of Basel, Switzerland
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
    <div>
    <div class="quarto-title-meta-heading">Doi</div>
    <div class="quarto-title-meta-contents">
      <p class="doi">
        <a href="https://doi.org/10.63744/njQVYcLndSPE">10.63744/njQVYcLndSPE</a>
      </p>
    </div>
  </div>
  </div>
  
<div>
  <div class="abstract">
    <div class="block-title">Abstract</div>
    <p>Digitized heritage collections remain partially inaccessible because images often lack descriptive alternative text (alt-text). We evaluate whether contemporary Vision-Language Models (VLMs) can assist in producing WCAG-compliant alt-text for heterogeneous historical materials. Using a 100-item dataset curated from the Stadt.Geschichte.Basel Open Research Data Platform—covering photographs, maps, drawings, objects, diagrams, and print ephemera across multiple eras—we generate candidate descriptions with four VLMs (Google Gemini 2.5 Flash Lite, Meta Llama 4 Maverick, OpenAI GPT-4o mini, Qwen 3 VL 8B Instruct). Our pipeline fixes WCAG and output constraints in the system prompt and injects concise, collection-specific metadata at the user turn to mitigate “lost-in-the-middle” effects. Feasibility benchmarks on a 20-item subset show 100 % coverage, latencies of ~2–4 s per item, and sub-cent costs per description. A rater study with 21 humanities scholars ranks per-image model outputs; Friedman and Wilcoxon tests reveal no statistically significant performance differences, while qualitative audits identify recurring errors: factual misrecognition, selective omission, and uncritical reproduction of harmful historical terminology. We argue that VLMs are operationally viable but epistemically fragile in heritage contexts. Effective adoption requires editorial policies, sensitivity filtering, and targeted human-in-the-loop review, especially for sensitive content and complex figures. The study contributes a transparent, reproducible workflow, a small but representative evaluation set, and an initial cost–quality baseline to inform GLAM institutions considering AI-assisted accessibility at scale.</p>
  </div>
</div>

<div>
  <div class="keywords">
    <div class="block-title">Keywords</div>
    <p>alt-text, vision-language models, accessibility, WCAG 2.2, digital heritage collections, historical accuracy, human-in-the-loop, ethical implications, metadata, disability justice</p>
  </div>
</div>

</header>


<section id="sec-introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>Digital archives promised to democratize access to cultural heritage, yet a significant portion of visual historical content remains inaccessible to people who are blind or have low vision. Many digitized photographs, maps, manuscripts, and other images lack descriptive alternative text (alt-text), creating an epistemic barrier to the past. This perpetuates an asymmetry in sensory access to history, where sighted people hold privileged insight into visual sources while non-sighted audiences encounter barriers to engagement. Making images legible through text is more than a technical fix—it is a matter of historical justice and inclusivity in digital humanities. Even beyond blind and low-vision users, rich image descriptions can aid others, such as neurodivergent readers who benefit from explicit detail that sighted users might glean implicitly <span class="citation" data-cites="cecilia2023b">(<a href="#ref-cecilia2023b" role="doc-biblioref">Cecilia, Moussouri, and Fraser 2023a</a>)</span>.</p>
<p>Alt-text itself is not new: the HTML <code>alt</code> attribute dates back to the 1990s to support accessibility. However, providing high-quality image descriptions has often been a secondary priority in scholarly communication <span class="citation" data-cites="cecilia2023a">(<a href="#ref-cecilia2023a" role="doc-biblioref">Cecilia, Moussouri, and Fraser 2023b</a>)</span>. Crafting alt-text is labor-intensive and typically left to authors or curators as a final step, if done at all. The burden often falls on sighted domain experts (not accessibility experts) to determine what information <em>is</em> or <em>is not</em> included in an image’s description. Human-generated descriptions are valued for capturing contextual meaning and can greatly enhance the accessibility, searchability, and archivability of digital scholarship. Yet in practice, many projects—especially smaller public history initiatives—lack the resources to implement accessibility from the start. The result is that visual evidence remains “unseen” by those who rely on assistive technologies.</p>
<p>Recent advances in multimodal AI offer a potential remedy. Vision-Language Models (VLMs) such as OpenAI’s GPT-4o mini, Google’s Gemini 2.5 Flash Lite, and open-weight systems like Meta’s Llama 4 Maverick or Qwen’s Qwen3 VL 8B Instruct now claim near-human performance in image description tasks. These models can ingest an image and generate a caption or description, essentially simulating the interpretive act of a human describer. If these models could produce alt-text that is both high-quality and historically informed as well as aligned with the Web Content Accessibility Guidelines (WCAG 2.2) <span class="citation" data-cites="wcag2023">(<a href="#ref-wcag2023" role="doc-biblioref">World Wide Web Consortium 2023</a>)</span>, this would dramatically reduce the human effort required to remediate large collections. Heritage institutions could then scale up accessibility by generating alt-text for thousands of images, because the costs of machine captioning are negligible in comparison to a human expert. Consequently, the “readership” of digital archives would expand to include those who were previously excluded.</p>
<p>However, adopting automated captioning in a heritage context raises critical questions about truth, evidence, and authenticity. Delegating descriptive labor to machines is not a neutral technical fix; it is an act imbued with values and biases <span class="citation" data-cites="bowker1999">(<a href="#ref-bowker1999" role="doc-biblioref">Bowker and Star 1999</a>)</span>. Deciding what details to include in an image’s description is technically difficult and ethically fraught, especially for historical images depicting people or sensitive cultural content. Vision models trained on general web images may uncritically adopt source terminology, inject anachronistic biases (e.g., misidentifying a 1920s street scene as “Victorian”), reinforce curatorial blind spots, or omit crucial context that a human historian would provide. There is also the danger of <em>techno-ableism</em> <span class="citation" data-cites="shew2023">(<a href="#ref-shew2023" role="doc-biblioref">Shew 2023</a>)</span>, where the needs of people who are blind are superficially addressed by technology without truly empowering them or respecting their perspectives. Uncritical use of AI could inadvertently recenter the sighted, algorithmic point of view rather than the lived experience of those using the alt-text.</p>
<p>In this work, we argue that AI-generated alt-text for historical collections is a pivotal test case for the entanglement of AI innovation, archival practice, and disability justice. But can a machine “see” history as we do? If a model can convincingly describe a photograph from 100 years ago, how does that change the way we verify and trust such descriptions? Embracing this kind of “machine vision” in historical scholarship may require new protocols akin to earlier paradigm shifts (for example, the move from handwritten catalog cards to MARC records, or from microfilm to digital scans). Just as those changes demanded critical awareness of how tools shape historical discovery, the use of AI-generated descriptions demands a new hermeneutic of suspicion. We must learn to critically read machine-generated metadata, much as we read any human-produced finding aid or annotation <span class="citation" data-cites="fickers2022">(<a href="#ref-fickers2022" role="doc-biblioref">Fickers 2022</a>)</span>.<br>
The central purpose of our study is to assess whether and how current AI models can serve as <em>accessibility assistants</em> in a digital history workflow, and to critically examine the conditions and implications of their responsible use.<br>
Our approach is interdisciplinary, blending computational experimentation with qualitative, historiographically informed analysis. The research design comprises the following steps:</p>
<ol type="1">
<li><strong>Data compilation:</strong> We compile a small yet balanced dataset consisting of historical sources and research data.</li>
<li><strong>Model selection and prompt development:</strong> We conduct <em>WCAG</em>-aligned prompt engineering and model selection in an iterative and exploratory manner.</li>
<li><strong>Generation and data collection:</strong> Once an optimal configuration of prompts and models has been identified, we generate candidate alternative texts (alt-text) and collect quantitative data on coverage, throughput, and unit cost.</li>
<li><strong>Expert evaluation:</strong> A group of 21 domain experts—humanities scholars with relevant disciplinary expertise—evaluate and rank the AI-generated alt-text.</li>
<li><strong>Expert review:</strong> The authors qualitatively assess a selection of the highest-ranked alt-text for factual accuracy, contextual adequacy, and bias reproductions.</li>
<li><strong>Analysis:</strong> We perform both statistical and qualitative analyses of the data obtained in steps 3–5.</li>
</ol>
<p>By doing so, we aim to illuminate both the opportunities and the pitfalls of integrating AI into inclusive humanities scholarship.</p>
<section id="research-questions" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="research-questions"><span class="header-section-number">1.1</span> Research questions</h2>
<p>To guide this inquiry, we pose the following research questions:</p>
<ul>
<li><strong>RQ1 Feasibility</strong>: What <strong>coverage, throughput, and unit cost</strong> can current VLMs achieve for WCAG-aligned alt-text on a heterogeneous heritage corpus, and where do they fail?</li>
<li><strong>RQ2 Relative quality</strong>: How do experts <strong>rank</strong> model outputs? What <strong>error patterns</strong> recur?</li>
</ul>
<p>By answering these questions, our work helps to establish an empirical baseline for <em>AI-assisted accessibility in the humanities</em>. It also offers a reflective critique, examining AI outputs as objects of study in their own right. In the following sections, we outline our data and methodology (<a href="#sec-data-methods" class="quarto-xref">Section&nbsp;2</a>), present initial observations from our experiments (<a href="#sec-results" class="quarto-xref">Section&nbsp;3</a>), and discuss implications for digital humanities practice (<a href="#sec-discussion" class="quarto-xref">Section&nbsp;4</a>), before concluding with planned next steps (<a href="#sec-future-work" class="quarto-xref">Section&nbsp;5</a>).</p>
</section>
</section>
<section id="sec-data-methods" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Data &amp; Methodology</h1>
<p>To ground our evaluation in a real-world scenario, we use data from Stadt.Geschichte.Basel, a large-scale historical research project tracing the history of Basel from 50’000 BCE to the present day. Research data is FAIRly available on the project’s Open Research Data Platform with metadata in a Dublin Core schema created by our Team for Research Data Management Team in a comprehensive annotation workflow, following guidelines set out in our handbook for the creation of non-discriminatory metadata <span class="citation" data-cites="maehrHandbuchZurErstellung2024">(<a href="#ref-maehrHandbuchZurErstellung2024" role="doc-biblioref">Mähr and Schnegg 2024</a>)</span>.</p>
<p>Crucially, alt-text has been missing in our data model until now, rendering this collection an ideal testing ground for our study. The diversity of the corpus poses a significant challenge to automated captioning: many figures are visually and historically complex, requiring domain knowledge to describe properly. This data thus allows us to investigate whether AI captioners can handle the ‘long tail’ of content found in historical archives, beyond the everyday photographs on which many models are trained <span class="citation" data-cites="cetinic2021">(<a href="#ref-cetinic2021" role="doc-biblioref">Cetinic 2021</a>)</span>.</p>
<section id="dataset-for-alt-text-generation-and-evaluation" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="dataset-for-alt-text-generation-and-evaluation"><span class="header-section-number">2.1</span> Dataset for Alt Text Generation and Evaluation</h2>
<p>For our survey, we compiled a dataset designed to represent both the heterogeneity of media types and the timeframe covered by the Stadt.Geschichte.Basel project. The project collection, published on our Open Research Data Platform, features more than 1700 media objects including metadata as of October 2025. From this corpus, we created a dataset to use for alt-text generation trials. This dataset comprises a hundred items and is released with this paper to be used for benchmarking purposes. Additionally, we created a subset of 20 items to make it more feasible to evaluate alt-text in an expert survey. For both sets, items were selected to maintain representativeness across the same dimensions while being manageable for expert reviewers to assess within a reasonable time frame. (See <a href="#sec-appendix-data" class="quarto-xref">Section&nbsp;8.1</a> for a more detailed description of the dataset).</p>
<p>All items were categorized into ten distinct media types (e.g.&nbsp;paintings, maps, scans of newspapers etc., see <a href="#sec-appendix-data" class="quarto-xref">Section&nbsp;8.1</a>), allowing us to ensure a balanced distribution of content. Data types primarily comprise images and figures, maps and geodata, tables and statistics, and bibliographic references <span class="citation" data-cites="maehr_2022">(<a href="#ref-maehr_2022" role="doc-biblioref">Mähr 2022</a>)</span>: Heterogeneous digitized items including historical photographs, reproductions of artifacts, city maps and architectural plans, handwritten letters and manuscripts, statistical charts, and printed ephemera (e.g., newspaper clippings, posters). We made sure to include items with complex visual structures (items that need additional information to convey their meaning, e.g., a legend for maps or diagrams), items with visible text in different languages (e.g., scans of newspapers or posters) as well as items with potentially sensitive content (e.g., content with derogatory and/or racist terminology).</p>
<p>To prompt the models as described below, we used JPG files at a standardized size of 800×800 pixels – the same resolution employed for human viewers on our online platform – and their corresponding metadata in JSON format.</p>
</section>
<section id="dataset-limitations" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="dataset-limitations"><span class="header-section-number">2.2</span> Dataset Limitations</h2>
<p>The number of eligible items is reduced by excluding items that are only available with placeholder images on our platform due to copyright restrictions. Additionally, due to the typesetting workflow during the production of the printed volumes, some collection items had to be split up into different files – maps and charts where the legend is provided in a second image file, separate from the main figure. This pertains to 19 out of 100 items in our data set, respectively four out of 20 items in the survey. Connections between these segmented files are made explicit in our metadata, but the models only receive one image file as input at a time, leading to some loss of information that would be visually available to a human reader. This could result in a lower description quality.</p>
</section>
<section id="model-selection" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="model-selection"><span class="header-section-number">2.3</span> Model Selection</h2>
<p>We selected <strong>four multimodal vision-language models (VLMs)</strong> representing a balance of <strong>open-weight</strong> and <strong>proprietary</strong> systems with <strong>comparable cost and capability</strong>:<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<table class="caption-top table">
<caption>Models used for evaluation. Data reported by <a href="https://openrouter.ai/">OpenRouter</a> (27.10.2025).</caption>
<thead>
<tr class="header">
<th style="text-align:left">Model</th>
<th style="text-align:left">Developer</th>
<th style="text-align:left">Openness</th>
<th style="text-align:right">Context</th>
<th style="text-align:right">Latency (s)</th>
<th style="text-align:right">Input $/M</th>
<th style="text-align:right">Output $/M</th>
<th style="text-align:left">Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align:left"><strong>Gemini 2.5 Flash Lite</strong></td>
<td style="text-align:left">Google</td>
<td style="text-align:left">Proprietary</td>
<td style="text-align:right">1.05M</td>
<td style="text-align:right">0.42</td>
<td style="text-align:right">0.10</td>
<td style="text-align:right">0.40</td>
<td style="text-align:left">Fast, low-cost; optimized for captioning.</td>
</tr>
<tr class="even">
<td style="text-align:left"><strong>Llama 4 Maverick</strong></td>
<td style="text-align:left">Meta</td>
<td style="text-align:left">Open weights</td>
<td style="text-align:right">1.05M</td>
<td style="text-align:right">0.56</td>
<td style="text-align:right">0.15</td>
<td style="text-align:right">0.60</td>
<td style="text-align:left">Multilingual, multimodal reasoning.</td>
</tr>
<tr class="odd">
<td style="text-align:left"><strong>GPT-4o mini</strong></td>
<td style="text-align:left">OpenAI</td>
<td style="text-align:left">Proprietary</td>
<td style="text-align:right">128K</td>
<td style="text-align:right">0.58</td>
<td style="text-align:right">0.15</td>
<td style="text-align:right">0.60</td>
<td style="text-align:left">Compact GPT-4o variant; strong factual grounding.</td>
</tr>
<tr class="even">
<td style="text-align:left"><strong>Qwen3 VL 8B Instruct</strong></td>
<td style="text-align:left">Alibaba</td>
<td style="text-align:left">Open-weight</td>
<td style="text-align:right">131K</td>
<td style="text-align:right">1.29</td>
<td style="text-align:right">0.08</td>
<td style="text-align:right">0.50</td>
<td style="text-align:left">Robust open baseline with OCR features.</td>
</tr>
</tbody>
</table>
<p>Selection criteria:</p>
<ul>
<li><strong>Openness &amp; diversity</strong> – two proprietary (OpenAI, Google) and two open-weight (Meta, Qwen) models.</li>
<li><strong>Cost-capability parity</strong> – all models priced between $0.08–$0.15/M input and $0.40–$0.60/M output tokens, with ≥100K context windows.</li>
<li><strong>Multilingual &amp; visual competence</strong> – explicit support for German and image understanding.</li>
</ul>
<p>The aim was to cover diverse architectures and governance regimes while maintaining fairness in performance evaluation.</p>
</section>
<section id="prompt-engineering" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="prompt-engineering"><span class="header-section-number">2.4</span> Prompt engineering</h2>
<p>We systematically varied prompt roles and placement, comparing instruction blocks in the system prompt versus the user prompt, and front-loading versus trailing constraints. We used the same user and system prompts for all models in zero-shot mode. Following evidence that models privilege information in short and well structured prompts, we fixed normative requirements (WCAG 2.2 aligned, de-CH style, length limits, handling of decorative/functional/complex images) in the system prompt and kept the user prompt minimal and image-bound to reduce “lost-in-the-middle” effects <span class="citation" data-cites="liu2023">(<a href="#ref-liu2023" role="doc-biblioref">Liu et al. 2023</a>)</span>. The user prompt injected collection-specific metadata—title, description, EDTF date, era, creator/publisher/source—and a concise description of the purpose of the alt text, then the image URL. Adding this structured context markedly improved specificity, reduced refusals, and lowered hallucinations, consistent with retrieval-style findings that supplying external, task-relevant evidence boosts generation quality and faithfulness. Recent work confirms that vision–language models can serve such accessibility roles when embedded in context-rich pipelines.<br>
In particular, user studies with blind and low-vision participants demonstrate that <strong>context-aware image descriptions</strong>—those combining visual and webpage metadata—are preferred and rated higher for quality, imaginability, and plausibility than context-free baselines <span class="citation" data-cites="mohanbabu2024">(<a href="#ref-mohanbabu2024" role="doc-biblioref">Mohanbabu and Pavel 2024</a>)</span>.<br>
This supports our design choice to inject structured collection metadata into the prompt..<br>
These results are consistent with findings that <strong>prompt structure and multimodal fusion</strong> can systematically shift which visual cues VLMs rely on <span class="citation" data-cites="gavrikov2025">(<a href="#ref-gavrikov2025" role="doc-biblioref">Gavrikov et al. 2025</a>)</span>.<br>
By anchoring metadata before the image input, we effectively steer the model toward shape- and context-based reasoning rather than shallow texture correlations—an effect analogous to prompt-based cue steering observed in vision-language bias studies.</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_prompt(media: MediaObject) <span class="op">-&gt;</span> <span class="bu">str</span>:</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="ss">f"""Titel: </span><span class="sc">{</span>media<span class="sc">.</span>title <span class="kw">or</span> <span class="st">"Kein Titel"</span><span class="sc">}</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="ss">Beschreibung: </span><span class="sc">{</span>media<span class="sc">.</span>description <span class="kw">or</span> <span class="st">"Keine Beschreibung"</span><span class="sc">}</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="ss">Ersteller: </span><span class="sc">{</span>media<span class="sc">.</span>creator <span class="kw">or</span> <span class="st">"Kein Ersteller"</span><span class="sc">}</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="ss">Herausgeber: </span><span class="sc">{</span>media<span class="sc">.</span>publisher <span class="kw">or</span> <span class="st">"Kein Herausgeber"</span><span class="sc">}</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="ss">Quelle: </span><span class="sc">{</span>media<span class="sc">.</span>source <span class="kw">or</span> <span class="st">"Keine Quelle"</span><span class="sc">}</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="ss">Datum: </span><span class="sc">{</span>media<span class="sc">.</span>date <span class="kw">or</span> <span class="st">"Kein Datum"</span><span class="sc">}</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="ss">Epoche: </span><span class="sc">{</span>media<span class="sc">.</span>era <span class="kw">or</span> <span class="st">"Keine Epoche"</span><span class="sc">}</span><span class="ss">"""</span>.strip()</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_messages(</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    prompt: <span class="bu">str</span>, image_url: <span class="bu">str</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> <span class="bu">tuple</span>[<span class="bu">list</span>[<span class="bu">dict</span>[<span class="bu">str</span>, Any]], <span class="bu">str</span>, <span class="bu">str</span>]:</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    system <span class="op">=</span> <span class="st">"""ZIEL</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="st">Alt-Texte für historische und archäologische Sammlungsbilder.</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="st">Kurz, sachlich, zugänglich. Erfassung der visuellen Essenz für Screenreader.</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="st">REGELN</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="st">1. Essenz statt Detail. Keine Redundanz zum Seitentext, kein „Bild von“.</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="st">2. Zentralen Text im Bild wiedergeben oder kurz paraphrasieren.</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="st">3. Kontext (Epoche, Ort, Gattung, Material, Datierung) nur bei Relevanz für Verständnis.</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="st">4. Prägnante visuelle Merkmale nennen: Farbe, Haltung, Zustand, Attribute.</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="st">5. Karten/Diagramme: zentrale Aussage oder Variablen.</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="st">6. Sprache: neutral, präzise, faktenbasiert; keine Wertung, keine Spekulation.</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="st">7. Umfang:</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="st">   * Standard: 90–180 Zeichen</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="st">   * Komplexe Karten/Tabellen: max. 400 Zeichen</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="st">VERBOTE</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="st">* Kein alt=, Anführungszeichen, Preambeln oder Füllwörter („zeigt“, „darstellt“).</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="st">* Keine offensichtlichen Metadaten (z. B. Jahreszahlen aus Beschriftung).</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="st">* Keine Bewertungen, Hypothesen oder Stilkommentare.</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="st">* Keine Emojis oder emotionalen Begriffe.</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="st">HEURISTIKEN</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a><span class="st">Porträt: Person (Name, falls bekannt), Epoche, Pose oder Attribut, ggf. Funktion.</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="st">Objekt: Gattung, Material, Datierung, auffällige Besonderheit.</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="st">Dokument: Typ, Sprache/Schrift, Datierung, Kernaussage.</span></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="st">Karte: Gebiet, Zeitraum, Zweck, Hauptvariablen.</span></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a><span class="st">Ereignisfoto: Wer, was, wo, situativer Kontext.</span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="st">Plakat/Cover: Titel, Zweck, zentrale Schlagzeile.</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="st">FALLBACK</span></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a><span class="st">Unklarer Inhalt: generische, aber sinnvolle Essenz aus Metadaten.</span></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a><span class="st">QUELLEN</span></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a><span class="st">Nur visuelle Analyse (Bildinhalt) und übergebene Metadaten. Keine externen Kontexte."""</span>.strip()</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>        [</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>            {<span class="st">"role"</span>: <span class="st">"system"</span>, <span class="st">"content"</span>: system},</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>            {</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>                <span class="st">"role"</span>: <span class="st">"user"</span>,</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>                <span class="st">"content"</span>: [</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>                    {<span class="st">"type"</span>: <span class="st">"text"</span>, <span class="st">"text"</span>: prompt},</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>                    {<span class="st">"type"</span>: <span class="st">"image_url"</span>, <span class="st">"image_url"</span>: {<span class="st">"url"</span>: image_url}},</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>                ],</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>            },</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>        ],</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>        system,</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>        prompt,</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
<section id="alt-text-generation-and-post-processing" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="alt-text-generation-and-post-processing"><span class="header-section-number">2.5</span> Alt Text Generation and Post-processing</h2>
<p>Using the carefully engineered system and user prompts, we ran each image through each of the four models, yielding four candidate descriptions per image. The generation process was automated via a Python script using OpenRouter as an API wrapper. We produced 80 candidate alt-texts (4 per image for n=20 images in our survey). After generation, no post-processing was applied. All results were stored along with metadata and model identifiers for evaluation.</p>
<p>No model refused to describe an image due to some built-in safety filter (labelling a historical photograph as sensitive content). Otherwise we would have handled those on a case-by-case basis by leaving that image for human description. Overall, this pipeline is designed to be simple, and maximize coverage (getting at least one description for every image) while maintaining quality through careful prompting.</p>
</section>
<section id="survey" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="survey"><span class="header-section-number">2.6</span> Survey</h2>
<p>Twenty-one humanities scholars ranked, per image, four model-generated descriptions from <strong>best (1) to worst (4)</strong> under <strong>WCAG-intended criteria for alt text</strong>. Raters were asked to consider: (a) concise rendering of the <strong>core visual content</strong>; (b) <strong>avoidance of redundant</strong> phrases (e.g., “image of”); (c) prioritisation <strong>of salient visual features</strong> (persons, objects, actions, visible text); and (d) <strong>context inclusion only when it improves comprehension</strong>. While factual accuracy, completeness, and absence of bias were <em>not</em> primary ranking dimensions, they may have been factored in implicitly.</p>
</section>
<section id="close-reading" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="close-reading"><span class="header-section-number">2.7</span> Close reading</h2>
<p>To check for these dimensions, the authors conducted a qualitative close reading of a selection of the generated alt-text. This analysis specifically targeted outputs that had received the highest rankings from the expert panel.</p>
<ul>
<li><strong>Factual accuracy</strong> (Did the generated description contain any incorrect identifications of people, objects, or actions?)</li>
<li><strong>Contextual adequacy</strong> (Did the generated description include any incorrect or misleading historical context?)</li>
<li><strong>Bias reproduction</strong> (Did the model reproduce sensitive, derogatory, or racist terminology from the source material?)</li>
</ul>
<p>This allowed us to investigate whether an alt-text could be ranked highly for WCAG alignment while simultaneously being factually incorrect or ethically problematic.</p>
</section>
</section>
<section id="sec-results" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Results and Analysis</h1>
<section id="rq1-feasibility-coverage-throughput-and-unit-cost" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="rq1-feasibility-coverage-throughput-and-unit-cost"><span class="header-section-number">3.1</span> RQ1 Feasibility: Coverage, Throughput, and Unit Cost</h2>
<p>To address the feasibility of automatic alt-text generation at corpus scale, we compared four state-of-the-art vision–language models (VLMs): <strong>Google Gemini 2.5 Flash Lite</strong>, <strong>Meta Llama 4 Maverick</strong>, <strong>OpenAI GPT-4o mini</strong>, and <strong>Qwen 3 VL 8B Instruct</strong>.<br>
Each model generated alt-text descriptions for 20 representative heritage images selected for diversity of content, medium, and metadata completeness.</p>
<section id="coverage-and-reliability" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="coverage-and-reliability"><span class="header-section-number">3.1.1</span> Coverage and reliability</h3>
<p>All models returned non-empty outputs for all 20 prompts, yielding <strong>100 % coverage</strong> and <strong>no failed responses</strong>.<br>
This demonstrates that current VLMs can reliably produce textual descriptions even for heterogeneous heritage data without the need for fallback mechanisms.</p>
</section>
<section id="throughput-and-latency" class="level3" data-number="3.1.2">
<h3 data-number="3.1.2" class="anchored" data-anchor-id="throughput-and-latency"><span class="header-section-number">3.1.2</span> Throughput and latency</h3>
<p>Processing speed ranged between <strong>0.24 – 0.43 items/s</strong>, corresponding to median latencies of <strong>2 – 4 s</strong> per item. Models were accessed via OpenRouter.ai with the following providers: Google – gemini-2.5-flash-lite, OpenAI – gpt-4o-mini, Together – llama-4-maverick, and Alibaba – Qwen 3 VL 8B.<br>
<em>Qwen 3 VL 8B</em> achieved the fastest throughput and lowest latency, while <em>OpenAI GPT-4o mini</em> was slower but consistent.<br>
All models showed moderate response-time variability (≈ 1.7 – 10 s).</p>
</section>
<section id="cost-efficiency" class="level3" data-number="3.1.3">
<h3 data-number="3.1.3" class="anchored" data-anchor-id="cost-efficiency"><span class="header-section-number">3.1.3</span> Cost efficiency</h3>
<p>Unit generation costs differed by two orders of magnitude, reflecting API pricing rather than architectural complexity. According to <strong>OpenRouter.ai</strong> cost reports, costs per item ranged from <strong>$1.8 × 10⁻⁴</strong> (Qwen) to <strong>$3.6 × 10⁻³</strong> (OpenAI).</p>
<table class="caption-top table">
<caption>Feasibility metrics for alt text generation (n = 20).</caption>
<colgroup>
<col style="width:20%">
<col style="width:20%">
<col style="width:20%">
<col style="width:20%">
<col style="width:20%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align:left">Model</th>
<th style="text-align:right">Coverage (%)</th>
<th style="text-align:right">Throughput (items/s)</th>
<th style="text-align:right">Median Latency (s)</th>
<th style="text-align:right">Mean Cost (USD/item)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align:left">Google Gemini 2.5 Flash Lite</td>
<td style="text-align:right">100</td>
<td style="text-align:right">0.31</td>
<td style="text-align:right">2.72</td>
<td style="text-align:right">0.000215</td>
</tr>
<tr class="even">
<td style="text-align:left">Meta Llama 4 Maverick</td>
<td style="text-align:right">100</td>
<td style="text-align:right">0.41</td>
<td style="text-align:right">2.38</td>
<td style="text-align:right">0.000395</td>
</tr>
<tr class="odd">
<td style="text-align:left">OpenAI GPT-4o Mini</td>
<td style="text-align:right">100</td>
<td style="text-align:right">0.24</td>
<td style="text-align:right">4.00</td>
<td style="text-align:right">0.003625</td>
</tr>
<tr class="even">
<td style="text-align:left">Qwen 3 VL 8B Instruct</td>
<td style="text-align:right">100</td>
<td style="text-align:right"><strong>0.43</strong></td>
<td style="text-align:right"><strong>2.27</strong></td>
<td style="text-align:right"><strong>0.000182</strong></td>
</tr>
</tbody>
</table>
</section>
<section id="summary-of-rq1" class="level3" data-number="3.1.4">
<h3 data-number="3.1.4" class="anchored" data-anchor-id="summary-of-rq1"><span class="header-section-number">3.1.4</span> Summary of RQ1</h3>
<p>All models achieved <strong>complete coverage</strong>, <strong>acceptable latency</strong>, and <strong>minimal cost</strong>, confirming the technical and economic feasibility of automated alt text generation for large, heterogeneous cultural collections.<br>
Failures were not due to empty outputs but to <strong>qualitative weaknesses</strong>, which are examined under RQ2.</p>
</section>
</section>
<section id="rq2-relative-quality-expert-ranking-and-qualitative-assessment" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="rq2-relative-quality-expert-ranking-and-qualitative-assessment"><span class="header-section-number">3.2</span> RQ2 Relative Quality: Expert Ranking and Qualitative Assessment</h2>
<section id="quantitative-ranking-analysis" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="quantitative-ranking-analysis"><span class="header-section-number">3.2.1</span> Quantitative ranking analysis</h3>
<p>Within each task, all models were directly compared; task-level median ranks were analyzed across the 20 tasks using the <strong>Friedman test</strong> for repeated measures, followed by <strong>pairwise Wilcoxon signed-rank tests</strong> with <strong>Holm–Bonferroni correction</strong>.<br>
Agreement across tasks was quantified with <strong>Kendall’s W</strong>.</p>
<p><span class="math display">\[
\chi^2(3, N = 20) = 6.02, \quad p = 0.11, \quad W = 0.0085.
\]</span></p>
<p>The results indicate <strong>no statistically significant difference</strong> among models (<span class="math inline">\(p &gt; 0.05\)</span>) and <strong>very low inter-task agreement</strong> (<span class="math inline">\(W \approx 0.01\)</span>), implying that relative rankings varied substantially by task. Pairwise Wilcoxon comparisons (<a href="#sec-appendix-pairwise-comparison" class="quarto-xref">Section&nbsp;8.4</a>) showed no significant differences after correction (<span class="math inline">\(p_{\text{Holm}} &gt; 0.5\)</span>); unadjusted <span class="math inline">\(p\)</span>-values suggested weak, non-significant trends favoring <strong>OpenAI GPT-4o Mini</strong> and <strong>Qwen 3 VL 8B</strong> over <strong>Google Gemini</strong> and <strong>Meta Llama</strong>.</p>
</section>
<section id="descriptive-patterns" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="descriptive-patterns"><span class="header-section-number">3.2.2</span> Descriptive patterns</h3>
<p>Twenty-one human experts each rated four alternative texts for 20 images, yielding a total of 420 individual ratings:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align:left">Model</th>
<th style="text-align:right">Rank 1</th>
<th style="text-align:right">Rank 2</th>
<th style="text-align:right">Rank 3</th>
<th style="text-align:right">Rank 4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align:left">Google Gemini 2.5 Flash Lite</td>
<td style="text-align:right">86</td>
<td style="text-align:right">84</td>
<td style="text-align:right">113</td>
<td style="text-align:right"><strong>137</strong></td>
</tr>
<tr class="even">
<td style="text-align:left">Meta Llama 4 Maverick</td>
<td style="text-align:right">88</td>
<td style="text-align:right">114</td>
<td style="text-align:right">110</td>
<td style="text-align:right">108</td>
</tr>
<tr class="odd">
<td style="text-align:left">OpenAI GPT-4o Mini</td>
<td style="text-align:right"><strong>132</strong></td>
<td style="text-align:right">101</td>
<td style="text-align:right">84</td>
<td style="text-align:right">103</td>
</tr>
<tr class="even">
<td style="text-align:left">Qwen 3 VL 8B Instruct</td>
<td style="text-align:right">114</td>
<td style="text-align:right"><strong>121</strong></td>
<td style="text-align:right">113</td>
<td style="text-align:right"><strong>72</strong></td>
</tr>
</tbody>
</table>
<p>OpenAI and Qwen outputs received more first-place and fewer last-place rankings, but overlapping rank distributions <a href="#sec-appendix-rank-distribution" class="quarto-xref">Section&nbsp;8.3</a> indicate that these tendencies remain descriptive rather than inferentially significant.</p>
</section>
<section id="sec-2-close-reading" class="level3" data-number="3.2.3">
<h3 data-number="3.2.3" class="anchored" data-anchor-id="sec-2-close-reading"><span class="header-section-number">3.2.3</span> Qualitative evaluation of top-rated outputs</h3>
<p>A manual close reading inspection of hand-picked alt texts with highest mean rank scores revealed that even those outputs deemed to be the ‘best’ were not free from substantive and ethical shortcomings. In fact, all models produced at least one error. Some are easy to catch in a manual review (factually wrong descriptions), others require expert domain knowledge (reproduction of stereotypes).</p>
<section id="example-for-factually-wrong-text" class="level4" data-number="3.2.3.1">
<h4 data-number="3.2.3.1" class="anchored" data-anchor-id="example-for-factually-wrong-text"><span class="header-section-number">3.2.3.1</span> Example for Factually Wrong Text</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/m92410.webp" class="figure-img img-fluid" width="600" alt="" loading="lazy" decoding="async" height="387" jampack-sized="true"></p>
<figcaption>Faltblatt der Gruppe ‹Freiräume für Frauen› (FFF) (<a href="https://forschung.stadtgeschichtebasel.ch/items/abb92410.html#m92410">m92410</a>)</figcaption>
</figure>
</div>
<ul>
<li><strong>Best-ranked alt-text</strong>: «Faltblatt der Gruppe ‹Freiräume für Frauen› (FFF) von 1992. Zwei Personen gehen eine Treppe hinunter, Frau mit Zigarette, Mann mit Sonnenbrille und Händen in den Hosentaschen» (Google Gemini 2.5 Flash Lite)</li>
<li><strong>Translation</strong>: “Leaflet of the ‘Freiräume für Frauen’ (FFF) group from 1992. Two people walk down a staircase, a woman with a cigarette, a man with sunglasses and hands in his pockets”</li>
<li><strong>Shortcoming</strong>: This description is factually incorrect. The two people in the photograph are clearly walking up the staircase, not down.</li>
</ul>
</section>
<section id="example-for-reproduction-of-stereotypes" class="level4" data-number="3.2.3.2">
<h4 data-number="3.2.3.2" class="anchored" data-anchor-id="example-for-reproduction-of-stereotypes"><span class="header-section-number">3.2.3.2</span> Example for Reproduction of Stereotypes</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/m91960.webp" class="figure-img img-fluid" width="600" alt="" loading="lazy" decoding="async" height="306" jampack-sized="true"></p>
<figcaption>‹Gegen den Eisenbahnhandel›, anonyme antisemitische Karikatur in der Extraausgabe der Basler ‹Allgemeinen Schweizer Zeitung› vom 14. Februar 1898 (<a href="https://forschung.stadtgeschichtebasel.ch/items/abb92410.html#m92410">m91960</a>)</figcaption>
</figure>
</div>
<ul>
<li><strong>Best-ranked alt-text</strong>: «Antisemitische Karikatur zur Eisenbahnverstaatlichung 1898. Links: Wehrhafte Helvetia zwingt knienden Geldverleiher nieder. Rechts: Kniende Helvetia bittet reichen Juden um Geld. Kontrastdarstellung zum Volksentscheid über die Eisenbahnverstaatlichung» (Meta Llama 4 Maverick)</li>
<li><strong>Translation</strong>: “Antisemitic caricature on the nationalization of the railways 1898. Left: Defensive Helvetia forces a kneeling moneylender down. Right: Kneeling Helvetia asks a rich Jew for money. Contrasting depiction of the referendum on the nationalization of the railways”</li>
<li><strong>Shortcoming</strong>: The text uncritically reproduces the term “reichen Juden” (rich Jew). This is an antisemitic stereotype likely drawn from the item’s metadata or biased training data, not a neutral description of the visual content.</li>
</ul>
</section>
<section id="example-for-selective-description" class="level4" data-number="3.2.3.3">
<h4 data-number="3.2.3.3" class="anchored" data-anchor-id="example-for-selective-description"><span class="header-section-number">3.2.3.3</span> Example for Selective Description</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/m88415_1.webp" class="figure-img img-fluid" width="600" alt="" loading="lazy" decoding="async" height="315" jampack-sized="true"></p>
<figcaption>Zuschauerzahlen von FC Basel und Konzerten im St.&nbsp;Jakob-Stadion, 1960–2022 (<a href="https://forschung.stadtgeschichtebasel.ch/items/abb88415.html#m88415_1">m88415_1</a>)</figcaption>
</figure>
</div>
<ul>
<li><strong>Best ranked alt-text</strong>: «Balkendiagramm: Zuschauerzahlen von FC Basel (blau) und Konzerten (gelb) im St.&nbsp;Jakob-Stadion, 1960–2022. Höchstwerte bei Fußballspielen ab 2001 im neuen St.&nbsp;Jakob-Park, Einbruch 2020 durch Corona-Pandemie» (Meta Llama 4 Maverick)</li>
<li><strong>Translation</strong>: “Bar chart: Audience numbers for FC Basel (blue) and concerts (yellow) at St.&nbsp;Jakob-Stadion, 1960–2022. Peak values for football matches from 2001 in the new St.&nbsp;Jakob-Park, slump in 2020 due to the Corona pandemic”</li>
<li><strong>Shortcoming</strong>: The description is selective and unbalanced. It provides a detailed interpretation of the trends for the football matches (blue bars) but does not give information or interpretation about neither the concert attendance (yellow bars), nor the number of football matches (green circles), omitting a lot of the chart’s comparative data.</li>
</ul>
</section>
</section>
<section id="interpretation" class="level3" data-number="3.2.4">
<h3 data-number="3.2.4" class="anchored" data-anchor-id="interpretation"><span class="header-section-number">3.2.4</span> Interpretation</h3>
<p>These examples for erroneous alt texts that still were ranked best in our survey illustrate that high quantitative rankings do not imply factual accuracy or ethical adequacy as illustrated by a close reading. Even when linguistically fluent and stylistically polished, VLM-generated alt texts may introduce epistemic distortions or perpetuate historical bias.</p>
<p>Quantitatively, no model achieved a statistically distinct performance profile; qualitatively, all exhibited <strong>systematic error patterns</strong>—misrecognition, omission, and uncritical reproduction of harmful source language.<br>
This combination highlights the <strong>limits of rank-based evaluation alone</strong>: expert preference captures relative quality but not factual or ethical soundness.</p>
</section>
</section>
<section id="synthesis" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="synthesis"><span class="header-section-number">3.3</span> Synthesis</h2>
<ul>
<li><strong>RQ1 Feasibility:</strong> All four VLMs achieved full coverage, low latency, and negligible cost, confirming the operational viability of automated WCAG-aligned alt text generation for heritage corpora.</li>
<li><strong>RQ2 Relative quality:</strong> Expert rankings showed <em>no statistically significant hierarchy</em> among models (<span class="math inline">\(p = 0.11\)</span>, <span class="math inline">\(W \approx 0.01\)</span>), and qualitative inspection exposed factual inaccuracies, biased reproduction, and selective omissions even in top-rated outputs.</li>
</ul>
<p>Overall, current VLMs can populate heritage databases at scale but require <strong>expert review and critical post-editing</strong> to ensure factual precision, ethical compliance, and contextual adequacy. Automated alt text workflows should therefore combine model ensembles with targeted human oversight to meet both accessibility and historiographical standards.</p>
</section>
</section>
<section id="sec-discussion" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Discussion</h1>
<p>Our findings confirm the central tension in using contemporary VLMs for heritage accessibility: they are <strong>operationally feasible but epistemically fragile</strong>. The 100% coverage, low latency, and negligible cost (RQ1) demonstrate that the technical and economic barriers to generating descriptions at a corpus-wide scale are virtually gone. However, the results from our expert evaluation (RQ2) reveal a gap between this operational success and the production of high-quality, trustworthy alt text.</p>
<p>The lack of a statistically significant winner among the models, combined with the low inter-task agreement (<span class="math inline">\(W \approx 0.01\)</span>), is a key finding. It suggests that <strong>no single model is a reliable one-shot solution</strong>. A model that performs well on a photograph might fail on a diagram, and vice-versa. This variability reinforces the findings of mechanistic analyses like <span class="citation" data-cites="gavrikov2025">Gavrikov et al. (<a href="#ref-gavrikov2025" role="doc-biblioref">2025</a>)</span>, which show that VLM outputs are highly sensitive to how their fusion layers mediate visual cues. Our metadata-rich prompts likely steered models toward more context-aware descriptions, but this “cue steering” was not a panacea against factual or ethical errors.</p>
<p>Critically, our mixed-methods approach exposed the <strong>limits of rank-based evaluation alone</strong>. The qualitative close reading (<a href="#sec-2-close-reading" class="quarto-xref">Section&nbsp;3.2.3</a>) revealed that outputs ranked highly by experts for WCAG alignment (i.e., conciseness and style) could still be factually wrong, ethically problematic, or epistemically shallow. The FFF flyer (m92410) example, which confidently misidentifies the walking direction, and the antisemitic cartoon (m91960) example, which uncritically reproduces the term ‘reicher Jude’ (rich Jew) from the source’s metadata, are stark illustrations. Fluency, in essence, is not a proxy for accuracy or ethical adequacy.</p>
<p>This study validates the framing of AI as an <strong>accessibility assistant</strong> (<a href="#sec-introduction" class="quarto-xref">Section&nbsp;1</a>) rather than an autonomous author. The VLM output should be treated as a <em>first draft</em> for human review, not a final product. This reframes the labor of digital humanists: from <em>authoring</em> descriptions from scratch to <em>critically editing</em> machine-generated drafts. This aligns with the hybrid assessment frameworks proposed in educational research <span class="citation" data-cites="reihanian2025">(<a href="#ref-reihanian2025" role="doc-biblioref">Reihanian et al. 2025</a>)</span> and necessitates the “new hermeneutic of suspicion” <span class="citation" data-cites="fickers2022">(<a href="#ref-fickers2022" role="doc-biblioref">Fickers 2022</a>)</span> advocated in our introduction. Curators and historians must be trained in AI literacy <span class="citation" data-cites="strien2022">(<a href="#ref-strien2022" role="doc-biblioref">Strien et al. 2022</a>)</span> to spot subtle biases and misinterpretations that a fluent-sounding description might otherwise obscure.</p>
<p>Finally, our study has limitations. The expert ranking (n=21) was based on a relatively small (n=20) subset of images, which, while diverse in content, limits the statistical power of our quantitative analysis. Furthermore, the survey criteria explicitly prioritized WCAG stylistic guidelines over factual accuracy, a dimension we could only capture post-hoc via our qualitative close reading. A crucial missing component, which we intentionally bracketed to first establish a baseline, is the perspective of <strong>blind and low-vision users themselves</strong>. Without their input, any AI-driven accessibility solution risks falling into the trap of “techno-ableism” <span class="citation" data-cites="shew2023">(<a href="#ref-shew2023" role="doc-biblioref">Shew 2023</a>)</span>, designing <em>for</em> a community without designing <em>with</em> them.</p>
</section>
<section id="sec-future-work" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Future Work</h1>
<p>Building on this study’s findings, we identify several critical paths for future research to bridge the gap between the operational promise and epistemic fragility of AI-generated alt text for heritage.</p>
<ul>
<li><strong>Usability and User-Experience (UX) Studies:</strong> The most urgent next step is to move beyond expert-as-proxy and engage directly with blind and low-vision (BLV) users. Future work should conduct qualitative usability studies to assess how BLV readers <em>experience</em> these AI-generated descriptions. Do they find them helpful, confusing, or biased? Does the inclusion of metadata (as prompted) improve or hinder “imaginability”? This addresses the techno-ableism critique and centers the lived experience of those the technology claims to serve.</li>
<li><strong>Domain-Specific Fine-Tuning:</strong> This study relied on general-purpose VLMs with prompt engineering. A promising avenue is the <strong>fine-tuning of open-weight models</strong> (such as Meta’s Llama 4 or Qwen’s Qwen3 VL) on a high-quality, domain-specific dataset. By training a model on thousands of expert-vetted alt texts from GLAM (Galleries, Libraries, Archives, and Museums) collections, it may be possible to create a model that is more factually accurate, context-aware, and ethically sensitive to heritage content than its general-purpose counterparts.</li>
<li><strong>Developing Human-in-the-Loop (HITL) Workflows:</strong> Our results confirm the necessity of expert review. Future research should move from <em>evaluation</em> to <em>implementation</em> by designing and testing <strong>HITL editorial interfaces</strong>. What is the most effective workflow for a historian to review, correct, and approve AI-generated alt text? How can we best integrate AI-generated “drafts” into existing collections management systems (CMS) and research data platforms, complete with policies for handling sensitive content?</li>
<li><strong>Scaling the Benchmark:</strong> This study established a 100-item benchmark dataset. The next phase should involve using this larger dataset to conduct a more robust quantitative analysis. This would allow for a more granular breakdown of model performance by media type (e.g., maps vs.&nbsp;manuscripts vs.&nbsp;photographs) and help establish more reliable cost-quality trade-offs to guide GLAM institutions in adopting these technologies.</li>
</ul>
</section>
<section id="acknowledgements" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Acknowledgements</h1>
<p>We thank <strong>Cristina Münch</strong> and <strong>Noëlle Schnegg</strong> for curating metadata and providing image assets from the <em>Stadt.Geschichte.Basel</em> collection. We are grateful to the <strong>21 expert participants</strong> for their careful rankings and comments. For insightful feedback on an early draft, we thank <strong>Dr.&nbsp;Mehrdad Almasi</strong> (Luxembourg Centre for Contemporary and Digital History, C²DH). Any remaining errors are our own.</p>
</section>
<section id="references" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> References</h1>
<div id="refs" class="csl-bib-body hanging-indent references" data-entry-spacing="0" role="list">
<div id="ref-bowker1999" class="csl-entry" role="listitem">
Bowker, Geoffrey C., and Susan Leigh Star. 1999. <em>Sorting Things Out: Classification and Its Consequences</em>. Inside Technology. Cambridge, Massachusetts: MIT Press.
</div>
<div id="ref-cecilia2023b" class="csl-entry" role="listitem">
Cecilia, Rafie, Theano Moussouri, and John Fraser. 2023a. <span>“<span>AltText</span>: <span>An</span> Institutional Tool for Change.”</span> <em>Curator</em> 66 (2): 225–31. <a href="https://doi.org/10.1111/cura.12551">https://doi.org/10.1111/cura.12551</a>.
</div>
<div id="ref-cecilia2023a" class="csl-entry" role="listitem">
———. 2023b. <span>“Creating Accessible Digital Images for Vision Impaired Audiences and Researchers.”</span> <em>Curator</em> 66 (1): 5–8. <a href="https://doi.org/10.1111/cura.12536">https://doi.org/10.1111/cura.12536</a>.
</div>
<div id="ref-cetinic2021" class="csl-entry" role="listitem">
Cetinic, Eva. 2021. <span>“Towards <span>Generating</span> and <span>Evaluating Iconographic Image Captions</span> of <span>Artworks</span>.”</span> <em>Journal of Imaging</em> 7 (8): 123. <a href="https://doi.org/10.3390/jimaging7080123">https://doi.org/10.3390/jimaging7080123</a>.
</div>
<div id="ref-fickers2022" class="csl-entry" role="listitem">
Fickers, Andreas. 2022. <span>“Digital Hermeneutics: <span>The</span> Reflexive Turn in Digital Public History?”</span> In <em>Handbook of Digital Public History</em>, edited by Serge Noiret, Valérie Schafer, and Gerben Zaagsma, 139–48. De Gruyter. <a href="https://doi.org/10.1515/9783110430295-012">https://doi.org/10.1515/9783110430295-012</a>.
</div>
<div id="ref-gavrikov2025" class="csl-entry" role="listitem">
Gavrikov, Paul, Jovita Lukasik, Steffen Jung, Robert Geirhos, M. Jehanzeb Mirza, Margret Keuper, and Janis Keuper. 2025. <span>“Can <span>We Talk Models Into Seeing</span> the <span>World Differently</span>?”</span> March 5, 2025. <a href="https://doi.org/10.48550/arXiv.2403.09193">https://doi.org/10.48550/arXiv.2403.09193</a>.
</div>
<div id="ref-liu2023" class="csl-entry" role="listitem">
Liu, Nelson F., Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. <span>“Lost in the <span>Middle</span>: <span>How Language Models Use Long Contexts</span>.”</span> November 20, 2023. <a href="https://doi.org/10.48550/arXiv.2307.03172">https://doi.org/10.48550/arXiv.2307.03172</a>.
</div>
<div id="ref-maehr_2022" class="csl-entry" role="listitem">
Mähr, Moritz. 2022. <span>“Research <span>Data Management</span> in (<span>Public</span>) <span>History</span>.”</span> Keynote. Istituto Svizzero di Roma. <a href="https://doi.org/10.5281/zenodo.6637118">https://doi.org/10.5281/zenodo.6637118</a>.
</div>
<div id="ref-maehrHandbuchZurErstellung2024" class="csl-entry" role="listitem">
Mähr, Moritz, and Noëlle Schnegg. 2024. <span>“Handbuch zur Erstellung diskriminierungsfreier Metadaten für historische Quellen und Forschungsdaten: Erfahrungen aus dem geschichtswissenschaftlichen Forschungsprojekt Stadt.Geschichte.Basel.”</span> Basel: Zenodo. <a href="https://doi.org/10.5281/ZENODO.11124720">https://doi.org/10.5281/ZENODO.11124720</a>.
</div>
<div id="ref-mohanbabu2024" class="csl-entry" role="listitem">
Mohanbabu, Ananya Gubbi, and Amy Pavel. 2024. <span>“Context-<span>Aware Image Descriptions</span> for <span>Web Accessibility</span>.”</span> In <em>The 26th <span>International ACM SIGACCESS Conference</span> on <span>Computers</span> and <span>Accessibility</span></em>, 1–17. <a href="https://doi.org/10.1145/3663548.3675658">https://doi.org/10.1145/3663548.3675658</a>.
</div>
<div id="ref-reihanian2025" class="csl-entry" role="listitem">
Reihanian, Iman, Yunfei Hou, Yu Chen, and Yifei Zheng. 2025. <span>“A <span>Review</span> of <span>Generative AI</span> in <span>Computer Science Education</span>: <span>Challenges</span> and <span>Opportunities</span> in <span>Accuracy</span>, <span>Authenticity</span>, and <span>Assessment</span>.”</span> June 17, 2025. <a href="https://doi.org/10.48550/arXiv.2507.11543">https://doi.org/10.48550/arXiv.2507.11543</a>.
</div>
<div id="ref-shew2023" class="csl-entry" role="listitem">
Shew, Ashley. 2023. <em>Against Technoableism: <span>Rethinking</span> Who Needs Improvement</em>. New York: W. W. Norton.
</div>
<div id="ref-strien2022" class="csl-entry" role="listitem">
Strien, Daniel van, Mark Bell, Nora Rose McGregor, and Michael Trizna. 2022. <span>“An <span>Introduction</span> to <span>AI</span> for <span>GLAM</span>.”</span> In <em>Proceedings of the <span>Second Teaching Machine Learning</span> and <span>Artificial Intelligence Workshop</span></em>, 20–24. PMLR. <a href="https://proceedings.mlr.press/v170/strien22a.html">https://proceedings.mlr.press/v170/strien22a.html</a>.
</div>
<div id="ref-wcag2023" class="csl-entry" role="listitem">
World Wide Web Consortium. 2023. <span>“Web Content Accessibility Guidelines (<span>WCAG</span>) 2.2.”</span> 2023. <a href="https://www.w3.org/TR/WCAG22/">https://www.w3.org/TR/WCAG22/</a>.
</div>
</div>
</section>



<a onclick="return window.scrollTo(0,0),!1;
" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a><div id="quarto-appendix" class="default"><section id="sec-appendix" class="appendix level1" data-number="8"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">8</span> Appendix</h2><div class="quarto-appendix-contents">







</div></section><section id="sec-appendix-data" class="level2 appendix" data-number="8.1"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">8.1</span> Dataset description</h2><div class="quarto-appendix-contents">

<p>In both our selection of 100-item and the 20-item survey subset, we tried to find an overall balance between all data types and eras that make up the Stadt.Geschichte.Basel collection. Due to its historical nature, not all data types appear in all eras, and the smaller size of the survey subset accentuates these constraints. We dropped <em>Painting</em> items and the <em>Antiquity</em> era from the survey subset due to their low prevalence in our corpus.</p>






</div></section><section id="distribution-by-type" class="level3 appendix" data-number="8.1.1"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">8.1.1</span> Distribution by Type</h2><div class="quarto-appendix-contents">

<table class="caption-top table">
<caption>Distribution of media types in the dataset and survey subset.</caption>
<thead>
<tr class="header">
<th style="text-align:left">Type</th>
<th style="text-align:left">Dataset</th>
<th style="text-align:left">Survey</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align:left">Painting</td>
<td style="text-align:left">12</td>
<td style="text-align:left">0</td>
</tr>
<tr class="even">
<td style="text-align:left">Object</td>
<td style="text-align:left">13</td>
<td style="text-align:left">2</td>
</tr>
<tr class="odd">
<td style="text-align:left">Photograph (Archaeological Site)</td>
<td style="text-align:left">10</td>
<td style="text-align:left">2</td>
</tr>
<tr class="even">
<td style="text-align:left">Photograph (Historical Scenes)</td>
<td style="text-align:left">10</td>
<td style="text-align:left">2</td>
</tr>
<tr class="odd">
<td style="text-align:left">Scan of Newspapers, Posters, Lists, etc.</td>
<td style="text-align:left">10</td>
<td style="text-align:left">3</td>
</tr>
<tr class="even">
<td style="text-align:left">Drawing (Archaeological Reconstruction)</td>
<td style="text-align:left">10</td>
<td style="text-align:left">3</td>
</tr>
<tr class="odd">
<td style="text-align:left">Drawing (Historical Drawing)</td>
<td style="text-align:left">10</td>
<td style="text-align:left">2</td>
</tr>
<tr class="even">
<td style="text-align:left">Map</td>
<td style="text-align:left">10</td>
<td style="text-align:left">2</td>
</tr>
<tr class="odd">
<td style="text-align:left">Diagram (Statistics)</td>
<td style="text-align:left">10</td>
<td style="text-align:left">2</td>
</tr>
<tr class="even">
<td style="text-align:left">Diagram (Flowchart, Schema etc.)</td>
<td style="text-align:left">5</td>
<td style="text-align:left">2</td>
</tr>
<tr class="odd">
<td style="text-align:left"><strong>Total</strong></td>
<td style="text-align:left"><strong>100</strong></td>
<td style="text-align:left"><strong>20</strong></td>
</tr>
</tbody>
</table>
</div></section><section id="distribution-by-era" class="level3 appendix" data-number="8.1.2"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">8.1.2</span> Distribution by Era</h2><div class="quarto-appendix-contents">

<p>With regards to the historical eras represented in the subset, we aimed to cover the full chronological span of the Stadt.Geschichte.Basel project, from 50’000 BCE until the 21st century. Since each item is tagged with an era in the metadata, we could systematically select items across periods in a way that resembles that distribution in the whole research data set (at the time of writing). Items from some eras, e.g.&nbsp;<em>Antiquity</em> and <em>21st Century</em>, are less frequent in the overall collection which is reflected in a lower representation in our dataset.</p>
<table class="caption-top table">
<caption>Distribution of historical eras in the dataset and survey subset.</caption>
<thead>
<tr class="header">
<th style="text-align:left">Era</th>
<th style="text-align:left">Dataset</th>
<th style="text-align:left">Survey</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align:left">Protohistory</td>
<td style="text-align:left">11</td>
<td style="text-align:left">3</td>
</tr>
<tr class="even">
<td style="text-align:left">Antiquity</td>
<td style="text-align:left">3</td>
<td style="text-align:left">0</td>
</tr>
<tr class="odd">
<td style="text-align:left">Middle Ages</td>
<td style="text-align:left">16</td>
<td style="text-align:left">2</td>
</tr>
<tr class="even">
<td style="text-align:left">Early Modern period</td>
<td style="text-align:left">21</td>
<td style="text-align:left">3</td>
</tr>
<tr class="odd">
<td style="text-align:left">19th century</td>
<td style="text-align:left">19</td>
<td style="text-align:left">5</td>
</tr>
<tr class="even">
<td style="text-align:left">20th century</td>
<td style="text-align:left">25</td>
<td style="text-align:left">5</td>
</tr>
<tr class="odd">
<td style="text-align:left">21st century</td>
<td style="text-align:left">5</td>
<td style="text-align:left">2</td>
</tr>
<tr class="even">
<td style="text-align:left"><strong>Total</strong></td>
<td style="text-align:left"><strong>100</strong></td>
<td style="text-align:left"><strong>20</strong></td>
</tr>
</tbody>
</table>
</div></section><section id="distribution-across-era-and-type-survey-count-in-parentheses" class="level3 appendix" data-number="8.1.3"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">8.1.3</span> Distribution across Era and Type (survey count in parentheses)</h2><div class="quarto-appendix-contents">

<table class="caption-top table">
<caption>Distribution of media types across historical eras in the survey subset (counts in parentheses).</caption>
<colgroup>
<col style="width:12%">
<col style="width:12%">
<col style="width:12%">
<col style="width:12%">
<col style="width:12%">
<col style="width:12%">
<col style="width:12%">
<col style="width:12%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align:left">Type</th>
<th style="text-align:left">Protohistory</th>
<th style="text-align:left">Antiquity</th>
<th style="text-align:left">Middle Ages</th>
<th style="text-align:left">Early Modern Period</th>
<th style="text-align:left">19th century</th>
<th style="text-align:left">20th century</th>
<th style="text-align:left">21st century</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align:left">Scan of Newspapers, Lists, etc.</td>
<td style="text-align:left">0 (0)</td>
<td style="text-align:left">0 (0)</td>
<td style="text-align:left">1 (0)</td>
<td style="text-align:left">2 (0)</td>
<td style="text-align:left">3 (2)</td>
<td style="text-align:left">4 (1)</td>
<td style="text-align:left">0 (0)</td>
</tr>
<tr class="even">
<td style="text-align:left">Photograph (Historical Scenes)</td>
<td style="text-align:left">0 (0)</td>
<td style="text-align:left">0 (0)</td>
<td style="text-align:left">0 (0)</td>
<td style="text-align:left">0 (0)</td>
<td style="text-align:left">3 (1)</td>
<td style="text-align:left">7 (1)</td>
<td style="text-align:left">0 (0)</td>
</tr>
<tr class="odd">
<td style="text-align:left">Photograph (Archaeological Site)</td>
<td style="text-align:left">0 (0)</td>
<td style="text-align:left">0 (0)</td>
<td style="text-align:left">0 (0)</td>
<td style="text-align:left">0 (0)</td>
<td style="text-align:left">0 (0)</td>
<td style="text-align:left">5 (1)</td>
<td style="text-align:left">5 (1)</td>
</tr>
<tr class="even">
<td style="text-align:left">Object</td>
<td style="text-align:left">3 (0)</td>
<td style="text-align:left">0 (0)</td>
<td style="text-align:left">3 (0)</td>
<td style="text-align:left">6 (1)</td>
<td style="text-align:left">1 (1)</td>
<td style="text-align:left">0 (0)</td>
<td style="text-align:left">0 (0)</td>
</tr>
<tr class="odd">
<td style="text-align:left">Map</td>
<td style="text-align:left">1 (0)</td>
<td style="text-align:left">3 (0)</td>
<td style="text-align:left">1 (0)</td>
<td style="text-align:left">1 (0)</td>
<td style="text-align:left">1 (1)</td>
<td style="text-align:left">3 (1)</td>
<td style="text-align:left">0 (0)</td>
</tr>
<tr class="even">
<td style="text-align:left">Drawing (Historical Drawing)</td>
<td style="text-align:left">0 (0)</td>
<td style="text-align:left">0 (0)</td>
<td style="text-align:left">1 (0)</td>
<td style="text-align:left">5 (2)</td>
<td style="text-align:left">4 (0)</td>
<td style="text-align:left">0 (0)</td>
<td style="text-align:left">0 (0)</td>
</tr>
<tr class="odd">
<td style="text-align:left">Drawing (Archaeological Reconstruction)</td>
<td style="text-align:left">5 (3)</td>
<td style="text-align:left">0 (0)</td>
<td style="text-align:left">5 (0)</td>
<td style="text-align:left">0 (0)</td>
<td style="text-align:left">0 (0)</td>
<td style="text-align:left">0 (0)</td>
<td style="text-align:left">0 (0)</td>
</tr>
<tr class="even">
<td style="text-align:left">Diagram (Statistics)</td>
<td style="text-align:left">0 (0)</td>
<td style="text-align:left">0 (0)</td>
<td style="text-align:left">0 (0)</td>
<td style="text-align:left">1 (0)</td>
<td style="text-align:left">2 (0)</td>
<td style="text-align:left">6 (2)</td>
<td style="text-align:left">1 (1)</td>
</tr>
<tr class="odd">
<td style="text-align:left">Diagram (Flowchart, Schema etc.)</td>
<td style="text-align:left">0 (0)</td>
<td style="text-align:left">0 (0)</td>
<td style="text-align:left">3 (2)</td>
<td style="text-align:left">0 (0)</td>
<td style="text-align:left">1 (0)</td>
<td style="text-align:left">1 (0)</td>
<td style="text-align:left">0 (0)</td>
</tr>
<tr class="even">
<td style="text-align:left">Painting</td>
<td style="text-align:left">0 (0)</td>
<td style="text-align:left">0 (0)</td>
<td style="text-align:left">2 (0)</td>
<td style="text-align:left">6 (0)</td>
<td style="text-align:left">4 (0)</td>
<td style="text-align:left">0 (0)</td>
<td style="text-align:left">0 (0)</td>
</tr>
</tbody>
</table>
</div></section><section id="language-distribution" class="level3 appendix" data-number="8.1.4"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">8.1.4</span> Language Distribution</h2><div class="quarto-appendix-contents">

<p>Our research data collection primarily contains items in German, with a small number of items in Latin, French and Dutch. We aimed to reflect this language distribution in our selection. In a similar vein, we wanted to take into account different typographic styles. However, writing is not fully legible in many cases anyway – since we are working with 800x800 pixel JPG image thumbnails – and thus played only a minor factor in the selection process.</p>
<table class="caption-top table">
<caption>Distribution of languages in the dataset and survey subset.</caption>
<thead>
<tr class="header">
<th style="text-align:left">Language</th>
<th style="text-align:left">Dataset</th>
<th style="text-align:left">Survey</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align:left">German</td>
<td style="text-align:left">33</td>
<td style="text-align:left">9</td>
</tr>
<tr class="even">
<td style="text-align:left">Latin</td>
<td style="text-align:left">8</td>
<td style="text-align:left">2</td>
</tr>
<tr class="odd">
<td style="text-align:left">French</td>
<td style="text-align:left">2</td>
<td style="text-align:left">0</td>
</tr>
<tr class="even">
<td style="text-align:left">Without written text</td>
<td style="text-align:left">57</td>
<td style="text-align:left">9</td>
</tr>
</tbody>
</table>
</div></section><section id="spatial-context" class="level3 appendix" data-number="8.1.5"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">8.1.5</span> Spatial Context</h2><div class="quarto-appendix-contents">

<p>While geospatial context is not a part of our data model, most items in the Stadt.Geschichte.Basel collection can be associated with specific locations in Basel or elsewhere. The geographical distribution of the collection items did not influence our selection process directly, but a rough categorization was done afterwards to see whether differences in geographical scope are represented in our dataset.</p>
<table class="caption-top table">
<caption>Distribution of spatial contexts in the dataset and survey subset.</caption>
<thead>
<tr class="header">
<th style="text-align:left">Spatial Context</th>
<th style="text-align:left">Dataset</th>
<th style="text-align:left">Survey</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align:left">City of Basel</td>
<td style="text-align:left">60</td>
<td style="text-align:left">14</td>
</tr>
<tr class="even">
<td style="text-align:left">Basel Region/Northwestern Switzerland/Upper Rhine</td>
<td style="text-align:left">16</td>
<td style="text-align:left">2</td>
</tr>
<tr class="odd">
<td style="text-align:left">Switzerland</td>
<td style="text-align:left">6</td>
<td style="text-align:left">0</td>
</tr>
<tr class="even">
<td style="text-align:left">Switzerland and Neighbouring Countries</td>
<td style="text-align:left">5</td>
<td style="text-align:left">1</td>
</tr>
<tr class="odd">
<td style="text-align:left">Europe</td>
<td style="text-align:left">3</td>
<td style="text-align:left">1</td>
</tr>
<tr class="even">
<td style="text-align:left">Worldwide</td>
<td style="text-align:left">5</td>
<td style="text-align:left">2</td>
</tr>
<tr class="odd">
<td style="text-align:left">NA</td>
<td style="text-align:left">5</td>
<td style="text-align:left">0</td>
</tr>
</tbody>
</table>
</div></section><section id="media-complexity" class="level3 appendix" data-number="8.1.6"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">8.1.6</span> Media Complexity</h2><div class="quarto-appendix-contents">

<p>For technical reasons, some objects in our collection consist of several media items. These are legends for maps and diagrams, visually supplying information that is crucial to fully grasp the meaning of the media item.</p>
<table class="caption-top table">
<caption>Distribution of media complexity in the dataset and survey subset.</caption>
<thead>
<tr class="header">
<th style="text-align:left">Media Complexity</th>
<th style="text-align:left">Dataset</th>
<th style="text-align:left">Survey</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align:left">Single-Item Object</td>
<td style="text-align:left">81</td>
<td style="text-align:left">19</td>
</tr>
<tr class="even">
<td style="text-align:left">Multiple-Item Object (Figure and separate Legend)</td>
<td style="text-align:left">16</td>
<td style="text-align:left">4</td>
</tr>
</tbody>
</table>
</div></section><section id="system-performance-analysis" class="level2 appendix" data-number="8.2"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">8.2</span> System Performance Analysis</h2><div class="quarto-appendix-contents">

<div id="run_analysis_boxplots" class="anchored quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/run_analysis_boxplots.webp" class="figure-img img-fluid" width="600" alt="" loading="lazy" decoding="async" height="192" jampack-sized="true" srcset="images/run_analysis_boxplots.webp 4458w, ./images/run_analysis_boxplots@3540w.webp 3540w, ./images/run_analysis_boxplots@3240w.webp 3240w, ./images/run_analysis_boxplots@2940w.webp 2940w, ./images/run_analysis_boxplots@2640w.webp 2640w, ./images/run_analysis_boxplots@2340w.webp 2340w, ./images/run_analysis_boxplots@2040w.webp 2040w, ./images/run_analysis_boxplots@1740w.webp 1740w, ./images/run_analysis_boxplots@1440w.webp 1440w, ./images/run_analysis_boxplots@1140w.webp 1140w, ./images/run_analysis_boxplots@840w.webp 840w" sizes="100vw"></p>
<figcaption>Throughput, Latency, and Cost by Model</figcaption>
</figure>
</div>
</div></section><section id="sec-appendix-rank-distribution" class="level2 appendix" data-number="8.3"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">8.3</span> Rank Distributions and Aggregate Performance</h2><div class="quarto-appendix-contents">

<div id="rank_counts_per_model" class="anchored quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/rank_counts_per_model.webp" class="figure-img img-fluid" width="600" alt="" loading="lazy" decoding="async" height="360" jampack-sized="true" srcset="images/rank_counts_per_model.webp 2000w, ./images/rank_counts_per_model@800w.webp 800w" sizes="100vw"></p>
<figcaption>Counts of Ranks per Model (All Ratings)</figcaption>
</figure>
</div>
<div id="rank_distributions_boxplot" class="anchored quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/rank_distributions_boxplot.webp" class="figure-img img-fluid" width="600" alt="" loading="lazy" decoding="async" height="450" jampack-sized="true"></p>
<figcaption>Rank Distributions per Model (Task-Level Medians; Lower = Better)</figcaption>
</figure>
</div>
<table class="caption-top table">
<caption>Rank counts per object and model.</caption>
<colgroup>
<col style="width:16%">
<col style="width:16%">
<col style="width:16%">
<col style="width:16%">
<col style="width:16%">
<col style="width:16%">
</colgroup>
<thead>
<tr class="header">
<th>objectid</th>
<th>model</th>
<th>count_rank_1</th>
<th>count_rank_2</th>
<th>count_rank_3</th>
<th>count_rank_4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>m12965</td>
<td>google/gemini-2.5-flash-lite</td>
<td>0</td>
<td>9</td>
<td>8</td>
<td>4</td>
</tr>
<tr class="even">
<td>m12965</td>
<td>meta-llama/llama-4-maverick</td>
<td>11</td>
<td>0</td>
<td>4</td>
<td>6</td>
</tr>
<tr class="odd">
<td>m12965</td>
<td>openai/gpt-4o-mini</td>
<td>7</td>
<td>5</td>
<td>4</td>
<td>5</td>
</tr>
<tr class="even">
<td>m12965</td>
<td>qwen/qwen3-vl-8b-instruct</td>
<td>3</td>
<td>7</td>
<td>5</td>
<td>6</td>
</tr>
<tr class="odd">
<td>m13176</td>
<td>google/gemini-2.5-flash-lite</td>
<td>2</td>
<td>4</td>
<td>5</td>
<td>10</td>
</tr>
<tr class="even">
<td>m13176</td>
<td>meta-llama/llama-4-maverick</td>
<td>4</td>
<td>4</td>
<td>10</td>
<td>3</td>
</tr>
<tr class="odd">
<td>m13176</td>
<td>openai/gpt-4o-mini</td>
<td>6</td>
<td>6</td>
<td>2</td>
<td>7</td>
</tr>
<tr class="even">
<td>m13176</td>
<td>qwen/qwen3-vl-8b-instruct</td>
<td>9</td>
<td>7</td>
<td>4</td>
<td>1</td>
</tr>
<tr class="odd">
<td>m15298_1</td>
<td>google/gemini-2.5-flash-lite</td>
<td>6</td>
<td>1</td>
<td>6</td>
<td>8</td>
</tr>
<tr class="even">
<td>m15298_1</td>
<td>meta-llama/llama-4-maverick</td>
<td>2</td>
<td>11</td>
<td>7</td>
<td>1</td>
</tr>
<tr class="odd">
<td>m15298_1</td>
<td>openai/gpt-4o-mini</td>
<td>6</td>
<td>2</td>
<td>4</td>
<td>9</td>
</tr>
<tr class="even">
<td>m15298_1</td>
<td>qwen/qwen3-vl-8b-instruct</td>
<td>7</td>
<td>7</td>
<td>4</td>
<td>3</td>
</tr>
<tr class="odd">
<td>m20435</td>
<td>google/gemini-2.5-flash-lite</td>
<td>5</td>
<td>5</td>
<td>8</td>
<td>3</td>
</tr>
<tr class="even">
<td>m20435</td>
<td>meta-llama/llama-4-maverick</td>
<td>2</td>
<td>6</td>
<td>4</td>
<td>9</td>
</tr>
<tr class="odd">
<td>m20435</td>
<td>openai/gpt-4o-mini</td>
<td>3</td>
<td>4</td>
<td>6</td>
<td>8</td>
</tr>
<tr class="even">
<td>m20435</td>
<td>qwen/qwen3-vl-8b-instruct</td>
<td>11</td>
<td>6</td>
<td>3</td>
<td>1</td>
</tr>
<tr class="odd">
<td>m22924</td>
<td>google/gemini-2.5-flash-lite</td>
<td>5</td>
<td>3</td>
<td>4</td>
<td>9</td>
</tr>
<tr class="even">
<td>m22924</td>
<td>meta-llama/llama-4-maverick</td>
<td>7</td>
<td>3</td>
<td>5</td>
<td>6</td>
</tr>
<tr class="odd">
<td>m22924</td>
<td>openai/gpt-4o-mini</td>
<td>3</td>
<td>9</td>
<td>6</td>
<td>3</td>
</tr>
<tr class="even">
<td>m22924</td>
<td>qwen/qwen3-vl-8b-instruct</td>
<td>6</td>
<td>6</td>
<td>6</td>
<td>3</td>
</tr>
<tr class="odd">
<td>m28635</td>
<td>google/gemini-2.5-flash-lite</td>
<td>2</td>
<td>6</td>
<td>7</td>
<td>6</td>
</tr>
<tr class="even">
<td>m28635</td>
<td>meta-llama/llama-4-maverick</td>
<td>6</td>
<td>9</td>
<td>2</td>
<td>4</td>
</tr>
<tr class="odd">
<td>m28635</td>
<td>openai/gpt-4o-mini</td>
<td>13</td>
<td>3</td>
<td>4</td>
<td>1</td>
</tr>
<tr class="even">
<td>m28635</td>
<td>qwen/qwen3-vl-8b-instruct</td>
<td>0</td>
<td>3</td>
<td>8</td>
<td>10</td>
</tr>
<tr class="odd">
<td>m29084</td>
<td>google/gemini-2.5-flash-lite</td>
<td>2</td>
<td>9</td>
<td>10</td>
<td>0</td>
</tr>
<tr class="even">
<td>m29084</td>
<td>meta-llama/llama-4-maverick</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>15</td>
</tr>
<tr class="odd">
<td>m29084</td>
<td>openai/gpt-4o-mini</td>
<td>7</td>
<td>5</td>
<td>3</td>
<td>6</td>
</tr>
<tr class="even">
<td>m29084</td>
<td>qwen/qwen3-vl-8b-instruct</td>
<td>10</td>
<td>5</td>
<td>6</td>
<td>0</td>
</tr>
<tr class="odd">
<td>m34620</td>
<td>google/gemini-2.5-flash-lite</td>
<td>3</td>
<td>8</td>
<td>9</td>
<td>1</td>
</tr>
<tr class="even">
<td>m34620</td>
<td>meta-llama/llama-4-maverick</td>
<td>5</td>
<td>5</td>
<td>4</td>
<td>7</td>
</tr>
<tr class="odd">
<td>m34620</td>
<td>openai/gpt-4o-mini</td>
<td>4</td>
<td>5</td>
<td>3</td>
<td>9</td>
</tr>
<tr class="even">
<td>m34620</td>
<td>qwen/qwen3-vl-8b-instruct</td>
<td>9</td>
<td>3</td>
<td>5</td>
<td>4</td>
</tr>
<tr class="odd">
<td>m37030_1</td>
<td>google/gemini-2.5-flash-lite</td>
<td>7</td>
<td>6</td>
<td>5</td>
<td>3</td>
</tr>
<tr class="even">
<td>m37030_1</td>
<td>meta-llama/llama-4-maverick</td>
<td>3</td>
<td>7</td>
<td>4</td>
<td>7</td>
</tr>
<tr class="odd">
<td>m37030_1</td>
<td>openai/gpt-4o-mini</td>
<td>9</td>
<td>7</td>
<td>3</td>
<td>2</td>
</tr>
<tr class="even">
<td>m37030_1</td>
<td>qwen/qwen3-vl-8b-instruct</td>
<td>2</td>
<td>1</td>
<td>9</td>
<td>9</td>
</tr>
<tr class="odd">
<td>m37716</td>
<td>google/gemini-2.5-flash-lite</td>
<td>3</td>
<td>2</td>
<td>7</td>
<td>9</td>
</tr>
<tr class="even">
<td>m37716</td>
<td>meta-llama/llama-4-maverick</td>
<td>3</td>
<td>4</td>
<td>8</td>
<td>6</td>
</tr>
<tr class="odd">
<td>m37716</td>
<td>openai/gpt-4o-mini</td>
<td>8</td>
<td>8</td>
<td>2</td>
<td>3</td>
</tr>
<tr class="even">
<td>m37716</td>
<td>qwen/qwen3-vl-8b-instruct</td>
<td>7</td>
<td>7</td>
<td>4</td>
<td>3</td>
</tr>
<tr class="odd">
<td>m39198_1</td>
<td>google/gemini-2.5-flash-lite</td>
<td>3</td>
<td>2</td>
<td>1</td>
<td>15</td>
</tr>
<tr class="even">
<td>m39198_1</td>
<td>meta-llama/llama-4-maverick</td>
<td>4</td>
<td>5</td>
<td>9</td>
<td>3</td>
</tr>
<tr class="odd">
<td>m39198_1</td>
<td>openai/gpt-4o-mini</td>
<td>10</td>
<td>4</td>
<td>4</td>
<td>3</td>
</tr>
<tr class="even">
<td>m39198_1</td>
<td>qwen/qwen3-vl-8b-instruct</td>
<td>4</td>
<td>10</td>
<td>7</td>
<td>0</td>
</tr>
<tr class="odd">
<td>m82972</td>
<td>google/gemini-2.5-flash-lite</td>
<td>11</td>
<td>5</td>
<td>3</td>
<td>2</td>
</tr>
<tr class="even">
<td>m82972</td>
<td>meta-llama/llama-4-maverick</td>
<td>2</td>
<td>9</td>
<td>7</td>
<td>3</td>
</tr>
<tr class="odd">
<td>m82972</td>
<td>openai/gpt-4o-mini</td>
<td>5</td>
<td>5</td>
<td>4</td>
<td>7</td>
</tr>
<tr class="even">
<td>m82972</td>
<td>qwen/qwen3-vl-8b-instruct</td>
<td>3</td>
<td>2</td>
<td>7</td>
<td>9</td>
</tr>
<tr class="odd">
<td>m88415_1</td>
<td>google/gemini-2.5-flash-lite</td>
<td>1</td>
<td>3</td>
<td>1</td>
<td>16</td>
</tr>
<tr class="even">
<td>m88415_1</td>
<td>meta-llama/llama-4-maverick</td>
<td>5</td>
<td>9</td>
<td>7</td>
<td>0</td>
</tr>
<tr class="odd">
<td>m88415_1</td>
<td>openai/gpt-4o-mini</td>
<td>9</td>
<td>3</td>
<td>5</td>
<td>4</td>
</tr>
<tr class="even">
<td>m88415_1</td>
<td>qwen/qwen3-vl-8b-instruct</td>
<td>6</td>
<td>6</td>
<td>8</td>
<td>1</td>
</tr>
<tr class="odd">
<td>m91000_1</td>
<td>google/gemini-2.5-flash-lite</td>
<td>1</td>
<td>3</td>
<td>6</td>
<td>11</td>
</tr>
<tr class="even">
<td>m91000_1</td>
<td>meta-llama/llama-4-maverick</td>
<td>7</td>
<td>7</td>
<td>6</td>
<td>1</td>
</tr>
<tr class="odd">
<td>m91000_1</td>
<td>openai/gpt-4o-mini</td>
<td>5</td>
<td>6</td>
<td>2</td>
<td>8</td>
</tr>
<tr class="even">
<td>m91000_1</td>
<td>qwen/qwen3-vl-8b-instruct</td>
<td>8</td>
<td>5</td>
<td>7</td>
<td>1</td>
</tr>
<tr class="odd">
<td>m91960</td>
<td>google/gemini-2.5-flash-lite</td>
<td>2</td>
<td>4</td>
<td>8</td>
<td>7</td>
</tr>
<tr class="even">
<td>m91960</td>
<td>meta-llama/llama-4-maverick</td>
<td>10</td>
<td>6</td>
<td>4</td>
<td>1</td>
</tr>
<tr class="odd">
<td>m91960</td>
<td>openai/gpt-4o-mini</td>
<td>4</td>
<td>1</td>
<td>6</td>
<td>10</td>
</tr>
<tr class="even">
<td>m91960</td>
<td>qwen/qwen3-vl-8b-instruct</td>
<td>5</td>
<td>10</td>
<td>3</td>
<td>3</td>
</tr>
<tr class="odd">
<td>m92357</td>
<td>google/gemini-2.5-flash-lite</td>
<td>8</td>
<td>3</td>
<td>5</td>
<td>5</td>
</tr>
<tr class="even">
<td>m92357</td>
<td>meta-llama/llama-4-maverick</td>
<td>3</td>
<td>11</td>
<td>3</td>
<td>4</td>
</tr>
<tr class="odd">
<td>m92357</td>
<td>openai/gpt-4o-mini</td>
<td>5</td>
<td>4</td>
<td>7</td>
<td>5</td>
</tr>
<tr class="even">
<td>m92357</td>
<td>qwen/qwen3-vl-8b-instruct</td>
<td>5</td>
<td>3</td>
<td>6</td>
<td>7</td>
</tr>
<tr class="odd">
<td>m92410</td>
<td>google/gemini-2.5-flash-lite</td>
<td>14</td>
<td>0</td>
<td>3</td>
<td>4</td>
</tr>
<tr class="even">
<td>m92410</td>
<td>meta-llama/llama-4-maverick</td>
<td>3</td>
<td>6</td>
<td>6</td>
<td>6</td>
</tr>
<tr class="odd">
<td>m92410</td>
<td>openai/gpt-4o-mini</td>
<td>3</td>
<td>11</td>
<td>4</td>
<td>3</td>
</tr>
<tr class="even">
<td>m92410</td>
<td>qwen/qwen3-vl-8b-instruct</td>
<td>1</td>
<td>4</td>
<td>8</td>
<td>8</td>
</tr>
<tr class="odd">
<td>m94271</td>
<td>google/gemini-2.5-flash-lite</td>
<td>9</td>
<td>3</td>
<td>5</td>
<td>4</td>
</tr>
<tr class="even">
<td>m94271</td>
<td>meta-llama/llama-4-maverick</td>
<td>2</td>
<td>3</td>
<td>4</td>
<td>12</td>
</tr>
<tr class="odd">
<td>m94271</td>
<td>openai/gpt-4o-mini</td>
<td>9</td>
<td>4</td>
<td>6</td>
<td>2</td>
</tr>
<tr class="even">
<td>m94271</td>
<td>qwen/qwen3-vl-8b-instruct</td>
<td>1</td>
<td>11</td>
<td>6</td>
<td>3</td>
</tr>
<tr class="odd">
<td>m94775</td>
<td>google/gemini-2.5-flash-lite</td>
<td>0</td>
<td>2</td>
<td>2</td>
<td>17</td>
</tr>
<tr class="even">
<td>m94775</td>
<td>meta-llama/llama-4-maverick</td>
<td>4</td>
<td>5</td>
<td>9</td>
<td>3</td>
</tr>
<tr class="odd">
<td>m94775</td>
<td>openai/gpt-4o-mini</td>
<td>7</td>
<td>5</td>
<td>8</td>
<td>1</td>
</tr>
<tr class="even">
<td>m94775</td>
<td>qwen/qwen3-vl-8b-instruct</td>
<td>10</td>
<td>9</td>
<td>2</td>
<td>0</td>
</tr>
<tr class="odd">
<td>m95804</td>
<td>google/gemini-2.5-flash-lite</td>
<td>2</td>
<td>6</td>
<td>10</td>
<td>3</td>
</tr>
<tr class="even">
<td>m95804</td>
<td>meta-llama/llama-4-maverick</td>
<td>3</td>
<td>2</td>
<td>5</td>
<td>11</td>
</tr>
<tr class="odd">
<td>m95804</td>
<td>openai/gpt-4o-mini</td>
<td>9</td>
<td>4</td>
<td>1</td>
<td>7</td>
</tr>
<tr class="even">
<td>m95804</td>
<td>qwen/qwen3-vl-8b-instruct</td>
<td>7</td>
<td>9</td>
<td>5</td>
<td>0</td>
</tr>
</tbody>
</table>
<table class="caption-top table">
<caption>Ranked Friedman and Kendall’s W Test Summary</caption>
<thead>
<tr class="header">
<th>Statistic</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Friedman χ²</strong></td>
<td>6.0191</td>
</tr>
<tr class="even">
<td><strong>p-value</strong></td>
<td>0.1107</td>
</tr>
<tr class="odd">
<td><strong>Kendall’s W (observed)</strong></td>
<td>0.0085</td>
</tr>
<tr class="even">
<td><strong>Kendall’s W (from Friedman)</strong></td>
<td>0.1003</td>
</tr>
<tr class="odd">
<td><strong>Number of tasks</strong></td>
<td>20</td>
</tr>
<tr class="even">
<td><strong>Number of unique raters</strong></td>
<td>21</td>
</tr>
<tr class="odd">
<td><strong>Total submissions</strong></td>
<td>420</td>
</tr>
</tbody>
</table>
</div></section><section id="sec-appendix-pairwise-comparison" class="level2 appendix" data-number="8.4"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">8.4</span> Pairwise Comparison of Models</h2><div class="quarto-appendix-contents">

<div id="pairwise_pvalues_heatmap" class="anchored quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/pairwise_pvalues_heatmap.webp" class="figure-img img-fluid" width="600" alt="" loading="lazy" decoding="async" height="514" jampack-sized="true"></p>
<figcaption>Pairwise Adjusted p-values (Holm) — Task-Level Inference</figcaption>
</figure>
</div>
<table class="caption-top table">
<caption>Pairwise Wilcoxon Signed-Rank Tests between Models with Holm Adjustment</caption>
<colgroup>
<col style="width:20%">
<col style="width:20%">
<col style="width:20%">
<col style="width:20%">
<col style="width:20%">
</colgroup>
<thead>
<tr class="header">
<th>model_a</th>
<th>model_b</th>
<th>statistic</th>
<th>pvalue</th>
<th>p_adjusted_holm</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>google/gemini-2.5-flash-lite</td>
<td>meta-llama/llama-4-maverick</td>
<td>100.0</td>
<td>0.864524</td>
<td>1.0</td>
</tr>
<tr class="even">
<td>google/gemini-2.5-flash-lite</td>
<td>openai/gpt-4o-mini</td>
<td>59.5</td>
<td>0.088574</td>
<td>0.513834</td>
</tr>
<tr class="odd">
<td>google/gemini-2.5-flash-lite</td>
<td>qwen/qwen3-vl-8b-instruct</td>
<td>60.5</td>
<td>0.095709</td>
<td>0.513834</td>
</tr>
<tr class="even">
<td>meta-llama/llama-4-maverick</td>
<td>openai/gpt-4o-mini</td>
<td>60.0</td>
<td>0.085639</td>
<td>0.513834</td>
</tr>
<tr class="odd">
<td>meta-llama/llama-4-maverick</td>
<td>qwen/qwen3-vl-8b-instruct</td>
<td>61.5</td>
<td>0.103028</td>
<td>0.513834</td>
</tr>
<tr class="even">
<td>openai/gpt-4o-mini</td>
<td>qwen/qwen3-vl-8b-instruct</td>
<td>105.0</td>
<td>1.0</td>
<td>1.0</td>
</tr>
</tbody>
</table>
</div></section><section id="reproducibility-and-data-availability" class="level2 appendix" data-number="8.5"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">8.5</span> Reproducibility and Data Availability</h2><div class="quarto-appendix-contents">

<p>All code, datasets, and analysis artefacts supporting this study are openly available under open licenses at:</p>
<p><strong>Repository:</strong> <a href="https://github.com/maehr/chr2025-seeing-history-unseen" class="uri">https://github.com/maehr/chr2025-seeing-history-unseen</a></p>
<p><strong>Persistent record:</strong> <a href="https://doi.org/10.5281/zenodo.17639517">DOI 10.5281/zenodo.17639517</a></p>
<p>The repository provides a complete, executable research pipeline for the CHR 2025 paper <em>“Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections.”</em></p>
<p><strong>Key components:</strong></p>
<ul>
<li><code>src/</code> — source code for data generation, cleaning, and statistical analysis</li>
<li><code>runs/</code> — timestamped outputs of alt-text generation runs, including raw API responses</li>
<li><code>data/processed/</code> — anonymised survey and ranking data used for evaluation</li>
<li><code>analysis/</code> — statistical summaries, CSVs, and figures referenced in this appendix</li>
<li><code>paper/images/</code> — figure assets for the manuscript</li>
</ul>
<p><strong>Reference run:</strong> <code>runs/20251021_233530/</code> — canonical example with subsample configuration (20 media objects × 4 models). All tables and plots in this appendix derive from this run and subsequent survey analyses.</p>
<p>A pre-configured <strong>GitHub Codespace</strong> enables fully containerised reproduction without local setup. All scripts print output paths and runtime logs to ensure transparent traceability.</p>
</div></section><section id="fair-and-care-compliance" class="level2 appendix" data-number="8.6"><h2 class="anchored quarto-appendix-heading"><span class="header-section-number">8.6</span> FAIR and CARE Compliance</h2><div class="quarto-appendix-contents">

<p>The project adheres to the <strong>FAIR</strong> (Findable, Accessible, Interoperable, Reusable) and <strong>CARE</strong> (Collective Benefit, Authority to Control, Responsibility, Ethics) principles for open humanities data.</p>
<table class="caption-top table">
<colgroup>
<col style="width:31%">
<col style="width:68%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align:left">Principle</th>
<th style="text-align:left">Implementation in this project</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align:left"><strong>Findable (FAIR)</strong></td>
<td style="text-align:left">Repository indexed on GitHub and Zenodo with persistent DOI; structured metadata and semantic filenames.</td>
</tr>
<tr class="even">
<td style="text-align:left"><strong>Accessible</strong></td>
<td style="text-align:left">Publicly accessible under AGPL-3.0 (code) and CC BY 4.0 (data, documentation). No authentication barriers.</td>
</tr>
<tr class="odd">
<td style="text-align:left"><strong>Interoperable</strong></td>
<td style="text-align:left">Machine-readable CSV, JSONL, and Parquet formats; consistent column schemas; human- and machine-readable metadata.</td>
</tr>
<tr class="even">
<td style="text-align:left"><strong>Reusable</strong></td>
<td style="text-align:left">Version-controlled pipeline, deterministic random seeds, explicit dependencies, and complete provenance logs.</td>
</tr>
<tr class="odd">
<td style="text-align:left"><strong>Collective Benefit (CARE)</strong></td>
<td style="text-align:left">Focus on accessibility and inclusion in digital heritage; results aim to improve equitable access to cultural data.</td>
</tr>
<tr class="even">
<td style="text-align:left"><strong>Authority to Control</strong></td>
<td style="text-align:left">No personal or culturally sensitive material; contributors retain authorship and citation credit.</td>
</tr>
<tr class="odd">
<td style="text-align:left"><strong>Responsibility</strong></td>
<td style="text-align:left">Transparent methodological reporting and ethical safeguards for AI-assisted heritage interpretation.</td>
</tr>
<tr class="even">
<td style="text-align:left"><strong>Ethics</strong></td>
<td style="text-align:left">Evaluation limited to non-personal, publicly available heritage materials; compliance with institutional research ethics guidelines.</td>
</tr>
</tbody>
</table>
<p>Together, these practices ensure that the entire workflow—from model evaluation to figure generation—is transparent, reproducible, and reusable across digital humanities and accessibility research contexts.</p>


</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Cheaper models such as Mistral Pixtral 12B, AllenAI Molmo 7B-D, and OpenAI GPT-4.1 Nano were tested but excluded due to consistently empty or nonsensical outputs.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">window.document.addEventListener("DOMContentLoaded",function(N){const T="\uE9CB",w=new window.AnchorJS;w.options={placement:"right",icon:T},w.add(".anchored");const M=o=>{for(const t of o.classList)if(t.startsWith("code-annotation-"))return!0;return!1},b=function(o){const t=o.trigger;t.blur(),t.classList.add("code-copy-button-checked");var e=t.getAttribute("title");t.setAttribute("title","Copied!");let n;window.bootstrap&&(t.setAttribute("data-bs-toggle","tooltip"),t.setAttribute("data-bs-placement","left"),t.setAttribute("data-bs-title","Copied!"),n=new bootstrap.Tooltip(t,{trigger:"manual",customClass:"code-copy-button-tooltip",offset:[0,-8]}),n.show()),setTimeout(function(){n&&(n.hide(),t.removeAttribute("data-bs-title"),t.removeAttribute("data-bs-toggle"),t.removeAttribute("data-bs-placement")),t.setAttribute("title",e),t.classList.remove("code-copy-button-checked")},1e3),o.clearSelection()},y=function(o){const e=o.parentElement.cloneNode(!0).querySelector("code");for(const n of e.children)M(n)&&n.remove();return e.innerText};new window.ClipboardJS(".code-copy-button:not([data-in-quarto-modal])",{text:y}).on("success",b),window.document.getElementById("quarto-embedded-source-code-modal")&&new window.ClipboardJS(".code-copy-button[data-in-quarto-modal]",{text:y,container:window.document.getElementById("quarto-embedded-source-code-modal")}).on("success",b);for(var q=new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//),I=new RegExp(/^mailto:/),H=new RegExp("https://maehr.github.io/chr2025-seeing-history-unseen/"),k=o=>H.test(o)||q.test(o)||I.test(o),v=window.document.querySelectorAll("a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)"),r=0;r<v.length;r++){const o=v[r];k(o.href)||o.dataset.originalHref!==void 0&&(o.href=o.dataset.originalHref)}function p(o,t,e,n){const i={allowHTML:!0,maxWidth:500,delay:100,arrow:!1,appendTo:function(l){return l.parentElement},interactive:!0,interactiveBorder:10,theme:"quarto",placement:"bottom-start"};t&&(i.content=t),e&&(i.onTrigger=e),n&&(i.onUntrigger=n),window.tippy(o,i)}const E=window.document.querySelectorAll('a[role="doc-noteref"]');for(var r=0;r<E.length;r++){const t=E[r];p(t,function(){let e=t.getAttribute("data-footnote-href")||t.getAttribute("href");try{e=new URL(e).hash}catch{}const n=e.replace(/^#\/?/,""),i=window.document.getElementById(n);return i?i.innerHTML:""})}const A=window.document.querySelectorAll("a.quarto-xref"),g=(o,t)=>{const e=n=>{if(n.classList.remove("page-full","page-columns"),n.children)for(const i of n.children)e(i)};if(e(t),o===null||o.startsWith("sec-")){const n=document.createElement("div");if(t.children&&t.children.length>2){n.appendChild(t.children[0].cloneNode(!0));for(let i=1;i<t.children.length;i++){const l=t.children[i];if(!(l.tagName==="P"&&l.innerText==="")){n.appendChild(l.cloneNode(!0));break}}return window.Quarto?.typesetMath&&window.Quarto.typesetMath(n),n.innerHTML}else return window.Quarto?.typesetMath&&window.Quarto.typesetMath(t),t.innerHTML}else{const n=t.querySelector("a.anchorjs-link");return n&&n.remove(),window.Quarto?.typesetMath&&window.Quarto.typesetMath(t),t.classList.contains("callout")?t.outerHTML:t.innerHTML}};for(var r=0;r<A.length;r++){const t=A[r];p(t,void 0,function(e){e.disable();let n=t.getAttribute("href"),i;if(n.startsWith("#"))i=n;else try{i=new URL(n).hash}catch{}if(i){const l=i.replace(/^#\/?/,""),a=window.document.getElementById(l);if(a!==null)try{const s=g(l,a.cloneNode(!0));e.setContent(s)}finally{e.enable(),e.show()}else fetch(n.split("#")[0]).then(s=>s.text()).then(s=>{const u=new DOMParser().parseFromString(s,"text/html").getElementById(l);if(u!==null){const c=g(l,u);e.setContent(c)}}).finally(()=>{e.enable(),e.show()})}else fetch(n).then(l=>l.text()).then(l=>{const d=new DOMParser().parseFromString(l,"text/html").querySelector("main.content");if(d!==null){d.children.length>0&&d.children[0].tagName==="HEADER"&&d.children[0].remove();const f=g(null,d);e.setContent(f)}}).finally(()=>{e.enable(),e.show()})},function(e){})}let m;const B=(o,t)=>{let e='data-code-cell="'+o+'"',n='data-code-annotation="'+t+'"';return"span["+e+"]["+n+"]"},L=o=>{const t=window.document,e=o.getAttribute("data-target-cell"),n=o.getAttribute("data-target-annotation"),a=window.document.querySelector(B(e,n)).getAttribute("data-code-lines").split(",").map(u=>e+"-"+u);let s=null,d=null,f=null;if(a.length>0){const u=window.document.getElementById(a[0]);if(s=u.offsetTop,d=u.offsetHeight,f=u.parentElement.parentElement,a.length>1){const c=window.document.getElementById(a[a.length-1]);d=c.offsetTop+c.offsetHeight-s}if(s!==null&&d!==null&&f!==null){let c=window.document.getElementById("code-annotation-line-highlight");c===null&&(c=window.document.createElement("div"),c.setAttribute("id","code-annotation-line-highlight"),c.style.position="absolute",f.appendChild(c)),c.style.top=s-2+"px",c.style.height=d+4+"px",c.style.left=0;let h=window.document.getElementById("code-annotation-line-highlight-gutter");h===null&&(h=window.document.createElement("div"),h.setAttribute("id","code-annotation-line-highlight-gutter"),h.style.position="absolute",window.document.getElementById(e).querySelector(".code-annotation-gutter").appendChild(h)),h.style.top=s-2+"px",h.style.height=d+4+"px"}m=o}},C=()=>{["code-annotation-line-highlight","code-annotation-line-highlight-gutter"].forEach(t=>{const e=window.document.getElementById(t);e&&e.remove()}),m=void 0};window.addEventListener("resize",D(()=>{elRect=void 0,m&&L(m)},10));function D(o,t){let e=!1,n;return(...i)=>{e?(n&&clearTimeout(n),n=setTimeout(()=>{o.apply(this,i),n=e=!1},t)):(o.apply(this,i),e=!0)}}const R=window.document.querySelectorAll("dt[data-target-cell]");for(const o of R)o.addEventListener("click",t=>{const e=t.target;if(e!==m){C();const n=window.document.querySelector("dt[data-target-cell].code-annotation-active");n&&n.classList.remove("code-annotation-active"),L(e),e.classList.add("code-annotation-active")}else C(),e.classList.remove("code-annotation-active")});const x=o=>{const t=o.parentElement;if(t){const e=t.dataset.cites;return e?{el:o,cites:e.split(" ")}:x(o.parentElement)}else return};for(var S=window.document.querySelectorAll('a[role="doc-biblioref"]'),r=0;r<S.length;r++){const t=S[r],e=x(t);e&&p(e.el,function(){var n=window.document.createElement("div");return e.cites.forEach(function(i){var l=window.document.createElement("div");l.classList.add("hanging-indent"),l.classList.add("csl-entry");var a=window.document.getElementById("ref-"+i);a&&(l.innerHTML=a.innerHTML),n.appendChild(l)}),n.innerHTML})}});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../presentation/index.html" class="pagination-link" aria-label="Presentation">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Presentation</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../abstract/index.html" class="pagination-link" aria-label="Abstract">
        <span class="nav-page-text">Abstract</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-md-none d-sm-block"><ul><li><a href="https://github.com/maehr/chr2025-seeing-history-unseen/edit/main/paper/index.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/maehr/chr2025-seeing-history-unseen/issues/new/choose" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>