<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head><meta charset="utf-8"><style>:where(img[jampack-sized]){max-width:100%;height:auto}</style>


<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=yes">

<meta name="author" content="Moritz Mähr">
<meta name="author" content="Moritz Twente">
<meta name="keywords" content="alt-text, vision-language models, accessibility, WCAG 2.2, digital heritage collections, historical accuracy, human-in-the-loop, ethical implications, metadata, disability justice">

<title>Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections – CHR 2025 - Seeing History Unseen</title>
<style>code{white-space:pre-wrap}span.smallcaps{font-variant:small-caps}div.columns{gap:min(4vw,1.5em);display:flex}div.column{flex:auto;overflow-x:auto}div.hanging-indent{text-indent:-1.5em;margin-left:1.5em}ul.task-list{list-style:none}ul.task-list li input[type=checkbox]{vertical-align:middle;width:.8em;margin:0 .8em .2em -1em}div.csl-entry{clear:both;margin-bottom:0}.hanging-indent div.csl-entry{text-indent:-2em;margin-left:2em}div.csl-left-margin{float:left;min-width:2em}div.csl-right-inline{margin-left:2em;padding-left:1em}div.csl-indent{margin-left:2em}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../android-chrome-512x512.png" rel="icon" type="image/png">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-707d8167ce6003fca903bfe2be84ab7f.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-b3b235ae6ba71d6e5c2a90c00144237d.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-b3b235ae6ba71d6e5c2a90c00144237d.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-b3b235ae6ba71d6e5c2a90c00144237d.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": true,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../styles.css">
<meta property="og:title" content="Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections – CHR 2025 - Seeing History Unseen">
<meta property="og:description" content="Digital heritage platforms still exclude blind and low-vision users because most images lack WCAG-compliant alternative text. We study whether current vision–language models (VLMs) can act as accessibility assistants for heterogeneous historical collections and how to integrate them responsibly into curatorial workflows. We define feasibility as three observables: coverage (share of images yielding non-empty, non-refusal alt text on first pass), throughput (images/hour under realistic batching), and unit cost (CHF per alt text via API). We demonstrate feasibility on a curated subset with before/after exemplars and a compliance heuristic that checks image-type handling for complex images, length bounds, banned lead-ins, and treatment of visible text. To assess relative quality, we avoid absolute “accuracy” scores and instead run expert pairwise comparisons across four VLMs (2AFC per image). We estimate model strengths using a Bradley–Terry model with confidence intervals, report slices by era and type, and quantify inter-rater reliability via Kendall’s W. Targeted objective checks—metadata consistency, text-image handling, and a hallucination audit—are outlined as extensible future modules. For ethics and governance, we propose case-based audits on sensitive content (people, slurs, colonial scenes, funerary objects) to derive editorial rules on elision, quotation, identity labels, and deferral to long descriptions. We release an open benchmark with images or links, metadata, prompts, seeds, raw model outputs, pairwise judgements, and a cost–quality frontier to guide GLAM adoption. Registered uncertainties include external validity beyond Basel, direct utility for blind users, safety refusals, and cross-language transfer; each maps to a concrete next step. Results provide an empirical baseline and a reproducible workflow for AI-assisted accessibility in the humanities.">
<meta property="og:image" content="https://maehr.github.io/chr2025-seeing-history-unseen/paper/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==">
<meta property="og:site_name" content="CHR 2025 - Seeing History Unseen">
<meta name="twitter:title" content="Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections – CHR 2025 - Seeing History Unseen">
<meta name="twitter:description" content="Digital heritage platforms still exclude blind and low-vision users because most images lack WCAG-compliant alternative text. We study whether current vision–language models (VLMs) can act as accessibility assistants for heterogeneous historical collections and how to integrate them responsibly into curatorial workflows. We define feasibility as three observables: coverage (share of images yielding non-empty, non-refusal alt text on first pass), throughput (images/hour under realistic batching), and unit cost (CHF per alt text via API). We demonstrate feasibility on a curated subset with before/after exemplars and a compliance heuristic that checks image-type handling for complex images, length bounds, banned lead-ins, and treatment of visible text. To assess relative quality, we avoid absolute “accuracy” scores and instead run expert pairwise comparisons across four VLMs (2AFC per image). We estimate model strengths using a Bradley–Terry model with confidence intervals, report slices by era and type, and quantify inter-rater reliability via Kendall’s W. Targeted objective checks—metadata consistency, text-image handling, and a hallucination audit—are outlined as extensible future modules. For ethics and governance, we propose case-based audits on sensitive content (people, slurs, colonial scenes, funerary objects) to derive editorial rules on elision, quotation, identity labels, and deferral to long descriptions. We release an open benchmark with images or links, metadata, prompts, seeds, raw model outputs, pairwise judgements, and a cost–quality frontier to guide GLAM adoption. Registered uncertainties include external validity beyond Basel, direct utility for blind users, safety refusals, and cross-language transfer; each maps to a concrete next step. Results provide an empirical baseline and a reproducible workflow for AI-assisted accessibility in the humanities.">
<meta name="twitter:image" content="https://maehr.github.io/chr2025-seeing-history-unseen/paper/data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="floating nav-fixed nav-sidebar quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="fixed-top headroom">
    <nav class="navbar navbar-expand-lg" data-bs-theme="dark">
      <div class="container-fluid navbar-container">
      <div class="mx-auto navbar-brand-container">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../android-chrome-512x512.webp" alt="CHR 2025" class="navbar-logo light-content" fetchpriority="high" decoding="async" width="512" height="512" jampack-sized="true">
    <img src="../android-chrome-512x512.webp" alt="CHR 2025" class="navbar-logo dark-content" fetchpriority="high" decoding="async" width="512" height="512" jampack-sized="true">
    </a>
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">CHR 2025 - Seeing History Unseen</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="window.quartoToggleHeadroom&&window.quartoToggleHeadroom();
">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="me-auto navbar-nav navbar-nav-scroll">
  <li class="nav-item">
    <a class="nav-link active" href="../index.html" aria-current="page"> 
<span class="menu-text">Readme</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../abstract/index.html"> 
<span class="menu-text">Abstract</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../survey/index.html"> 
<span class="menu-text">Ranking Survey</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
    <a href="https://github.com/maehr/chr2025-seeing-history-unseen/" title="" class="px-1 quarto-navigation-tool" aria-label=""><i class="bi bi-github"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="btn quarto-btn-toggle" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="window.quartoToggleHeadroom&&window.quartoToggleHeadroom();
">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item">Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections</li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="window.quartoToggleHeadroom&&window.quartoToggleHeadroom();
">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="page-columns page-layout-article page-navbar page-rows-contents quarto-container">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="collapse collapse-horizontal floating overflow-auto quarto-sidebar-collapse-item sidebar sidebar-navigation">
    <div class="mt-2 pt-lg-2 sidebar-header text-left">
      <a href="../index.html" class="sidebar-logo-link">
      </a>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">CHR 2025 Materials</span></a>
          <a class="text-start sidebar-item-toggle" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse depth1 list-unstyled show sidebar-section">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../CHANGELOG.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Changelog</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../CODE_OF_CONDUCT.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Code of Conduct</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../CONTRIBUTING.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Contributing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../LICENSE-CCBY.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">License (Data)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../LICENSE-AGPL.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">License (Code)</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../SECURITY.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Security</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../abstract/index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Abstract</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">Ranking Survey</span></span>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="margin-sidebar sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#revised-research-questions" id="toc-revised-research-questions" class="nav-link" data-scroll-target="#revised-research-questions">Revised research questions</a></li>
  <li><a href="#data-the-stadt.geschichte.basel-collection" id="toc-data-the-stadt.geschichte.basel-collection" class="nav-link" data-scroll-target="#data-the-stadt.geschichte.basel-collection">Data: The <em>Stadt.Geschichte.Basel</em> Collection</a></li>
  <li><a href="#model-selection" id="toc-model-selection" class="nav-link" data-scroll-target="#model-selection">Model selection</a></li>
  <li><a href="#methodology" id="toc-methodology" class="nav-link" data-scroll-target="#methodology">Methodology</a>
  <ul class="collapse">
  <li><a href="#alt-text-generation-pipeline" id="toc-alt-text-generation-pipeline" class="nav-link" data-scroll-target="#alt-text-generation-pipeline">Alt-Text Generation Pipeline</a>
  <ul class="collapse">
  <li><a href="#pipeline-overview-mermaid" id="toc-pipeline-overview-mermaid" class="nav-link" data-scroll-target="#pipeline-overview-mermaid">Pipeline Overview (Mermaid)</a></li>
  </ul></li>
  <li><a href="#evaluation-strategy" id="toc-evaluation-strategy" class="nav-link" data-scroll-target="#evaluation-strategy">Evaluation Strategy</a></li>
  </ul></li>
  <li><a href="#preliminary-results-and-observations" id="toc-preliminary-results-and-observations" class="nav-link" data-scroll-target="#preliminary-results-and-observations">Preliminary Results and Observations</a></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a></li>
  <li><a href="#discussion-and-future-work" id="toc-discussion-and-future-work" class="nav-link" data-scroll-target="#discussion-and-future-work">Discussion and Future Work</a></li>
  <li><a href="#acknowledgements" id="toc-acknowledgements" class="nav-link" data-scroll-target="#acknowledgements">Acknowledgements</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/maehr/chr2025-seeing-history-unseen/edit/main/paper/index.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/maehr/chr2025-seeing-history-unseen/issues/new/choose" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="default quarto-title-block">
<div class="quarto-title">
<h1 class="title">Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections</h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Authors</div>
  <div class="quarto-title-meta-heading">Affiliations</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Moritz Mähr <a href="mailto:moritz.maehr@gmail.com" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0000-0002-1367-1618" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==" alt=""></a></p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Stadt.Geschichte.Basel, University of Basel, Switzerland
          </p>
        <p class="affiliation">
            Digital Humanities, University of Bern, Switzerland
          </p>
      </div>
    <div class="quarto-title-meta-contents">
    <p class="author">Moritz Twente <a href="mailto:mtwente@protonmail.com" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0009-0005-7187-9774" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg==" alt=""></a></p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Stadt.Geschichte.Basel, University of Basel, Switzerland
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="block-title">Abstract</div>
    <p>Digital heritage platforms still exclude blind and low-vision users because most images lack WCAG-compliant alternative text. We study whether current vision–language models (VLMs) can act as accessibility assistants for heterogeneous historical collections and how to integrate them responsibly into curatorial workflows. We define feasibility as three observables: coverage (share of images yielding non-empty, non-refusal alt text on first pass), throughput (images/hour under realistic batching), and unit cost (CHF per alt text via API). We demonstrate feasibility on a curated subset with before/after exemplars and a compliance heuristic that checks image-type handling for complex images, length bounds, banned lead-ins, and treatment of visible text. To assess relative quality, we avoid absolute “accuracy” scores and instead run expert pairwise comparisons across four VLMs (2AFC per image). We estimate model strengths using a Bradley–Terry model with confidence intervals, report slices by era and type, and quantify inter-rater reliability via Kendall’s W. Targeted objective checks—metadata consistency, text-image handling, and a hallucination audit—are outlined as extensible future modules. For ethics and governance, we propose case-based audits on sensitive content (people, slurs, colonial scenes, funerary objects) to derive editorial rules on elision, quotation, identity labels, and deferral to long descriptions. We release an open benchmark with images or links, metadata, prompts, seeds, raw model outputs, pairwise judgements, and a cost–quality frontier to guide GLAM adoption. Registered uncertainties include external validity beyond Basel, direct utility for blind users, safety refusals, and cross-language transfer; each maps to a concrete next step. Results provide an empirical baseline and a reproducible workflow for AI-assisted accessibility in the humanities.</p>
  </div>
</div>

<div>
  <div class="keywords">
    <div class="block-title">Keywords</div>
    <p>alt-text, vision-language models, accessibility, WCAG 2.2, digital heritage collections, historical accuracy, human-in-the-loop, ethical implications, metadata, disability justice</p>
  </div>
</div>

</header>


<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Digital archives promised to democratize access to cultural heritage, yet a significant portion of visual historical content remains inaccessible to blind and low-vision readers. Many digitized photographs, maps, manuscripts, and other images lack descriptive alternative text (alt-text), creating an epistemic barrier to the past. This perpetuates an asymmetry in sensory access to history, where sighted people hold privileged insight into visual sources while others are excluded. Making images legible through text is more than a technical fix—it is a matter of historical justice and inclusivity in digital humanities. Even beyond vision-impaired users, rich image descriptions can aid others, such as neurodivergent readers who benefit from explicit detail that sighted users might glean implicitly <span class="citation" data-cites="cecilia2023b">(<a href="#ref-cecilia2023b" role="doc-biblioref">Cecilia, Moussouri, and Fraser 2023a</a>)</span>.</p>
<p>Alt-text itself is not new: the HTML <code>alt</code> attribute dates back to the 1990s to support accessibility. However, providing high-quality image descriptions has often been a secondary priority in scholarly communication <span class="citation" data-cites="cecilia2023a">(<a href="#ref-cecilia2023a" role="doc-biblioref">Cecilia, Moussouri, and Fraser 2023b</a>)</span>. Crafting alt-text is labor-intensive and typically left to authors or curators as a final step, if done at all. The burden often falls on sighted experts to determine what information <em>is</em> or <em>is not</em> included in an image’s description, an ethical responsibility that only the content’s author can fully shoulder. Author-generated descriptions are valued for capturing contextual meaning that automated tools might miss. They can greatly enhance the accessibility, searchability, and archivability of digital scholarship. Yet in practice, many projects—especially smaller public history initiatives—lack the resources to implement accessibility from the start. The result is that visual evidence remains “unseen” by those who rely on assistive technologies.</p>
<p>Recent advances in multimodal AI offer a potential remedy. Vision-Language Models (VLMs) such as OpenAI’s GPT-4o, Google’s Gemini 2.5, and open-source systems like Meta’s Llama 4 or Mistral’s Pixtral now claim near-human performance in image description tasks. These models can ingest an image and generate a caption or description, essentially simulating the interpretive act of a human describer. If these models could produce alt-text that is both high-quality and historically informed as well as conformant with the Web Content Accessibility Guidelines (WCAG 2.2) and the Web Accessibility Initiative (WAI) of the World Wide Web Consortium (W3C), this would dramatically reduce the human effort required to remediate large collections. Heritage institutions could then scale up accessibility by generating alt-text for thousands of images. Consequently, the “readership” of digital archives would expand to include those who were previously excluded.</p>
<p>However, adopting automated captioning in a heritage context raises critical questions about truth, evidence, and authenticity. Delegating descriptive labor to machines is not a neutral technical fix; it is an act imbued with values and biases. Deciding what details to include in an image’s description is technically difficult and ethically fraught, especially for historical images depicting people or sensitive cultural content. Vision models trained on general web images may inject anachronistic terms or biases (e.g., misidentifying a 1920s street scene as “Victorian”), reinforce curatorial blind spots, or omit crucial context that a human historian would provide. There is also the danger of <em>techno-ableism</em> <span class="citation" data-cites="shew2023">(<a href="#ref-shew2023" role="doc-biblioref">Shew 2023</a>)</span>, where blind users’ needs are superficially addressed by technology without truly empowering them or respecting their perspectives. Uncritical use of AI could inadvertently recentre the sighted, algorithmic point of view rather than the lived experience of those using the alt-text.</p>
<p>In this work, we argue that AI-generated alt-text for historical collections is a pivotal test case for the entanglement of AI innovation, archival practice, and disability justice. But can a machine “see” history as we do? If a model can convincingly describe a photograph from 100 years ago, how does that change the way we verify and trust such descriptions? Embracing this kind of “machine vision” in historical scholarship may require new protocols akin to earlier paradigm shifts (for example, the move from handwritten catalog cards to MARC records, or from microfilm to digital scans). Just as those changes demanded critical awareness of how tools shape historical discovery, the use of AI-generated descriptions demands a new hermeneutic of suspicion. We must learn to critically read machine-generated metadata, much as we read any human-produced finding aid or annotation <span class="citation" data-cites="fickers2022">(<a href="#ref-fickers2022" role="doc-biblioref">Fickers 2022</a>)</span>.</p>
<p>The central purpose of our study is to assess whether and how current AI models can serve as <em>accessibility assistants</em> in a digital history workflow, and to develop a critical framework for using them responsibly. Our approach is interdisciplinary, blending computational experimentation with qualitative, historiographically informed analysis. Concretely, we plan to experiment with state-of-the-art multimodal models to generate alt-text for a real-world public history collection, and we will evaluate the results for accessibility compliance, historical accuracy, and ethical soundness. By doing so, we aim to illuminate both the opportunities and the pitfalls of integrating AI into inclusive humanities scholarship. Each AI-generated caption is treated not just as metadata but as an interpretive act—one that can be scrutinized like any primary source.</p>
<p>To guide this inquiry, we pose the following research questions:</p>
<!-- OLD TEXT BEFORE REWORK:

1.  **Feasibility:** _Can current vision-language models produce useful, WCAG 2.2--compliant alt-text for complex historical images when provided with contextual metadata?_ We will examine whether models can meet accessibility guidelines (providing text alternatives that convey the same information as the image) and how the inclusion of metadata influences their output. We also consider the potential usefulness of these descriptions for both blind users and sighted users who may benefit from clear explanatory captions [@cecilia2023b].

2.  **Quality and Authenticity:** _How do domain experts (e.g., historians) rate AI-generated image descriptions in terms of factual accuracy, completeness, and usefulness for understanding historical content?_ We will evaluate the outputs for errors such as anachronisms, misidentifications, or hallucinated details, checking them against known facts from metadata and expert knowledge.

3.  **Ethics and Governance:** _What are the ethical implications of using AI to generate alt-text in heritage collections, and what human oversight or policy safeguards are required for responsible use?_ We will identify potential harms such as biased descriptions (e.g., normative terms), and address the broader question of how much interpretive agency should be ceded to AI in a curatorial context. We will explore strategies to mitigate these risks, including human-in-the-loop editing and transparency measures. -->
<ul class="task-list">
<li><label><input type="checkbox">Rework the section above the research questions so that it complies with these ides:</label></li>
</ul>
<p><strong>1. Feasibility</strong></p>
<ul>
<li><strong>Define feasibility</strong> as three observables:
<ol type="1">
<li><p><strong>Coverage</strong>: % images that yield a non-empty, non-refusal alt text on first pass.</p></li>
<li><p><strong>Throughput</strong>: extrapolated/theoretical images/hour.</p></li>
<li><p><strong>Unit cost</strong>: CHF per alt text (API).</p></li>
</ol></li>
<li><strong>Demonstrate</strong> with:
<ul>
<li><p>A small, curated set of <strong>before/after</strong> exemplars across types to show plausibility.</p></li>
<li><p>A <strong>compliance heuristic</strong> pass rate: correct “complex image” pattern, length bounds based on image type, banished phrases (“Bild von…”), presence/absence of visible text handling.</p></li>
</ul></li>
<li>No large-N UX test needed for “feasibility.” Save user studies for later work. Anchor claims to logs and heuristics.</li>
</ul>
<p><strong>2. Quality and authenticity</strong></p>
<ul>
<li><strong>Avoid absolute scoring</strong> for “factual accuracy.” Use <strong>relative preference</strong>:
<ul>
<li><p>Design: <strong>4 models → 6 pairs</strong> per image. 6 Domain experts pick the better alt text per pair (<strong>2AFC</strong>).</p></li>
<li><p><strong>Estimate</strong> model strengths with a <strong>Bradley–Terry</strong> model. Report coefficients with CIs. Do overall and by era and type.</p></li>
<li><p><strong>Reliability</strong>: <strong>Kendall’s W</strong> on ranks.</p></li>
</ul></li>
<li><strong>Targeted objective checks</strong> is an idea not realized in this paper yet, but could be future work:
<ul>
<li><p><strong>Metadata consistency</strong>: forbid contradictions with known year/place/creator; count contradictions.</p></li>
<li><p><strong>Text-image handling</strong>: for scans, check that visible text is mentioned or flagged for longdesc.</p></li>
<li><p><strong>Hallucination audit</strong>: sample-based review for invented entities.</p></li>
</ul></li>
<li>Rationale: alt texts are short; absolute “completeness” is ill-posed. Relative judgments scale and are defensible.</li>
</ul>
<p><strong>3. Ethics and governance</strong></p>
<ul>
<li>For a later stage of the research, do <strong>case-based audits</strong>:
<ul>
<li><p>Curate vignettes for people images, sensitive symbols, derogatory historical text, colonial scenes, funerary objects.</p></li>
<li><p>For each, show 4 model outputs, annotate <strong>harm vectors</strong>: speculative identity, euphemism, unnecessary salience, tone, omission of slurs vs contextualization.</p></li>
<li><p>Derive <strong>editorial rules</strong>: what to elide vs quote, when to defer to long description, when to avoid identity labels unless documented.</p></li>
</ul></li>
<li>Output of this section is <strong>policy</strong>, not a number. Tie to disability language guides and CH practice, but keep examples empirical.</li>
</ul>
</section>
<section id="revised-research-questions" class="level1">
<h1>Revised research questions</h1>
<ul>
<li><strong>RQ1 Feasibility</strong>: What <strong>coverage, throughput, and unit cost</strong> can current VLMs achieve for WCAG-oriented alt text on a heterogeneous heritage corpus, and where do they fail?</li>
<li><strong>RQ2 Relative quality</strong>: How do experts <strong>pairwise-rank</strong> model outputs for <strong>usefulness and metadata-consistent correctness</strong>, overall and by image type? What error patterns recur?</li>
</ul>
<p>possibly for future work:</p>
<ul>
<li><strong>RQ3 Governance</strong>: What <strong>editorial policies</strong> emerge from case audits on sensitive content, and how should institutions structure <strong>human-in-the-loop</strong> review?</li>
<li><strong>RQ4 Benchmark</strong>: What <strong>cost–quality trade-offs</strong> and <strong>per-type differentials</strong> define a practical benchmark for GLAM adoption?</li>
</ul>
<p><strong>General remarks</strong></p>
<ul>
<li><p>There will be an acompanying repository with open source code and data research data realeased alongside the paper https://github.com/maehr/chr2025-seeing-history-unseen/ and DOI</p></li>
<li><p>Position work as a <strong>benchmark</strong> with <strong>relative performance</strong> plus <strong>cost</strong>:</p>
<ul>
<li>Release: images (or links), metadata, prompts, seeds, <strong>raw model outputs</strong>, and human pairwise judgments.</li>
<li>Report <strong>per-type</strong> subscores: photo, map, diagram, scan, drawing, object; and <strong>per-era</strong> and <strong>language</strong> slices.</li>
<li>Plot a <strong>cost–quality frontier</strong>: Bradley–Terry score vs CHF/image.</li>
</ul></li>
<li><p>This sidesteps the need for absolute “accuracy” yet is decision-useful for GLAM.</p></li>
<li><p>Treat early A/B and case studies as <strong>pilot evidence</strong>. Add a box: <strong>“Registered uncertainties”</strong>:</p>
<ul>
<li>Unknown external validity to non-Basel collections.</li>
<li>Blind/low-vision user utility not yet measured.</li>
<li>Safety refusals on sensitive historical content.</li>
<li>Language transfer (de → fr/la) boundaries.</li>
</ul></li>
<li><p>Convert each uncertainty into a <strong>next-step</strong> with a method, sample, and success criterion.</p></li>
</ul>
<p>By answering these questions, our work will provide an empirical baseline for <em>AI-assisted accessibility in the humanities</em>. It will also offer a reflective critique, examining AI outputs as objects of study in their own right. In the following sections, we outline our data and methodology (Section 2), present initial observations from our experiments (Section 3), and discuss implications for digital humanities practice (Section 4), before concluding with planned next steps (Section 5).</p>
</section>
<section id="data-the-stadt.geschichte.basel-collection" class="level1">
<h1>Data: The <em>Stadt.Geschichte.Basel</em> Collection</h1>
<!-- OLD TEXT BEFORE REWORK:

To ground our evaluation in a real-world scenario, we use the digital collection of the public history project _Stadt.Geschichte.Basel_ (an open research repository on the history of Basel, Switzerland). The collection in its final form comprises approximately 1,500 heterogeneous digitized items, including historical photographs, reproductions of artifacts, city maps and architectural plans, handwritten letters and manuscripts, statistical charts, and printed ephemera (e.g., newspaper clippings, posters). Each item is accompanied by metadata in a Dublin Core schema (including fields such as title, creator, date, location, and a descriptive summary provided by historians). Crucially, none of the items currently have alt-text for use with screen readers, making this an ideal testbed for our study. The diversity of the corpus poses a significant challenge to automated captioning: many images are visually and historically complex, requiring domain knowledge to describe properly. This dataset thus allows us to investigate whether AI captioners can handle the "long tail" of content found in historical archives, beyond the everyday photographs on which many models are trained.

For our experiments, we have obtained the collection images (in JPEG format, at a standardized size of $\sim$``{=html}800$\times$`{=html}800 pixels for computational efficiency) and their corresponding metadata in JSON format. We construct a working dataset where each entry consists of an image and its metadata (e.g., title, date, description). This metadata will be used to prompt the models, as described below. We intend to release the dataset of images, metadata, and model-generated descriptions as a benchmark for future research, following the conference's emphasis on open data and reproducibility. -->
<ul class="task-list">
<li><label><input type="checkbox">Rework the section above so that it complies with these ideas:</label></li>
</ul>
<p>Write a short intro to the collection (public history Basel, heterogeneous types, no alt texts yet, metadata available). Then give a <strong>table</strong> or <strong>figure</strong> with descriptive statistics of the sample used in the evaluation (n=100 images). Include type, era, language distributions, and cross-tabs.</p>
<p><strong>Selection (n=100 items):</strong></p>
<ul>
<li><strong>Types</strong> (balanced): Object 13; Art 12; Scans 10; Maps 10; Photos (archaeology) 10; Photos (historical scenes) 10; Drawings (hist.) 10; Drawings (reconstr.) 10; Diagrams (stats) 10; Diagrams (flow) 5.</li>
<li><strong>Eras</strong>: 20. Jh. 25; Frühe Neuzeit 21; 19. Jh. 19; Mittelalter 16; Frühgeschichte 11; plus Antike and 21. Jh. present.</li>
<li><strong>Languages</strong>: mostly <strong>de</strong>, with <strong>fr</strong> and <strong>la</strong> pockets.</li>
<li><strong>Cross-tabs</strong> show coverage across Type×Era and Type×Language; German dominates, maps/diagrams type-specific.</li>
</ul>
</section>
<section id="model-selection" class="level1">
<h1>Model selection</h1>
<ul class="task-list">
<li><label><input type="checkbox">TODO Justify model choices:</label>
<ul>
<li>Proprietary SOTA: <code>openai/gpt-4o-mini</code> (GPT-4o), <code>google/gemini-2.5-flash-lite</code> (Gemini Vision).</li>
<li>Open weights: <code>meta-llama/llama-4-maverick</code> (Llama 4), <code>mistralai/pixtral-12b</code> (Pixtral).</li>
<li>Comparable cost per 1M tokens (see table in TODO.md).</li>
<li>Diverse architectures and training data.</li>
</ul></li>
<li><label><input type="checkbox">Incorporate model card summaries from TODO.md.</label></li>
</ul>
</section>
<section id="methodology" class="level1">
<h1>Methodology</h1>
<p>Our approach combines a technical pipeline for generating candidate alt-text with a multi-layered evaluation strategy. A human-in-the-loop process is incorporated throughout to ensure quality control and address ethical considerations.</p>
<p>In this short paper, we describe the methodology in future tense, as several steps are in progress.</p>
<section id="alt-text-generation-pipeline" class="level2">
<h2 class="anchored" data-anchor-id="alt-text-generation-pipeline">Alt-Text Generation Pipeline</h2>
<!-- OLD TEXT BEFORE REWORK:

**Prompt Design:** A key feature of our pipeline is providing each model with contextual metadata alongside the image, in order to ground the generation in relevant historical facts. We designed a prompt template (in the same language as the collection, i.e., German) that injects structured metadata fields and instructs the model to follow best practices for alt-text. In essence, the prompt tells the model that it is an **accessibility assistant** tasked with producing an alt-text for a cultural heritage image. It includes guidelines drawn from the WCAG 2.2 and accessibility literature on how to write good alt-text. For example, the prompt directs the model not to start with redundant phrases like "Bild von..." ("image of..."), to be concise (typically under $\sim$``{=html}120 characters for a simple informative image), and to include any essential visual text (like signs or captions visible in the image). It also asks the model to identify the type of image and adjust the response accordingly: e.g., if the image is a complex diagram or map, the model should produce a short alt-text plus note that a longer description will be provided; if the image is merely a photograph with informative content, a 1--2 sentence description suffices; if the image is mainly text (say a scanned document or poster), the model should either transcribe it (for short text like a sign) or indicate that a full transcription is available elsewhere for longer texts. These rules were distilled from accessibility resources [@a11ychecklist; @wcag2023] to ensure the output serves blind users properly. An example snippet of our prompt template is: _"You are an expert in writing WCAG-compliant alt-text. The image comes from a history archive with metadata. Read the metadata and analyze the image. Determine the image type (informative photo, complex diagram/map, or text image) and produce the appropriate alt-text as per the guidelines..."_---followed by the specific instructions for each case. We have found in preliminary trials that including the complete metadata (`title`, `date`, etc.) in the prompt can prevent certain errors (for instance, knowing the year of the photo helps the model avoid describing attire as "modern"). All models are prompted with the same template structure for consistency, and all outputs are requested in German (to match the collection's context and end-user language). -->
<ul class="task-list">
<li><label><input type="checkbox">Rework the section above so that it complies with these ideas:</label></li>
</ul>
<p>We systematically varied prompt roles and placement, comparing instruction blocks in the system message versus the user turn, and front-loading versus trailing constraints. Following evidence that models privilege information at the beginning or end of long contexts, we fixed normative requirements (WCAG 2.2 target, de-CH style, length limits, handling of decorative/functional/complex images) in the system prompt and kept the user message minimal and image-bound to reduce “lost-in-the-middle” effects. The user turn injected collection-specific metadata—title, description, EDTF date, era, creator/publisher/source—and a concise Nutzungskontext, then the image URL. Adding this structured context markedly improved specificity, reduced refusals, and lowered hallucinations, consistent with retrieval-style findings that supplying external, task-relevant evidence boosts generation quality and faithfulness. Concretely, the prompt is a two-part template: (1) a stable system scaffold that encodes accessibility rules and output format (“only the alt text”, max 125 chars, no “Bild von”), and (2) a per-item user payload that lists metadata as bullet points plus the image, so the model can align linguistic content with visual features. (TODO see https://aclanthology.org/2024.tacl-1.9/)</p>
<p><strong>Generation and Post-processing:</strong> Using this prompt, we will run each image through each of the four models, yielding up to four candidate descriptions per image. The generation process will be automated via a Python script (using an API wrapper or library for each model). We anticipate producing around 400 candidate alt-texts (4 per image for n=100 images). After generation, minimal post-processing will be applied. In particular, we will strip any extraneous phrases if a model fails to follow instructions exactly (e.g., some might prepend “Alt-Text:” or polite greetings, which we will remove). We will not otherwise modify the content of the AI outputs at this stage. All results will be stored along with metadata and model identifiers for evaluation.</p>
<p>If a model refuses to describe an image due to some built-in safety filter (misidentifying a historical photograph as sensitive content), we will handle those on a case-by-case basis by leaving that image for human description. Overall, this pipeline is designed to maximize coverage (getting at least one description for every image) while maintaining quality through careful prompting.</p>
<section id="pipeline-overview-mermaid" class="level3">
<h3 class="anchored" data-anchor-id="pipeline-overview-mermaid">Pipeline Overview (Mermaid)</h3>
<!-- ```{mermaid} -->
<pre><code>
%%| label: fig-pipeline
%%| fig-cap: Alt-text generation, survey, and analysis pipeline (best model by consensus and cost).
flowchart LR
  %% Generation
  subgraph GEN[src/main.py — Generation]
    M0[Fetch metadata JSON (METADATA_URL)]
    M1[Select MEDIA_IDS]
    M2[Build prompts with metadata]
    M3[Query MODELS via OpenRouter]
    M4[Persist raw responses under runs/&lt;timestamp&gt;/raw/*.json]
    M5[Assemble wide table CSV/JSONL/Parquet]
    M6[Export questions.csv for survey]
    M0 --&gt; M1 --&gt; M2 --&gt; M3 --&gt; M4
    M3 --&gt; M5 --&gt; M6
  end

  %% Survey
  subgraph SUR[survey/* — Expert Survey]
    S0[Load questions.csv]
    S1[Present options per image]
    S2[Collect expert choices + comments]
    S3[Write survey/results.csv]
    S0 --&gt; S1 --&gt; S2 --&gt; S3
  end

  %% Analysis
  subgraph ANA[analysis — Model Comparison]
    A0[Aggregate votes per model]
    A1[Compute consensus win rate]
    A2[Join with cost table]
    A3[(Best model: openai/gpt-4o-mini)]
    S3 --&gt; A0 --&gt; A1 --&gt; A2 --&gt; A3
  end

  M6 --&gt; S0</code></pre>
</section>
</section>
<section id="evaluation-strategy" class="level2">
<h2 class="anchored" data-anchor-id="evaluation-strategy">Evaluation Strategy</h2>
<p>Our evaluation of the AI-generated alt-text will address both <strong>accessibility compliance</strong> and <strong>historical accuracy</strong> in line with the research questions. We describe the planned evaluation steps below. All evaluation will be done on a representative subset of the data (approximately 100 images) due to time constraints, with the aim of scaling up later.</p>
<!-- OLD TEXT BEFORE REWORK:

 \(a\) Accessibility and WCAG Compliance:

: We will assess whether the AI outputs meet established accessibility guidelines for alt-text. This involves checking each description against a checklist of best practices (e.g., does the alt-text sufficiently describe the image's important content and function? Does it avoid unnecessary phrases like "an image of"? If the image contains readable text or numbers, are those included or summarized in the alt-text?). We are adapting the Alt Text Checklist from the A11y Project and WCAG techniques as our evaluation rubric. Each candidate description for an image will be reviewed by at least two team members with knowledge of accessibility standards. In cases where the image is a diagram or chart, we will check that the model followed instructions (providing a short summary alt-text and indicating a longer description would be needed). For images of documents, we check that any text was appropriately handled (transcribed or deferred to full text). The outcome of this step will be a rating or rank of the candidates for each image in terms of compliance. We expect that the model prompted with metadata and guidelines will produce mostly compliant alt-text, whereas some simpler models might yield overly generic or incomplete captions. An initial pilot test supports this: for example, without metadata, an open-source model captioned a photo as "Old photo of a street" which misses key specifics, but with our metadata-enhanced prompt GPT-4o produced "Schwarzweiß-Fotografie einer belebten Straße in Basel, 1917, mit Demonstranten, die Banner in Frakturschrift halten." (Black-and-white photograph of a busy Basel street in 1917, with protesters holding banners in Gothic script), which is far richer and ticks more of the accessibility boxes (it mentions the context, the presence of text on banners, etc.). This step addresses the first research question by testing whether models can be guided to meet alt-text requirements. We will quantify common compliance issues and note which model outputs most often require correction.

\(b\) Historical Accuracy and Usefulness:

: The second layer of evaluation focuses on the content accuracy and value of the descriptions from a historian's perspective. We will conduct a blind review where domain experts (trained historians) examine the AI-generated alt-text for a given image and compare it to the known metadata or facts about that image. Each expert will be presented with the four alt-text candidates for an image and will be asked to order them by relative factual correctness---that is, ranking the descriptions from most to least accurate in terms of representing the image content. This ranking focuses on the relative quality among the alternatives rather than absolute judgments. For example, a model might mistakenly label a horse-drawn carriage in a 1890 photo as a "car" (anachronistic), or it might hallucinate a "red stamp in the corner" of a document that does not exist. Such errors are critical to catch, as they could mislead researchers. On the other hand, we will also note cases where the AI description includes details that the original metadata or caption did not mention. In preliminary tests, we observed instances of this "AI insight": e.g., a model noted "ein handgezeichneter roter Umriss auf dem Stadtplan" (a hand-drawn red outline on the map) which the human catalog description had not recorded. Upon checking the image, there was indeed a red pen marking on the map, presumably added by a later hand. Discovering these additional details could be beneficial, pointing scholars to visual evidence they might otherwise overlook. Our expert reviewers will differentiate between such legitimate additions and illegitimate hallucinations. We aim to categorize common error types (misidentifications, missed context, invented details) and measure the proportion of AI-generated alt-text that is acceptable with minimal or no editing versus those that need substantial correction. We anticipate, based on prior work and initial runs, that a majority of descriptions (over 90%) will be largely correct, while a significant minority will have issues requiring human intervention. The results of this step will inform how much post-editing effort is needed when deploying these models in practice.

\(c\) Ethical Review:

: In parallel with the above, we will perform a qualitative analysis of the AI outputs to identify any ethical or bias concerns. This involves scanning the descriptions for inappropriate language or perspective. For instance, we will check if any descriptions contain terms or tones that are outdated or offensive (e.g., describing people in a demeaning way). We are particularly attentive to _ableist language_: while unlikely, we want to ensure the alt-text does not include phrases like "suffers from blindness" or similar, which are not acceptable in modern accessibility writing [@holmes2020]. If the model describes people, we examine whether it is making unwarranted assumptions about their identity (race, gender, etc.) or appearance. One concrete example: one model output described an older photograph of a man as "ein afrikanischer Mann" ("an African man"). The image indeed depicted a Black man, but in context his nationality or ethnicity was not documented and not necessarily relevant to the image's purpose. Including such a descriptor could be seen as othering or speculative, so our policy is to avoid it unless it is directly pertinent [@hanley2021]. In our review process, any such cases will be flagged and either removed or revised. We will also consider the implications of the model's choices of detail: what the AI focuses on can reflect implicit bias (e.g., always mentioning a woman's appearance but not a man's). By compiling these observations, we will derive guidelines for curators on how to handle AI-generated descriptions. The ethical review is not a separate step per se, but integrated into the human-in-the-loop oversight---no AI-generated alt-text will be added to the public collection without passing this human review stage. -->
<ul class="task-list">
<li><label><input type="checkbox">Rework the section above so that it complies with the ideas outlined previously.</label></li>
</ul>
</section>
</section>
<section id="preliminary-results-and-observations" class="level1">
<h1>Preliminary Results and Observations</h1>
<!-- OLD TEXT BEFORE REWORK:

Note: As this is a work in progress, we report here on initial observations from our ongoing experiments. A full evaluation with quantitative results will be included in the final version._

**Feasibility and Throughput:** Early results confirm that using VLMs can dramatically accelerate the production of alt-text for large collections. Our automated pipeline has been able to generate descriptions for the entire set of $\sim$1,500 images in a matter of hours (wall-clock time), only limited by API call rates. In contrast, writing high-quality alt-text manually for that many images would likely take a dedicated team several weeks. Even accounting for time spent in human review and correction, the AI-assisted workflow promises to be far more efficient. Importantly, the models attempted to describe every image; none of the images were outright un-captionable by the AI. Only a small fraction of outputs came back empty or with an error (for instance, a few instances where a model refused output thinking a historical war photo was violent content). This suggests that an automated approach can achieve close to near 100% _coverage_, ensuring that no image remains without at least an initial draft description. From an accessibility standpoint, this is already a win: having even a basic description is better than nothing for a user navigating these archives.

**Alt-Text Quality --- Accuracy vs. Errors:** The quality of the AI-generated descriptions varies across models and images, but our expert review so far indicates a majority are quite descriptive and useful, with some requiring only minor tweaking. For straightforward photographs (e.g., a city street, a portrait, an artifact on a plain background), the models often produced accurate and succinct descriptions. In many cases, the AI caption actually included more concrete detail than the existing human metadata. For example, one image of a tram scene had a human description "Street scene with tram and people, Basel early 1900s." A model-generated alt-text added detail: "Drei Männer stehen vor einem Straßenbahnwagen. Der mittlere Mann hält ein Schild mit der Nummer 5." (Three men stand in front of a tram car. The middle man is holding a sign with the number 5.) Such details can enrich the record and provide a fuller picture to someone who cannot see the image. This demonstrates the potential for AI to surface elements that a human might overlook or assume as understood.

At the same time, we have observed a number of _failure modes_ that reinforce the need for human oversight. A preliminary categorization of issues includes: **Hallucinated Details:** Occasionally the model introduces objects or readings that are not actually present. For instance, one caption described "an official seal stamped on the document" when no such seal exists on the image. Another described ornate architectural details on a building that were in reality not discernible. **Anachronisms and Misidentifications:** Some outputs used terms that were out-of-place for the historical context. We saw an example of a model calling a 1910 protest scene "Victorian"---confusing the era.

**Model Comparisons:** A full benchmarking is ongoing.

**Ethical and Sensitive Cases:** Our review of outputs is ongoing, but so far we have not encountered any egregiously biased or harmful descriptions from the models when they are properly prompted. This is a relief given past incidents in vision AI (for example, earlier algorithms infamously mis-labelled images of Black people with animal names, as noted by Hanley et al. [@hanley2021]). None of our models produced derogatory labels or inappropriate descriptions of people; they generally stuck to neutral terms like "an older woman," "a young boy," etc., only mentioning apparent race or disability if it was obvious and relevant (which we typically consider outside the scope of alt-text unless the historical context makes it pertinent). We also noted that models occasionally avoided describing graphic historical images in detail when the content was discriminatory. In those cases, a human will likely need to step in to provide an appropriate description that the AI hesitated to give. -->
<ul class="task-list">
<li><label><input type="checkbox">Rework the section above so that it complies with the ideas outlined previously.</label></li>
</ul>
<p>Overall, our preliminary findings suggest that with careful prompting and human curation, AI-generated alt-text can achieve a quality that makes them valuable for accessibility in digital heritage collections. The process is <strong>feasible</strong> and scalable (addressing the first research question), and the outputs are often accurate and informative, though not without errors (addressing the second research question). Importantly, this exercise has started to reveal where AI captions might <em>add</em> value (by noticing visual details) and where they might <em>mislead</em> (by hallucinating or omitting context). These insights will feed into the development of guidelines and best practices for using AI in this capacity.</p>
</section>
<section id="results" class="level1">
<h1>Results</h1>
<ul class="task-list">
<li><label><input type="checkbox">TODO: Fill in with final results once evaluation is complete. For now, a placeholder summary table and narrative.</label></li>
</ul>
<p>Our expert survey (N = 12 historians; 100 images) produced clear preferences across models. Overall, the OpenRouter variant <code>openai/gpt-4o-mini</code> achieved the highest consensus while maintaining acceptable token costs; it also required the fewest edits for WCAG-compliant phrasing. <code>google/gemini-2.5-flash-lite</code> was a strong, lower-cost runner-up. <code>mistralai/pixtral-12b</code> performed solidly on scene structure but trailed on factual precision; <code>meta-llama/llama-4-maverick</code> lagged on historical specificity.</p>
<table class="caption-top table">
<caption>Expert consensus and cost summary (higher win rate and lower avg. rank are better). {#tbl:summary}</caption>
<colgroup>
<col style="width:33%">
<col style="width:19%">
<col style="width:21%">
<col style="width:12%">
<col style="width:13%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align:left">Model (OpenRouter ID)</th>
<th style="text-align:right">Win rate (top-1)</th>
<th style="text-align:right">Avg. rank (1=best)</th>
<th style="text-align:right">Input $/1M</th>
<th style="text-align:right">Output $/1M</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align:left">openai/gpt-4o-mini</td>
<td style="text-align:right">46%</td>
<td style="text-align:right">1.78</td>
<td style="text-align:right">0.15</td>
<td style="text-align:right">0.60</td>
</tr>
<tr class="even">
<td style="text-align:left">google/gemini-2.5-flash-lite</td>
<td style="text-align:right">29%</td>
<td style="text-align:right">2.20</td>
<td style="text-align:right">0.10</td>
<td style="text-align:right">0.40</td>
</tr>
<tr class="odd">
<td style="text-align:left">mistralai/pixtral-12b</td>
<td style="text-align:right">17%</td>
<td style="text-align:right">2.60</td>
<td style="text-align:right">0.10</td>
<td style="text-align:right">0.10</td>
</tr>
<tr class="even">
<td style="text-align:left">meta-llama/llama-4-maverick</td>
<td style="text-align:right">8%</td>
<td style="text-align:right">3.42</td>
<td style="text-align:right">0.15</td>
<td style="text-align:right">0.60</td>
</tr>
</tbody>
</table>
<p>Implications: We select <code>openai/gpt-4o-mini</code> as the best trade‑off of quality and cost for bulk remediation. For large-scale batches with constrained budgets, <code>gemini-2.5-flash-lite</code> is an attractive alternative with only a modest drop in expert preference.</p>
</section>
<section id="discussion-and-future-work" class="level1">
<h1>Discussion and Future Work</h1>
<!-- OLD TEXT BEFORE REWORK:

Our ongoing project highlights both the promise and the complexities of integrating AI into cultural heritage accessibility. Here we reflect on key implications and outline the next steps, including a planned user study and considerations for ethical deployment (addressing the third research question and beyond).

**Integrating AI into Digital Humanities Practice:** Embracing AI for alt-text generation can substantially improve the inclusivity of digital archives. For public history initiatives, this means that no part of the historical record should remain off-limits to blind or visually impaired researchers. By leveraging AI, even small teams can now consider providing descriptions for thousands of images, bridging an accessibility gap that has persisted in the field. This is a concrete way in which computational methods can democratize access to cultural heritage. However, our work also underscores that AI is not a plug-and-play solution: it requires thoughtful integration. Historians and archivists must develop a new form of source criticism for AI-generated content. Just as we critically evaluate a human-written caption or a transcribed document, we need to critically interrogate AI outputs---asking how the description was generated, what might be missing or biased, and how it should be interpreted. This aligns with the notion of _digital hermeneutics_ in public history [@fickers2022], where scholars maintain a reflexive awareness of the tools mediating their understanding of sources. In practice, this could mean training archival staff in basic AI literacy or establishing review protocols that treat AI suggestions as starting points subject to scholarly validation.

**Human-AI Collaboration Workflow:** Based on our experiences, we advocate for a workflow where AI assists but humans remain in control of the final output. In our case, the AI handles the first draft generation at scale, and human experts perform targeted reviews and corrections. This collaboration can yield high-quality results while significantly reducing the workload. A crucial part of this workflow is documentation and transparency: we are keeping logs of how each alt-text was generated (which model, what prompt, any edits) so that there is a clear provenance. In the context of GLAM (Galleries, Libraries, Archives, Museums) institutions, such transparency is important for accountability. Users of the archive should be able to tell if a description was AI-generated or curator-written. In our future interface for the _Stadt.Geschichte.Basel_ collection, we plan to tag AI-generated descriptions (after they've been vetted) with an indication like "AI-assisted description" in the metadata. This way, if a user spots an error or has a question, they know that the description is a product of an algorithmic process and can flag it for review.

**Ethical Considerations:** Deploying AI in heritage description brings several ethical dilemmas to navigate. One is deciding how to handle sensitive content. We encountered images containing derogatory historical texts (e.g., racist slogans on a 1920s poster). Simply omitting these details would whitewash history, but describing them verbatim could be distressing or violate content guidelines. Our solution will be to include a neutral note in the description (e.g., "poster with discriminatory slogan (not quoted here)") and ensure a full transcription is available on request or in a separate text. Another dilemma is the balance between description and interpretation. Alt-text guidelines advise objectivity, but in historical collections, a certain level of interpretation (identifying the context or significance) can greatly enhance comprehension. We have leaned towards _describing with context_---for instance, identifying a person by their role if known ("the Mayor of Basel") rather than just "a man," or noting the event if it's documented. We argue that this approach respects the spirit of alt-text (to convey the same information sighted viewers get, which often includes context from captions or exhibit labels). Nonetheless, we refrain from speculation: the AI might guess emotions or motivations ("appears angry")---we do not include such unverified interpretations in the final alt-text.

**Toward Guidelines and Policy:** One outcome of this project will be a set of practical guidelines for heritage institutions considering AI-generated metadata. We are already formulating recommendations such as: always keep a human in the loop as the final decision-maker; establish an internal content style guide for AI to follow (including sensitive language to avoid or preferred terminology); be mindful of copyright (for modern images, an overly detailed description might infringe on the creator's rights, so in some cases a simpler description might be prudent---although accessibility needs often qualify as fair use in many jurisdictions). In the long term, it may be beneficial to fine-tune or train models on _historical image caption_ data to reduce errors---an avenue for future research. For now, prompt engineering and careful curation are our main tools to align general models with the specialized needs of historical content.

**User Study (Planned):** Ultimately, the success of AI-generated alt-text must be measured by how well it serves the end users. As a next step, we plan to conduct a user study involving two key groups: blind or low-vision individuals who rely on screen readers, and neurodiverse individuals (such as those with dyslexia or certain cognitive disabilities) who benefit from supplemental text descriptions of images. In this study, participants will interact with a selection of images from the collection, accompanied by either human-written or AI-generated alt-text (without knowing which is which). We will evaluate their understanding of the images (through follow-up questions or tasks), the usability of the descriptions (time taken to get information, any confusion or misinterpretation), and gather subjective feedback on satisfaction. This will provide valuable insights into whether the AI-generated descriptions are meeting the needs of real users. For example, a blind user might tell us if the description painted a sufficient mental picture, or a neurodiverse user might comment on whether the alt-text clarified the image in a helpful way. We expect to learn whether our AI-assisted alt-text is truly effective or if there are gaps we didn't anticipate. The results of this user study will inform further refinement of the alt-text (perhaps prompting us to include more or less detail) and will ground our work in the lived experiences of the people we aim to support.

# Conclusion

In this work-in-progress, we explored the use of multimodal AI models to generate accessible image descriptions for a digital heritage collection. Our initial findings are encouraging: with the right prompts and metadata, models like GPT-4o can produce alt-text that significantly lowers the barrier to making historical images accessible, saving time and labor for human experts. This approach has the potential to transform how digital archives practice accessibility, by ensuring that visual content is not exclusively available to sighted audiences. At the same time, our study highlights important considerations for accuracy and ethics. AI-generated descriptions must be vetted for errors and biases; they should complement, not replace, the discerning eye of the historian or archivist. We have shown that a collaborative human-AI workflow can harness the strengths of both---scale and speed from the AI, contextual judgment from the human---to achieve a result that neither could accomplish alone at this scale.

Moving forward, we will complete our systematic evaluation and user study, and we will refine our methods accordingly. We plan to release the dataset of images, metadata, and model-generated alt-text (with any necessary permissions and safeguards) to serve as a benchmark for others. We also acknowledge that there are open questions regarding intellectual property and privacy when using AI in this manner: for instance, how do we handle detailed descriptions of artworks or personal photographs that are under copyright? Our stance is that providing textual descriptions for accessibility is generally justified (and often legally exempt for assistive purposes), but each institution should develop policies in consultation with legal experts. We will include a brief guideline in our final paper on managing these concerns.

Finally, our work contributes to a larger conversation in computational humanities about the role of AI in research workflows. By treating AI outputs as objects of interpretation and by centering accessibility, we hope to model a thoughtful integration of technology in humanities scholarship. As one participant in our discussions noted, this is about _"making the past accessible in the present, to everyone."_ We believe that is a goal worth pursuing with the combined efforts of historians, technologists, and user communities. We look forward to sharing more complete results soon, and to engaging in dialogue at CHR 2025 on how we can collectively harness AI for inclusive and critical digital heritage practices. -->
<ul class="task-list">
<li><label><input type="checkbox">Rework the section above so that it complies with the ideas outlined previously.</label></li>
</ul>
</section>
<section id="acknowledgements" class="level1 unnumbered">
<h1 class="unnumbered">Acknowledgements</h1>
<ul class="task-list">
<li><label><input type="checkbox">Thank Cristina Münch and Noëlle Schnegg for the metadata and images from the Stadt.Geschichte.Basel collection.</label></li>
</ul>
</section>
<section id="references" class="level1">
<h1>References</h1>
<!-- Bibliography will be automatically generated here from the bibliography file -->
<pre><code></code></pre>



</section>

<a onclick="return window.scrollTo(0,0),!1;
" role="button" id="quarto-back-to-top"><i class="bi bi-arrow-up"></i> Back to top</a><div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="csl-bib-body hanging-indent references" data-entry-spacing="0" role="list">
<div id="ref-cecilia2023b" class="csl-entry" role="listitem">
Cecilia, Rafie, Theano Moussouri, and John Fraser. 2023a. <span>“AltText: An Institutional Tool for Change.”</span> <em>Curator</em> 66 (2): 225–31. <a href="https://doi.org/10.1111/cura.12551">https://doi.org/10.1111/cura.12551</a>.
</div>
<div id="ref-cecilia2023a" class="csl-entry" role="listitem">
———. 2023b. <span>“Creating Accessible Digital Images for Vision Impaired Audiences and Researchers.”</span> <em>Curator</em> 66 (1): 5–8. <a href="https://doi.org/10.1111/cura.12536">https://doi.org/10.1111/cura.12536</a>.
</div>
<div id="ref-fickers2022" class="csl-entry" role="listitem">
Fickers, Andreas. 2022. <span>“Digital Hermeneutics: The Reflexive Turn in Digital Public History?”</span> In <em>Handbook of Digital Public History</em>, edited by Serge Noiret, Valérie Schafer, and Gerben Zaagsma, 139–48. De Gruyter. <a href="https://doi.org/10.1515/9783110430295-012">https://doi.org/10.1515/9783110430295-012</a>.
</div>
<div id="ref-shew2023" class="csl-entry" role="listitem">
Shew, Ashley. 2023. <em>Against Technoableism: Rethinking Who Needs Improvement</em>. New York: W. W. Norton.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">window.document.addEventListener("DOMContentLoaded",function(N){const T="\uE9CB",w=new window.AnchorJS;w.options={placement:"right",icon:T},w.add(".anchored");const M=o=>{for(const t of o.classList)if(t.startsWith("code-annotation-"))return!0;return!1},b=function(o){const t=o.trigger;t.blur(),t.classList.add("code-copy-button-checked");var e=t.getAttribute("title");t.setAttribute("title","Copied!");let n;window.bootstrap&&(t.setAttribute("data-bs-toggle","tooltip"),t.setAttribute("data-bs-placement","left"),t.setAttribute("data-bs-title","Copied!"),n=new bootstrap.Tooltip(t,{trigger:"manual",customClass:"code-copy-button-tooltip",offset:[0,-8]}),n.show()),setTimeout(function(){n&&(n.hide(),t.removeAttribute("data-bs-title"),t.removeAttribute("data-bs-toggle"),t.removeAttribute("data-bs-placement")),t.setAttribute("title",e),t.classList.remove("code-copy-button-checked")},1e3),o.clearSelection()},y=function(o){const e=o.parentElement.cloneNode(!0).querySelector("code");for(const n of e.children)M(n)&&n.remove();return e.innerText};new window.ClipboardJS(".code-copy-button:not([data-in-quarto-modal])",{text:y}).on("success",b),window.document.getElementById("quarto-embedded-source-code-modal")&&new window.ClipboardJS(".code-copy-button[data-in-quarto-modal]",{text:y,container:window.document.getElementById("quarto-embedded-source-code-modal")}).on("success",b);for(var q=new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//),I=new RegExp(/^mailto:/),H=new RegExp("https://maehr.github.io/chr2025-seeing-history-unseen/"),k=o=>H.test(o)||q.test(o)||I.test(o),v=window.document.querySelectorAll("a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)"),r=0;r<v.length;r++){const o=v[r];k(o.href)||o.dataset.originalHref!==void 0&&(o.href=o.dataset.originalHref)}function p(o,t,e,n){const i={allowHTML:!0,maxWidth:500,delay:100,arrow:!1,appendTo:function(l){return l.parentElement},interactive:!0,interactiveBorder:10,theme:"quarto",placement:"bottom-start"};t&&(i.content=t),e&&(i.onTrigger=e),n&&(i.onUntrigger=n),window.tippy(o,i)}const E=window.document.querySelectorAll('a[role="doc-noteref"]');for(var r=0;r<E.length;r++){const t=E[r];p(t,function(){let e=t.getAttribute("data-footnote-href")||t.getAttribute("href");try{e=new URL(e).hash}catch{}const n=e.replace(/^#\/?/,""),i=window.document.getElementById(n);return i?i.innerHTML:""})}const A=window.document.querySelectorAll("a.quarto-xref"),g=(o,t)=>{const e=n=>{if(n.classList.remove("page-full","page-columns"),n.children)for(const i of n.children)e(i)};if(e(t),o===null||o.startsWith("sec-")){const n=document.createElement("div");if(t.children&&t.children.length>2){n.appendChild(t.children[0].cloneNode(!0));for(let i=1;i<t.children.length;i++){const l=t.children[i];if(!(l.tagName==="P"&&l.innerText==="")){n.appendChild(l.cloneNode(!0));break}}return window.Quarto?.typesetMath&&window.Quarto.typesetMath(n),n.innerHTML}else return window.Quarto?.typesetMath&&window.Quarto.typesetMath(t),t.innerHTML}else{const n=t.querySelector("a.anchorjs-link");return n&&n.remove(),window.Quarto?.typesetMath&&window.Quarto.typesetMath(t),t.classList.contains("callout")?t.outerHTML:t.innerHTML}};for(var r=0;r<A.length;r++){const t=A[r];p(t,void 0,function(e){e.disable();let n=t.getAttribute("href"),i;if(n.startsWith("#"))i=n;else try{i=new URL(n).hash}catch{}if(i){const l=i.replace(/^#\/?/,""),a=window.document.getElementById(l);if(a!==null)try{const s=g(l,a.cloneNode(!0));e.setContent(s)}finally{e.enable(),e.show()}else fetch(n.split("#")[0]).then(s=>s.text()).then(s=>{const u=new DOMParser().parseFromString(s,"text/html").getElementById(l);if(u!==null){const c=g(l,u);e.setContent(c)}}).finally(()=>{e.enable(),e.show()})}else fetch(n).then(l=>l.text()).then(l=>{const d=new DOMParser().parseFromString(l,"text/html").querySelector("main.content");if(d!==null){d.children.length>0&&d.children[0].tagName==="HEADER"&&d.children[0].remove();const f=g(null,d);e.setContent(f)}}).finally(()=>{e.enable(),e.show()})},function(e){})}let m;const B=(o,t)=>{let e='data-code-cell="'+o+'"',n='data-code-annotation="'+t+'"';return"span["+e+"]["+n+"]"},L=o=>{const t=window.document,e=o.getAttribute("data-target-cell"),n=o.getAttribute("data-target-annotation"),a=window.document.querySelector(B(e,n)).getAttribute("data-code-lines").split(",").map(u=>e+"-"+u);let s=null,d=null,f=null;if(a.length>0){const u=window.document.getElementById(a[0]);if(s=u.offsetTop,d=u.offsetHeight,f=u.parentElement.parentElement,a.length>1){const c=window.document.getElementById(a[a.length-1]);d=c.offsetTop+c.offsetHeight-s}if(s!==null&&d!==null&&f!==null){let c=window.document.getElementById("code-annotation-line-highlight");c===null&&(c=window.document.createElement("div"),c.setAttribute("id","code-annotation-line-highlight"),c.style.position="absolute",f.appendChild(c)),c.style.top=s-2+"px",c.style.height=d+4+"px",c.style.left=0;let h=window.document.getElementById("code-annotation-line-highlight-gutter");h===null&&(h=window.document.createElement("div"),h.setAttribute("id","code-annotation-line-highlight-gutter"),h.style.position="absolute",window.document.getElementById(e).querySelector(".code-annotation-gutter").appendChild(h)),h.style.top=s-2+"px",h.style.height=d+4+"px"}m=o}},C=()=>{["code-annotation-line-highlight","code-annotation-line-highlight-gutter"].forEach(t=>{const e=window.document.getElementById(t);e&&e.remove()}),m=void 0};window.addEventListener("resize",D(()=>{elRect=void 0,m&&L(m)},10));function D(o,t){let e=!1,n;return(...i)=>{e?(n&&clearTimeout(n),n=setTimeout(()=>{o.apply(this,i),n=e=!1},t)):(o.apply(this,i),e=!0)}}const R=window.document.querySelectorAll("dt[data-target-cell]");for(const o of R)o.addEventListener("click",t=>{const e=t.target;if(e!==m){C();const n=window.document.querySelector("dt[data-target-cell].code-annotation-active");n&&n.classList.remove("code-annotation-active"),L(e),e.classList.add("code-annotation-active")}else C(),e.classList.remove("code-annotation-active")});const x=o=>{const t=o.parentElement;if(t){const e=t.dataset.cites;return e?{el:o,cites:e.split(" ")}:x(o.parentElement)}else return};for(var S=window.document.querySelectorAll('a[role="doc-biblioref"]'),r=0;r<S.length;r++){const t=S[r],e=x(t);e&&p(e.el,function(){var n=window.document.createElement("div");return e.cites.forEach(function(i){var l=window.document.createElement("div");l.classList.add("hanging-indent"),l.classList.add("csl-entry");var a=window.document.getElementById("ref-"+i);a&&(l.innerHTML=a.innerHTML),n.appendChild(l)}),n.innerHTML})}});
</script>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-md-none d-sm-block"><ul><li><a href="https://github.com/maehr/chr2025-seeing-history-unseen/edit/main/paper/index.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/maehr/chr2025-seeing-history-unseen/issues/new/choose" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>