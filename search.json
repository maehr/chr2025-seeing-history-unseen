[
  {
    "objectID": "SECURITY.html",
    "href": "SECURITY.html",
    "title": "Security Policy",
    "section": "",
    "text": "To report a security issue, please email moritz.maehr@gmail.com with a description of the issue, the steps you took to create the issue, affected versions, and, if known, mitigations for the issue. This project follows a 90 day disclosure timeline.",
    "crumbs": [
      "Home",
      "Project",
      "Security"
    ]
  },
  {
    "objectID": "SECURITY.html#reporting-a-vulnerability",
    "href": "SECURITY.html#reporting-a-vulnerability",
    "title": "Security Policy",
    "section": "",
    "text": "To report a security issue, please email moritz.maehr@gmail.com with a description of the issue, the steps you took to create the issue, affected versions, and, if known, mitigations for the issue. This project follows a 90 day disclosure timeline.",
    "crumbs": [
      "Home",
      "Project",
      "Security"
    ]
  },
  {
    "objectID": "survey/index.html",
    "href": "survey/index.html",
    "title": "CHR 2025 - Seeing History Unseen",
    "section": "",
    "text": "WarningSurvey Closed\n\n\n\nThank you for your interest! The survey period has ended, and we are no longer accepting responses. We appreciate everyone who took the time to participate.\n\n\n\n\n\n    \n        \n            Studie: Vergleich von automatisch erzeugten Alternativtexten\n\n            Liebe Teilnehmerin, lieber Teilnehmer,\n            \n                im Rahmen des Projekts Stadt.Geschichte.Basel stellen wir historische Quellen und\n                Forschungsdaten Open Access zur Verf√ºgung. Ziel ist es, diese digitale Infrastruktur\n                m√∂glichst barrierearm zu gestalten, damit auch sehbehinderte und blinde Personen Zugang zu\n                den visuellen Best√§nden erhalten.\n            \n            \n                Das Verfassen von Alternativtexten (Alt-Texten) f√ºr Bilder ist jedoch sehr aufwendig. Da uns\n                die personellen Mittel f√ºr eine vollst√§ndige manuelle Erschliessung fehlen, untersuchen wir\n                in dieser Vorstudie, ob multimodale KI-Modelle (Vision-Language Models) bei dieser Aufgabe\n                unterst√ºtzen k√∂nnen.\n            \n\n            Ziel der Studie\n            \n                Wir testen, wie gut verschiedene KI-Systeme kurze Beschreibungen von historischen Objekten\n                erzeugen, die als barrierefreie Alternativtexte verwendet werden k√∂nnen. Dazu wurden 20\n                Bilder ausgew√§hlt und mit ihren Metadaten an vier verschiedene Modelle √ºbergeben. Die\n                Auswahl bildet die unterschiedlichen in der Sammlung von Stadt.Geschichte.Basel abgedeckten\n                Epochen und Medientypen (Fotografien, Karten, Drucke, Objekte etc.) repr√§sentativ ab.\n            \n            \n                Ihre Einsch√§tzung hilft uns zu ermitteln, welches Modell sich am besten f√ºr den Einsatz in\n                der historischen Forschung eignet.\n            \n            \n                Weitere Fragen der Studie sind, welche Kosten damit verbunden sind, und wie algorithmische\n                Verfahren verantwortungsvoll in geisteswissenschaftliche Arbeitsprozesse integriert werden\n                k√∂nnen. Die Ergebnisse werden auf der Computational Humanities Research Conference (CHR\n                2025) vom 9.‚Äì12. Dezember 2025 am Luxembourg Centre for Contemporary and Digital History\n                (C¬≤DH) vorgestellt.\n            \n\n            \n\n            Ablauf und Bewertungskriterien\n            \n                Die Umfrage umfasst 20 Objekte und dauert etwa 10 bis 15 Minuten (ca. 30\n                bis 45 Sekunden pro Bild).\n            \n            \n                Auf jeder Seite sehen Sie ein Sammlungsbild und vier kurze Beschreibungen. Ordnen Sie die\n                Beschreibungen von beste Beschreibung nach\n                schlechteste Beschreibung\n                per Drag & Drop. Alle Texte wurden automatisch generiert und k√∂nnen Fehler enthalten.\n            \n\n            \n                Bewertungsregeln (nach WCAG-Intention):\n                \n                    den Kerninhalt sachlich und knapp wiedergeben,\n                    keine redundanten Angaben wie ‚ÄûBild von ‚Ä¶‚Äú enthalten,\n                    \n                        wesentliche visuelle Merkmale (z. B. Personen, Objekte, Handlungen, Schrift)\n                        priorisieren,\n                    \n                    und nur dann Kontext liefern, wenn er zum Verst√§ndnis beitr√§gt.\n                    \n                        Weiterf√ºhrende Links:\n                        7 Regeln f√ºr gute Alt-Texte\n                        (nur lesen, falls Unklarheiten bestehen)\n                    \n                \n                \n                    Tastatur: ‚Üë/‚Üì verschiebt fokussierten Eintrag; 1‚Äì4 setzt direkt auf\n                    Position; Tab f√ºr Fokus.\n                \n                \n                    Fokus-Indikator sichtbar. Elemente sind per ARIA ausgezeichnet.\n                \n            \n            \n                Wir danken Ihnen herzlich f√ºr Ihre Zeit und Ihre fachliche Einsch√§tzung. Sie leisten damit\n                einen wichtigen Beitrag zu inklusiver, nachhaltiger und kritischer digitaler\n                Geschichtsforschung.\n            \n            \n                Mit freundlichen Gr√ºssen,\n                Dr. Moritz M√§hr und Moritz Twente\n                Universit√§t Bern / Universit√§t Basel\n                    Kontakt: moritz.maehr@unibas.ch /\n                    moritz.twente@unibas.ch\n            \n\n            \n\n            Einwilligung und Start\n            \n                Ihre Teilnahme ist freiwillig. Die personenbezogene Angabe (E-Mail-Adresse) dient\n                ausschliesslich m√∂glichen R√ºckfragen und wird nicht in die Forschungsdatenbank aufgenommen.\n            \n\n            \n                \n                    E-Mail*\n                    (nur f√ºr eventuelle R√ºckfragen)\n                \n                \n            \n\n            \n                \n                \n                    Ich willige in die Verarbeitung meiner Daten f√ºr diese Studie ein.\n                \n            \n\n            \n                Hinweis: Einige Objekte enthalten veraltete, vorurteilsbeladene oder diskriminierende\n                Darstellungen. Als historische Quellen werden diese Inhalte unver√§ndert pr√§sentiert.\n            \n\n            Start\n            \n        \n    \n\n    \n        \n            \n\n            \n                \n                    \n                    \n                \n\n                \n                    \n                    \n                    \n                        Verschiebe Elemente per Drag und Drop oder nutze Pfeiltasten. Beste Beschreibung nach\n                        oben.\n                    \n                    \n\n                    \n                        \n                            Zur√ºck\n                        \n                        \n                            Weiter\n                        \n                    \n                \n            \n        \n    \n\n    \n        \n            Abschluss\n            √úbermittlung‚Ä¶\n            Antworten senden\n            \n        \n    \n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Data & Survey",
      "Survey"
    ]
  },
  {
    "objectID": "AGENTS.html",
    "href": "AGENTS.html",
    "title": "Repository Guidelines",
    "section": "",
    "text": "Authoring sources live in:\n\npaper/ ‚Äî manuscript (paper.md), wrapper (index.qmd), bibliography.bib, and images/\nsrc/ ‚Äî Python alt-text pipeline (generate_alt_text.py) writing timestamped outputs to runs/\nabstract/, presentation/ ‚Äî ancillary materials; published HTML in _site/\n\n\n\n\n\ngenerate_alt_text.py ‚Äî fetch metadata, call configured models, and write timestamped runs under runs/&lt;timestamp&gt;/ (raw JSON, prompt tables, manifests, cached images).\nclean_survey_data.py ‚Äî strip excluded submissions/emails from data/raw/formspree_*_export.json, saving data/processed/processed_survey_submissions.json.\nprocess_survey_rankings.py ‚Äî expand per-object rankings into data/processed/survey_rankings.csv with elapsed time per rater.\nprocess_best_answers.py ‚Äî derive consensus winners and export data/processed/best_answers.csv with exemplar texts.\nanalyze_survey_time.py ‚Äî summarise completion durations and write analysis/time_stats_by_{object,submission}.csv.\nranking_tests.py ‚Äî run Friedman/Wilcoxon tests, generate CSV summaries, and save plots (boxplots, heatmaps) in analysis/.\nviz_dataset.py ‚Äî build manuscript figures at paper/images/fig_type_era_{full,subset}.png.\n\nSee README.md for the mermaid workflow diagram and an artefact directory table.\n\n\n\n\nEdit paper/paper.md; paper/index.qmd includes it for rendering.\nPreview: quarto preview (live reload).\nCitations: add BibTeX to paper/bibliography.bib and cite with [@key]. Example: ‚Äú‚Ä¶explicit detail [@cecilia2023b]‚Äù.\nFigures: place under paper/images/ and reference with labels. Example: ![Model comparison.](images/fig_models.png){#fig:models width=60%}.\nTables: use Markdown tables with captions. Example: Table: Model costs {#tbl:models} then reference as [Table @tbl:models].\nFrom TODO to paper: items under TODO.md ‚Üí ‚ÄúPaper/Literature and Context‚Äù and ‚ÄúMethodology Updates‚Äù should become citations and a maintained @tbl:models in paper.md. Keep model IDs and costs aligned with src/generate_alt_text.py and recent runs.\n\n\n\n\n\nnpm install && uv sync ‚Äî install Node and Python toolchains.\nquarto preview ‚Äî preview the website; add paper/ to focus on the manuscript.\nuv run python src/generate_alt_text.py ‚Äî execute the batch job (requires .env). [DO NOT EXECUTE UNLESS EXPLICITLY AUTHORIZED]\nnpm run check / npm run format ‚Äî Prettier verify/fix.\nuv run ruff check / uv run ruff fix ‚Äî Ruff verify/fix.\n\n\n\n\n\nPrettier for Markdown/Quarto/JSON; Ruff for Python (PEP 8, typed).\nUse snake_case filenames; figures fig_&lt;short-key&gt;.png; labels #fig:key, #tbl:key.\nCommit notebooks with cleared outputs (jupyter nbconvert --clear-output).\n\n\n\n\n\nUse npm run commit (Conventional Commits); reference issues with #NNN.\nPRs: describe scope, note affected sections of paper.md, and attach before/after renders when changing figures/tables. Ensure npm run check passes.\n\n\n\n\n\nReference run artifacts via relative paths (e.g., runs/2025.../manifest.json); avoid committing large intermediates.\nConfigure secrets in .env (e.g., OPENROUTER_API_KEY); never commit credentials."
  },
  {
    "objectID": "AGENTS.html#project-structure-module-organization",
    "href": "AGENTS.html#project-structure-module-organization",
    "title": "Repository Guidelines",
    "section": "",
    "text": "Authoring sources live in:\n\npaper/ ‚Äî manuscript (paper.md), wrapper (index.qmd), bibliography.bib, and images/\nsrc/ ‚Äî Python alt-text pipeline (generate_alt_text.py) writing timestamped outputs to runs/\nabstract/, presentation/ ‚Äî ancillary materials; published HTML in _site/"
  },
  {
    "objectID": "AGENTS.html#pipeline-workflow-src",
    "href": "AGENTS.html#pipeline-workflow-src",
    "title": "Repository Guidelines",
    "section": "",
    "text": "generate_alt_text.py ‚Äî fetch metadata, call configured models, and write timestamped runs under runs/&lt;timestamp&gt;/ (raw JSON, prompt tables, manifests, cached images).\nclean_survey_data.py ‚Äî strip excluded submissions/emails from data/raw/formspree_*_export.json, saving data/processed/processed_survey_submissions.json.\nprocess_survey_rankings.py ‚Äî expand per-object rankings into data/processed/survey_rankings.csv with elapsed time per rater.\nprocess_best_answers.py ‚Äî derive consensus winners and export data/processed/best_answers.csv with exemplar texts.\nanalyze_survey_time.py ‚Äî summarise completion durations and write analysis/time_stats_by_{object,submission}.csv.\nranking_tests.py ‚Äî run Friedman/Wilcoxon tests, generate CSV summaries, and save plots (boxplots, heatmaps) in analysis/.\nviz_dataset.py ‚Äî build manuscript figures at paper/images/fig_type_era_{full,subset}.png.\n\nSee README.md for the mermaid workflow diagram and an artefact directory table."
  },
  {
    "objectID": "AGENTS.html#paper-authoring-workflow",
    "href": "AGENTS.html#paper-authoring-workflow",
    "title": "Repository Guidelines",
    "section": "",
    "text": "Edit paper/paper.md; paper/index.qmd includes it for rendering.\nPreview: quarto preview (live reload).\nCitations: add BibTeX to paper/bibliography.bib and cite with [@key]. Example: ‚Äú‚Ä¶explicit detail [@cecilia2023b]‚Äù.\nFigures: place under paper/images/ and reference with labels. Example: ![Model comparison.](images/fig_models.png){#fig:models width=60%}.\nTables: use Markdown tables with captions. Example: Table: Model costs {#tbl:models} then reference as [Table @tbl:models].\nFrom TODO to paper: items under TODO.md ‚Üí ‚ÄúPaper/Literature and Context‚Äù and ‚ÄúMethodology Updates‚Äù should become citations and a maintained @tbl:models in paper.md. Keep model IDs and costs aligned with src/generate_alt_text.py and recent runs."
  },
  {
    "objectID": "AGENTS.html#build-test-and-development-commands",
    "href": "AGENTS.html#build-test-and-development-commands",
    "title": "Repository Guidelines",
    "section": "",
    "text": "npm install && uv sync ‚Äî install Node and Python toolchains.\nquarto preview ‚Äî preview the website; add paper/ to focus on the manuscript.\nuv run python src/generate_alt_text.py ‚Äî execute the batch job (requires .env). [DO NOT EXECUTE UNLESS EXPLICITLY AUTHORIZED]\nnpm run check / npm run format ‚Äî Prettier verify/fix.\nuv run ruff check / uv run ruff fix ‚Äî Ruff verify/fix."
  },
  {
    "objectID": "AGENTS.html#coding-style-naming-conventions",
    "href": "AGENTS.html#coding-style-naming-conventions",
    "title": "Repository Guidelines",
    "section": "",
    "text": "Prettier for Markdown/Quarto/JSON; Ruff for Python (PEP 8, typed).\nUse snake_case filenames; figures fig_&lt;short-key&gt;.png; labels #fig:key, #tbl:key.\nCommit notebooks with cleared outputs (jupyter nbconvert --clear-output)."
  },
  {
    "objectID": "AGENTS.html#commit-pull-request-guidelines",
    "href": "AGENTS.html#commit-pull-request-guidelines",
    "title": "Repository Guidelines",
    "section": "",
    "text": "Use npm run commit (Conventional Commits); reference issues with #NNN.\nPRs: describe scope, note affected sections of paper.md, and attach before/after renders when changing figures/tables. Ensure npm run check passes."
  },
  {
    "objectID": "AGENTS.html#reproducibility-security",
    "href": "AGENTS.html#reproducibility-security",
    "title": "Repository Guidelines",
    "section": "",
    "text": "Reference run artifacts via relative paths (e.g., runs/2025.../manifest.json); avoid committing large intermediates.\nConfigure secrets in .env (e.g., OPENROUTER_API_KEY); never commit credentials."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CHR 2025 Conference Materials",
    "section": "",
    "text": "This repository contains the abstract and presentation materials for the CHR 2025 conference paper ‚ÄúSeeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections‚Äù by Moritz M√§hr (University of Bern and Basel) and Moritz Twente (University of Basel).\n     \n\n\nThis repository hosts the conference materials for CHR 2025 (Conference on Computational Humanities Research) including:\n\nAbstract: A LaTeX document containing the complete abstract for our paper\nPresentation Materials: Slides and supporting materials for the conference presentation\nDocumentation: Supporting documentation and setup instructions\n\n\n\n\nOur research explores the feasibility, accuracy, and ethics of using state-of-the-art vision-language models to generate WCAG-compliant alt-text for heterogeneous digital heritage collections. We combine computational experiments with qualitative evaluation to develop a framework for responsible AI-assisted accessibility in the humanities.\n\n\n\nFeasibility: Can current vision-language models produce useful, WCAG 2.2‚Äìcompliant alt-text for complex historical images when provided with contextual metadata?\nQuality and Authenticity: How do domain experts rate AI-generated image descriptions in terms of factual accuracy, completeness, and usefulness for understanding historical content?\nEthics and Governance: What are the ethical implications of using AI to generate alt-text in heritage collections, and what human oversight or policy safeguards are required for responsible use?\n\n\n\n\n\n\nabstract/: Contains the LaTeX source, class files, and bibliography for the conference abstract\npresentation/: Will contain presentation slides and supporting materials\nsrc/: Alt-text generation and survey analysis pipeline\n\ngenerate_alt_text.py: Batch alt-text generator that writes timestamped outputs to runs/\nclean_survey_data.py: Removes excluded submissions and email addresses from raw Formspree exports\nprocess_survey_rankings.py: Expands cleaned submissions into per-object model rankings (survey_rankings.csv)\nprocess_best_answers.py: Aggregates consensus winners and texts per object (best_answers.csv)\nanalyze_survey_time.py: Summarises completion times across objects and raters\nranking_tests.py: Runs Friedman/Wilcoxon tests and produces comparison plots under analysis/\nviz_dataset.py: Creates figure assets for the manuscript (paper/images/fig_type_era_*.png)\nplayground.ipynb: Interactive Jupyter notebook for experimenting with the pipeline\n\nruns/: Output directory for generated alt-text results, including raw API responses and CSV/JSONL/Parquet tables\ndata/: Data directories for raw and cleaned datasets\n\n\n\n\nWe recommend using GitHub Codespaces for a reproducible setup.\n\n\n\n\n\n\nUse this template for your project in a new repository on your GitHub account.\n\n\n\nClick the green &lt;&gt; Code button at the top right of this repository.\nSelect the ‚ÄúCodespaces‚Äù tab and click ‚ÄúCreate codespace on main‚Äù. GitHub will now build a container that includes:\n\n‚úÖ Node.js (via npm)\n‚úÖ Python with uv\n‚úÖ R with renv\n‚úÖ Quarto\n\n\n\n\nOnce the Codespace is ready, open a terminal and preview the documentation:\nuv run quarto preview\n\n\n\n\n\nNote: All dependencies (Node.js, Python, R, Quarto) are pre-installed in the Codespace.\n\n\n\nüë©‚Äçüíª Advanced Local Installation\n\n\n\n\nNode.js\nR and Rtools (on Windows)\nuv (Python manager)\nQuarto\n\n\nNote: uv installs and manages the correct Python version automatically.\n\n\n\n\n# 1. Install Node.js dependencies\nnpm install\nnpm run prepare\n\n# 2. Setup Python environment\nuv sync\n\n# 3. Setup R environment\nRscript -e 'install.packages(\"renv\"); renv::restore()'\n\n# 4. Preview documentation\nuv run quarto preview",
    "crumbs": [
      "Home",
      "Project",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#about-this-repository",
    "href": "index.html#about-this-repository",
    "title": "CHR 2025 Conference Materials",
    "section": "",
    "text": "This repository hosts the conference materials for CHR 2025 (Conference on Computational Humanities Research) including:\n\nAbstract: A LaTeX document containing the complete abstract for our paper\nPresentation Materials: Slides and supporting materials for the conference presentation\nDocumentation: Supporting documentation and setup instructions",
    "crumbs": [
      "Home",
      "Project",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#research-overview",
    "href": "index.html#research-overview",
    "title": "CHR 2025 Conference Materials",
    "section": "",
    "text": "Our research explores the feasibility, accuracy, and ethics of using state-of-the-art vision-language models to generate WCAG-compliant alt-text for heterogeneous digital heritage collections. We combine computational experiments with qualitative evaluation to develop a framework for responsible AI-assisted accessibility in the humanities.\n\n\n\nFeasibility: Can current vision-language models produce useful, WCAG 2.2‚Äìcompliant alt-text for complex historical images when provided with contextual metadata?\nQuality and Authenticity: How do domain experts rate AI-generated image descriptions in terms of factual accuracy, completeness, and usefulness for understanding historical content?\nEthics and Governance: What are the ethical implications of using AI to generate alt-text in heritage collections, and what human oversight or policy safeguards are required for responsible use?",
    "crumbs": [
      "Home",
      "Project",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#repository-structure",
    "href": "index.html#repository-structure",
    "title": "CHR 2025 Conference Materials",
    "section": "",
    "text": "abstract/: Contains the LaTeX source, class files, and bibliography for the conference abstract\npresentation/: Will contain presentation slides and supporting materials\nsrc/: Alt-text generation and survey analysis pipeline\n\ngenerate_alt_text.py: Batch alt-text generator that writes timestamped outputs to runs/\nclean_survey_data.py: Removes excluded submissions and email addresses from raw Formspree exports\nprocess_survey_rankings.py: Expands cleaned submissions into per-object model rankings (survey_rankings.csv)\nprocess_best_answers.py: Aggregates consensus winners and texts per object (best_answers.csv)\nanalyze_survey_time.py: Summarises completion times across objects and raters\nranking_tests.py: Runs Friedman/Wilcoxon tests and produces comparison plots under analysis/\nviz_dataset.py: Creates figure assets for the manuscript (paper/images/fig_type_era_*.png)\nplayground.ipynb: Interactive Jupyter notebook for experimenting with the pipeline\n\nruns/: Output directory for generated alt-text results, including raw API responses and CSV/JSONL/Parquet tables\ndata/: Data directories for raw and cleaned datasets",
    "crumbs": [
      "Home",
      "Project",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "CHR 2025 Conference Materials",
    "section": "",
    "text": "We recommend using GitHub Codespaces for a reproducible setup.",
    "crumbs": [
      "Home",
      "Project",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "CHR 2025 Conference Materials",
    "section": "",
    "text": "Use this template for your project in a new repository on your GitHub account.\n\n\n\nClick the green &lt;&gt; Code button at the top right of this repository.\nSelect the ‚ÄúCodespaces‚Äù tab and click ‚ÄúCreate codespace on main‚Äù. GitHub will now build a container that includes:\n\n‚úÖ Node.js (via npm)\n‚úÖ Python with uv\n‚úÖ R with renv\n‚úÖ Quarto\n\n\n\n\nOnce the Codespace is ready, open a terminal and preview the documentation:\nuv run quarto preview\n\n\n\n\n\nNote: All dependencies (Node.js, Python, R, Quarto) are pre-installed in the Codespace.\n\n\n\nüë©‚Äçüíª Advanced Local Installation\n\n\n\n\nNode.js\nR and Rtools (on Windows)\nuv (Python manager)\nQuarto\n\n\nNote: uv installs and manages the correct Python version automatically.\n\n\n\n\n# 1. Install Node.js dependencies\nnpm install\nnpm run prepare\n\n# 2. Setup Python environment\nuv sync\n\n# 3. Setup R environment\nRscript -e 'install.packages(\"renv\"); renv::restore()'\n\n# 4. Preview documentation\nuv run quarto preview",
    "crumbs": [
      "Home",
      "Project",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#use",
    "href": "index.html#use",
    "title": "CHR 2025 Conference Materials",
    "section": "Use",
    "text": "Use\n\nBuilding the Abstract\nTo build the LaTeX abstract:\ncd abstract\nmake paper\n\n\nDevelopment Commands\nCheck that all files are properly formatted:\nnpm run check\nFormat all files:\nnpm run format\nRun the wizard to write meaningful commit messages:\nnpm run commit\nGenerate a changelog:\nnpm run changelog\n\n\nAlt-Text Generation Pipeline\nThe repository includes a focused Python pipeline for generating WCAG-compliant alternative texts using OpenRouter-compatible vision-language models. The pipeline supports systematic evaluation of VLM performance on alt-text generation tasks for digital heritage collections.\n\nFeatures\n\nAutomated alt-text generation using multiple VLM models in parallel\nWCAG 2.2 compliance with structured prompts based on accessibility guidelines\nMetadata integration from remote sources with provenance tracking\nWide-format output with model responses in CSV, JSONL, and Parquet formats\nRaw API response storage for reproducibility and analysis\nInteractive playground via Jupyter notebook for experimentation\n\n\n\nQuick Start\n\nInstall Python dependencies with uv:\n\nuv sync\n\nSet up your OpenRouter API key in a .env file:\n\ncp example.env .env\n# Edit .env to add your OPENROUTER_API_KEY\n\nRun the alt-text generation pipeline:\n\nuv run python src/generate_alt_text.py\nThis will:\n\nFetch metadata from the configured URL\nGenerate alt-text for specified media IDs using all configured models\nSave results in runs/YYYYmmdd_HHMMSS/ including:\n\nmetadata.json: Copy of fetched metadata for provenance\nalt_text_runs_*.csv: Wide-format table with all model responses\nalt_text_runs_*.jsonl: Same data in JSONL format\nalt_text_runs_*.parquet: Same data in Parquet format (if available)\nraw/*.json: Individual raw API responses from each model\nmanifest.json: Run metadata including configuration and file paths\n\n\n\nExperiment interactively with the Jupyter notebook:\n\nuv run jupyter notebook src/playground.ipynb\n\n\nConfiguration\nEdit src/generate_alt_text.py to customize:\n\nMODELS: List of OpenRouter model identifiers to use\nMEDIA_IDS: List of media object IDs to process\nMETADATA_URL: URL to fetch media metadata JSON\n\nCurrent models configured in generate_alt_text.py:\n\ngoogle/gemini-2.5-flash-lite\nqwen/qwen3-vl-8b-instruct\nopenai/gpt-4o-mini\nmeta-llama/llama-4-maverick\n\n\n\n\nScript workflow and artefacts\n\n\n\n\n\nflowchart TD\n  A[generate_alt_text.py&lt;br/&gt;Fetch metadata + call models] --&gt;|runs/&lt;timestamp&gt;/*| B{Survey prep}\n  B --&gt; C[clean_survey_data.py&lt;br/&gt;Sanitise Formspree export]\n  C --&gt; D[process_survey_rankings.py&lt;br/&gt;Expand per object + rater ranks]\n  D --&gt; E[process_best_answers.py&lt;br/&gt;Consensus winner per object]\n  D --&gt; F[analyze_survey_time.py&lt;br/&gt;Timing summaries]\n  D --&gt; G[ranking_tests.py&lt;br/&gt;Statistical tests + plots]\n  A --&gt; H[viz_dataset.py&lt;br/&gt;Paper figures]\n\n  E --&gt; I[analysis/&lt;br/&gt;CSVs + plots]\n  F --&gt; I\n  G --&gt; I\n  H --&gt; J[paper/images/fig_type_era_*.png]\n\n\n\n\n\n\n\n\nOutputs by directory\n\nruns/&lt;timestamp&gt;/ ‚Äî generate_alt_text.py writes manifest.json, raw/*.json (per model √ó object), cached images/*.jpg, and timestamped tables (alt_text_runs_*_{wide,long}.csv|parquet|jsonl, optional prompts CSV).\ndata/raw/ ‚Äî manual Formspree exports (e.g., formspree_*_export.json).\ndata/processed/ ‚Äî clean_survey_data.py, process_survey_rankings.py, and process_best_answers.py materialise processed_survey_submissions.json, survey_rankings.csv, and best_answers.csv.\nanalysis/ ‚Äî analyze_survey_time.py and ranking_tests.py produce time_stats_by_{object,submission}.csv, rank_counts_*.csv, statistical summaries, and comparison plots (rank_distributions_boxplot.png, pairwise_pvalues_heatmap.png, etc.).\npaper/images/ ‚Äî viz_dataset.py renders figure assets such as fig_type_era_full.png and fig_type_era_subset.png.\n\nEach script prints the paths it writes; check those logs for exact filenames when running new experiments.\n\n\nReference run (2025-10-21 subsample)\nUse runs/20251021_233530/ as the canonical example of a recent full pipeline execution.\n\nConfiguration: mode=\"subsample\" across 20 media IDs and four models (google/gemini-2.5-flash-lite, qwen/qwen3-vl-8b-instruct, openai/gpt-4o-mini, meta-llama/llama-4-maverick).\nRuntime: 244 seconds wall time; no errors recorded in run.log.\nArtefacts:\n\nalt_text_runs_20251021_233933_wide.{csv,parquet} ‚Äî pivoted responses (one row per media object with model-specific columns).\nalt_text_runs_20251021_233933_long.{csv,parquet,jsonl} ‚Äî long format table with 80 model/object rows.\nalt_text_runs_20251021_233933_prompts.csv ‚Äî per-item prompt, system, and image URL trace.\nraw/*.json ‚Äî individual API responses (model √ó object).\nimages/*.jpg ‚Äî thumbnails cached during the run.\nmanifest.json ‚Äî reproducibility metadata (models, media IDs, durations, output pointers).\n\n\nMirror this structure when staging new runs for survey generation or reporting.",
    "crumbs": [
      "Home",
      "Project",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#support",
    "href": "index.html#support",
    "title": "CHR 2025 Conference Materials",
    "section": "Support",
    "text": "Support\nThis project is maintained by @maehr. Please understand that we can‚Äôt provide individual support via email. We also believe that help is much more valuable when it‚Äôs shared publicly, so more people can benefit from it.\n\n\n\nType\nPlatforms\n\n\n\n\nüö® Bug Reports\nGitHub Issue Tracker\n\n\nüìö Docs Issue\nGitHub Issue Tracker\n\n\nüéÅ Feature Requests\nGitHub Issue Tracker\n\n\nüõ° Report a security vulnerability\nSee SECURITY.md\n\n\nüí¨ General Questions\nGitHub Discussions",
    "crumbs": [
      "Home",
      "Project",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#roadmap",
    "href": "index.html#roadmap",
    "title": "CHR 2025 Conference Materials",
    "section": "Roadmap",
    "text": "Roadmap\n\nComplete the conference abstract and presentation preparation\nCreate presentation slides for CHR 2025\nFinalize user study design and implementation\nPublish dataset and benchmark for future research",
    "crumbs": [
      "Home",
      "Project",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "CHR 2025 Conference Materials",
    "section": "Contributing",
    "text": "Contributing\nPlease see CONTRIBUTING.md for details on our code of conduct and the process for submitting pull requests.",
    "crumbs": [
      "Home",
      "Project",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#authors-and-credits",
    "href": "index.html#authors-and-credits",
    "title": "CHR 2025 Conference Materials",
    "section": "Authors and credits",
    "text": "Authors and credits\n\nMoritz M√§hr - University of Bern & Basel - maehr\nMoritz Twente - University of Basel - moritztwente\n\nSee also the list of contributors who contributed to this project.",
    "crumbs": [
      "Home",
      "Project",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "CHR 2025 Conference Materials",
    "section": "License",
    "text": "License\nThe abstract, presentation materials, and documentation in this repository are released under the Creative Commons Attribution 4.0 International (CC BY 4.0) License - see the LICENSE-CCBY file for details. By using these materials, you agree to give appropriate credit to the original author(s) and to indicate if any modifications have been made.\nAny code in this repository is released under the GNU Affero General Public License v3.0 - see the LICENSE-AGPL file for details.",
    "crumbs": [
      "Home",
      "Project",
      "Overview"
    ]
  },
  {
    "objectID": "CHANGELOG.html",
    "href": "CHANGELOG.html",
    "title": "Changelog",
    "section": "",
    "text": "All notable changes to this project will be documented in this file.\nThe format is based on Keep a Changelog, and this project adheres to Semantic Versioning.\n\n\n\n\n\nDocs for Zenodo added\nFixed various typos and replaced favicon\n\n\n\n\n\nInitial version\n\n\n\n\n\n\n\n\n‚Ä¶\n\n\n\n\n\n\n\n\n‚Ä¶",
    "crumbs": [
      "Home",
      "Project",
      "Changelog"
    ]
  },
  {
    "objectID": "CHANGELOG.html#unreleased",
    "href": "CHANGELOG.html#unreleased",
    "title": "Changelog",
    "section": "",
    "text": "Docs for Zenodo added\nFixed various typos and replaced favicon\n\n\n\n\n\nInitial version",
    "crumbs": [
      "Home",
      "Project",
      "Changelog"
    ]
  },
  {
    "objectID": "CHANGELOG.html#section",
    "href": "CHANGELOG.html#section",
    "title": "Changelog",
    "section": "",
    "text": "‚Ä¶",
    "crumbs": [
      "Home",
      "Project",
      "Changelog"
    ]
  },
  {
    "objectID": "CHANGELOG.html#section-1",
    "href": "CHANGELOG.html#section-1",
    "title": "Changelog",
    "section": "",
    "text": "‚Ä¶",
    "crumbs": [
      "Home",
      "Project",
      "Changelog"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.\n\n\n\nExamples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others‚Äô private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\n\n\n\nCommunity leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.\n\n\n\nThis Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.\n\n\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at moritz.maehr@gmail.com. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.\n\n\n\nCommunity leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.\n\n\n\n\nThis Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla‚Äôs code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.",
    "crumbs": [
      "Home",
      "Project",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-pledge",
    "href": "CODE_OF_CONDUCT.html#our-pledge",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.",
    "crumbs": [
      "Home",
      "Project",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-standards",
    "href": "CODE_OF_CONDUCT.html#our-standards",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Examples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others‚Äô private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting",
    "crumbs": [
      "Home",
      "Project",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "href": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.",
    "crumbs": [
      "Home",
      "Project",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#scope",
    "href": "CODE_OF_CONDUCT.html#scope",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.",
    "crumbs": [
      "Home",
      "Project",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement",
    "href": "CODE_OF_CONDUCT.html#enforcement",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at moritz.maehr@gmail.com. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.",
    "crumbs": [
      "Home",
      "Project",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "href": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.",
    "crumbs": [
      "Home",
      "Project",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#attribution",
    "href": "CODE_OF_CONDUCT.html#attribution",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla‚Äôs code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.",
    "crumbs": [
      "Home",
      "Project",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "paper/paper.html",
    "href": "paper/paper.html",
    "title": "Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections",
    "section": "",
    "text": "Digital archives promised to democratize access to cultural heritage, yet a significant portion of visual historical content remains inaccessible to blind and low-vision readers. Many digitized photographs, maps, manuscripts, and other images lack descriptive alternative text (alt-text), creating an epistemic barrier to the past. This perpetuates an asymmetry in sensory access to history, where sighted people hold privileged insight into visual sources while others are excluded. Making images legible through text is more than a technical fix‚Äîit is a matter of historical justice and inclusivity in digital humanities. Even beyond vision-impaired users, rich image descriptions can aid others, such as neurodivergent readers who benefit from explicit detail that sighted users might glean implicitly (Cecilia, Moussouri, and Fraser 2023a).\nAlt-text itself is not new: the HTML alt attribute dates back to the 1990s to support accessibility. However, providing high-quality image descriptions has often been a secondary priority in scholarly communication (Cecilia, Moussouri, and Fraser 2023b). Crafting alt-text is labor-intensive and typically left to authors or curators as a final step, if done at all. The burden often falls on sighted experts to determine what information is or is not included in an image‚Äôs description, an ethical responsibility that only the content‚Äôs author can fully shoulder. Author-generated descriptions are valued for capturing contextual meaning that automated tools might miss. They can greatly enhance the accessibility, searchability, and archivability of digital scholarship. Yet in practice, many projects‚Äîespecially smaller public history initiatives‚Äîlack the resources to implement accessibility from the start. The result is that visual evidence remains ‚Äúunseen‚Äù by those who rely on assistive technologies.\nRecent advances in multimodal AI offer a potential remedy. Vision-Language Models (VLMs) such as OpenAI‚Äôs GPT-4o, Google‚Äôs Gemini 2.5, and open-source systems like Meta‚Äôs Llama 4 or Mistral‚Äôs Pixtral now claim near-human performance in image description tasks. These models can ingest an image and generate a caption or description, essentially simulating the interpretive act of a human describer. If these models could produce alt-text that is both high-quality and historically informed as well as conformant with the Web Content Accessibility Guidelines (WCAG 2.2) and the Web Accessibility Initiative (WAI) of the World Wide Web Consortium (W3C), this would dramatically reduce the human effort required to remediate large collections. Heritage institutions could then scale up accessibility by generating alt-text for thousands of images. Consequently, the ‚Äúreadership‚Äù of digital archives would expand to include those who were previously excluded.\nHowever, adopting automated captioning in a heritage context raises critical questions about truth, evidence, and authenticity. Delegating descriptive labor to machines is not a neutral technical fix; it is an act imbued with values and biases. Deciding what details to include in an image‚Äôs description is technically difficult and ethically fraught, especially for historical images depicting people or sensitive cultural content. Vision models trained on general web images may inject anachronistic terms or biases (e.g., misidentifying a 1920s street scene as ‚ÄúVictorian‚Äù), reinforce curatorial blind spots, or omit crucial context that a human historian would provide. There is also the danger of techno-ableism (Shew 2023), where blind users‚Äô needs are superficially addressed by technology without truly empowering them or respecting their perspectives. Uncritical use of AI could inadvertently recentre the sighted, algorithmic point of view rather than the lived experience of those using the alt-text.\nIn this work, we argue that AI-generated alt-text for historical collections is a pivotal test case for the entanglement of AI innovation, archival practice, and disability justice. But can a machine ‚Äúsee‚Äù history as we do? If a model can convincingly describe a photograph from 100 years ago, how does that change the way we verify and trust such descriptions? Embracing this kind of ‚Äúmachine vision‚Äù in historical scholarship may require new protocols akin to earlier paradigm shifts (for example, the move from handwritten catalog cards to MARC records, or from microfilm to digital scans). Just as those changes demanded critical awareness of how tools shape historical discovery, the use of AI-generated descriptions demands a new hermeneutic of suspicion. We must learn to critically read machine-generated metadata, much as we read any human-produced finding aid or annotation (Fickers 2022).\nThe central purpose of our study is to assess whether and how current AI models can serve as accessibility assistants in a digital history workflow, and to develop a critical framework for using them responsibly. Our approach is interdisciplinary, blending computational experimentation with qualitative, historiographically informed analysis. Concretely, we plan to experiment with state-of-the-art multimodal models to generate alt-text for a real-world public history collection, and we will evaluate the results for accessibility compliance, historical accuracy, and ethical soundness. By doing so, we aim to illuminate both the opportunities and the pitfalls of integrating AI into inclusive humanities scholarship. Each AI-generated caption is treated not just as metadata but as an interpretive act‚Äîone that can be scrutinized like any primary source.\nTo guide this inquiry, we pose the following research questions:\n\n\nRework the section above the research questions so that it complies with these ides:\n\n1. Feasibility\n\nDefine feasibility as three observables:\n\nCoverage: % images that yield a non-empty, non-refusal alt text on first pass.\nThroughput: extrapolated/theoretical images/hour.\nUnit cost: CHF per alt text (API).\n\nDemonstrate with:\n\nA small, curated set of before/after exemplars across types to show plausibility.\nA compliance heuristic pass rate: correct ‚Äúcomplex image‚Äù pattern, length bounds based on image type, banished phrases (‚ÄúBild von‚Ä¶‚Äù), presence/absence of visible text handling.\n\nNo large-N UX test needed for ‚Äúfeasibility.‚Äù Save user studies for later work. Anchor claims to logs and heuristics.\n\n2. Quality and authenticity\n\nAvoid absolute scoring for ‚Äúfactual accuracy.‚Äù Use relative preference:\n\nDesign: 4 models ‚Üí 6 pairs per image. 6 Domain experts pick the better alt text per pair (2AFC).\nEstimate model strengths with a Bradley‚ÄìTerry model. Report coefficients with CIs. Do overall and by era and type.\nReliability: Kendall‚Äôs W on ranks.\n\nTargeted objective checks is an idea not realized in this paper yet, but could be future work:\n\nMetadata consistency: forbid contradictions with known year/place/creator; count contradictions.\nText-image handling: for scans, check that visible text is mentioned or flagged for longdesc.\nHallucination audit: sample-based review for invented entities.\n\nRationale: alt texts are short; absolute ‚Äúcompleteness‚Äù is ill-posed. Relative judgments scale and are defensible.\n\n3. Ethics and governance\n\nFor a later stage of the research, do case-based audits:\n\nCurate vignettes for people images, sensitive symbols, derogatory historical text, colonial scenes, funerary objects.\nFor each, show 4 model outputs, annotate harm vectors: speculative identity, euphemism, unnecessary salience, tone, omission of slurs vs contextualization.\nDerive editorial rules: what to elide vs quote, when to defer to long description, when to avoid identity labels unless documented.\n\nOutput of this section is policy, not a number. Tie to disability language guides and CH practice, but keep examples empirical."
  },
  {
    "objectID": "paper/paper.html#dataset-for-alt-text-generation-and-evaluation",
    "href": "paper/paper.html#dataset-for-alt-text-generation-and-evaluation",
    "title": "Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections",
    "section": "Dataset for Alt-Text Generation and Evaluation",
    "text": "Dataset for Alt-Text Generation and Evaluation\nFrom the available project corpus, we compiled a dataset where each entry consists of an image and its metadata (e.g.¬†title, date, description) of 100 images from the Stadt.Geschichte.Basel Research Data Platform to be used for alt-text generation. To prompt the models as described below, we use the files in JPG format, at a standardized size of 800x800 pixels for computational efficiency, and their corresponding metadata in JSON format. We intend to release the dataset of images, metadata, and model-generated descriptions as a benchmark for future research, following the conference‚Äôs emphasis on open data and reproducibility.\nThe dataset was manually curated prior to test runs of the AI-generation process and slightly adapted after first trials. We aimed to have a selection reflecting both the heterogeneity of data types and the diverse historical content that make up the Stadt.Geschichte.Basel collection, while keeping ourselves to a size that is feasible both for processing in our pipeline and for the conduction of an evaluation survey. Although the dataset was limited to the media items available on the Research Data Platform at the time of writing, we strove to create a balanced selection across data types, eras, languages, and geographical context.\nFor the alt-text evaluation survey, we created a smaller subset of 20 items from the original 100-item dataset. This survey subset was selected to maintain representativeness across the same dimensions ‚Äì type, era, language, geography ‚Äì while being manageable for expert reviewers to assess within a reasonable time frame.\n\nDistribution by Type\nOur dataset comprising 100 items was designed to represent the various types of media present in the overall Stadt.Geschichte.Basel collection. We categorized items into ten distinct types, and selected ten items from each of them, except for flowcharts, which are not as prevalent in our data.\n\n\n\nType\nDataset\nSurvey\n\n\n\n\nArt\n12\n0\n\n\nObject\n13\n2\n\n\nPhotograph (Archaeological Site)\n10\n2\n\n\nPhotograph (Historical Scenes)\n10\n2\n\n\nScan of Newspapers, Posters, Lists etc.\n10\n3\n\n\nDrawing (Archaeological Reconstruction)\n10\n3\n\n\nDrawing (Historical Drawing)\n10\n2\n\n\nMap\n10\n2\n\n\nDiagram (Statistics)\n10\n2\n\n\nDiagram (Flowchart, Schema etc.)\n5\n2\n\n\nTotal\n100\n20\n\n\n\n\n\nDistribution by Era\nWith regards to the historical eras represented in the subset, we aimed to cover the full chronological span of the Stadt.Geschichte.Basel project. Since each item is tagged with an era in the metadata, we could systematically select items across periods in a way that resembles that distribution in the whole research data set (Project Count, at the time of writing). Items from some eras, e.g.¬†Antike and 21. Jahrhundert, are less frequent in the overall collection which is reflected in a lower representation in our dataset.\n\n\n\nEra\nProject (Oct 25)\nDataset\nSurvey\n\n\n\n\nFr√ºhgeschichte\n95\n11\n3\n\n\nAntike\n8\n3\n0\n\n\nMittelalter\n134\n16\n2\n\n\nFr√ºhe Neuzeit\n159\n21\n3\n\n\n19. Jahrhundert\n162\n19\n5\n\n\n20. Jahrhundert\n194\n25\n5\n\n\n21. Jahrhundert\n28\n5\n2\n\n\nTotal\n780\n100\n20\n\n\n\n\n\nDistribution across Eras and Types\nOur data set covers all eras and all data types, but due to its historical nature, not all data types appear in all eras. In both our selection and survey subset, we tried to find an overall balance between data types and eras.\n\n\n\nDistribution of item types across historical eras in the selected subset of 100 items from the Stadt.Geschichte.Basel collection.\n\n\nThe survey subset accentuates the characteristics of our data set:\n\n\n\nDistribution of item types across historical eras in the selected subset of 100 items from the Stadt.Geschichte.Basel collection.\n\n\n\nfix ‚ÄúFr√ºhgeschichte‚Äù tag for one photograph\nfind better label for ‚ÄòArt‚Äô: almost all items are paintings, but a few drawings. Overlap with ‚ÄòDrawing (Historical Drawing)‚Äô.\ntranslate ‚Äòera‚Äô values to English?\n\nWe dropped Art items and the Antike era from the survey subset due to their low prevalence in our corpus and instead added other items that we deemed more interesting to evaluate within this study. This includes items with complex visual structures (e.g., maps, diagrams), items with visible text (e.g., scans of newspapers or posters) as well as items with potentially sensitive content (e.g., depictions of humans, historical content with derogatory and/or racist terminology).\n\n\nLanguage Distribution\nOur research data collection primarily contains items in German, with a small number of items in Latin, French and Dutch. We aimed to reflect this language distribution in our selection.\n\n\n\nLanguage\nProject (Oct 25)\nDataset\nSurvey\n\n\n\n\nGerman\n1573\n92\n19\n\n\nLatin\n14\n7\n1\n\n\nFrench\n5\n1\n0\n\n\nDutch\n2\n0\n0\n\n\n\nIn a similar vein, we wanted to take into account different typographic styles. However, since we are working with 800x800 pixel JPG image thumbnails, writing is not fully legible in many cases and thus only a minor factor in the selection process.\n\n\nSpatial Context\nWhile geospatial context is not a part of our data model, most items in the Stadt.Geschichte.Basel collection can be associated with specific locations in Basel or elsewhere. The geographical distribution of the collection items did not influence our selection process directly, but a rough categorization was done afterwards to see whether differences in geographical scope are represented in our dataset.\n\n\n\nSpatial Context\nDataset\nSurvey\n\n\n\n\nCity of Basel\n60\n14\n\n\nBasel Region/Northwestern Switzerland/Upper Rhine\n16\n2\n\n\nSwitzerland\n6\n0\n\n\nSwitzerland and Neighbouring Countries\n5\n1\n\n\nEurope\n3\n1\n\n\nWorldwide\n5\n2\n\n\nNA\n5\n0"
  },
  {
    "objectID": "paper/paper.html#dataset-limitations",
    "href": "paper/paper.html#dataset-limitations",
    "title": "Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections",
    "section": "Dataset Limitations",
    "text": "Dataset Limitations\nA number of limitations apply to our dataset. For instance, the process of uploading media items to the Research Data Platform is ongoing, meaning that not all items from the printed volumes are yet available online. Therefore, our selection was constrained to the items that were already accessible at the time of writing. The number of eligible items was further reduced by excluding items that are only available with placeholder images on our platform due to copyright restrictions. The validation process for the metadata used by the models is still ongoing, meaning that some metadata fields may contain inaccuracies or inconsistencies that could affect model performance. However, the reduced size of our dataset allows us to manually verify the metadata for each selected item to prevent faulty model inputs. Additionally, due to the typesetting workflow during the production of the printed volumes, some collection items had to be split up into different files. Mostly, this pertains to maps and charts where the legend is provided in a second image file, separate from the main figure. The connection between these files is made explicit in our metadata, but the models only receive one image file as input at a time, leading to some loss of information that would be visually available to a human reader. This could result in a lower description quality."
  },
  {
    "objectID": "paper/paper.html#alt-text-generation-pipeline",
    "href": "paper/paper.html#alt-text-generation-pipeline",
    "title": "Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections",
    "section": "Alt-Text Generation Pipeline",
    "text": "Alt-Text Generation Pipeline\n\n\nRework the section above so that it complies with these ideas:\n\nWe systematically varied prompt roles and placement, comparing instruction blocks in the system message versus the user turn, and front-loading versus trailing constraints. Following evidence that models privilege information at the beginning or end of long contexts, we fixed normative requirements (WCAG 2.2 target, de-CH style, length limits, handling of decorative/functional/complex images) in the system prompt and kept the user message minimal and image-bound to reduce ‚Äúlost-in-the-middle‚Äù effects. The user turn injected collection-specific metadata‚Äîtitle, description, EDTF date, era, creator/publisher/source‚Äîand a concise Nutzungskontext, then the image URL. Adding this structured context markedly improved specificity, reduced refusals, and lowered hallucinations, consistent with retrieval-style findings that supplying external, task-relevant evidence boosts generation quality and faithfulness. Concretely, the prompt is a two-part template: (1) a stable system scaffold that encodes accessibility rules and output format (‚Äúonly the alt text‚Äù, max 125 chars, no ‚ÄúBild von‚Äù), and (2) a per-item user payload that lists metadata as bullet points plus the image, so the model can align linguistic content with visual features. (TODO see https://aclanthology.org/2024.tacl-1.9/)\nGeneration and Post-processing: Using this prompt, we will run each image through each of the four models, yielding up to four candidate descriptions per image. The generation process will be automated via a Python script (using an API wrapper or library for each model). We anticipate producing around 400 candidate alt-texts (4 per image for n=100 images). After generation, minimal post-processing will be applied. In particular, we will strip any extraneous phrases if a model fails to follow instructions exactly (e.g., some might prepend ‚ÄúAlt-Text:‚Äù or polite greetings, which we will remove). We will not otherwise modify the content of the AI outputs at this stage. All results will be stored along with metadata and model identifiers for evaluation.\nIf a model refuses to describe an image due to some built-in safety filter (misidentifying a historical photograph as sensitive content), we will handle those on a case-by-case basis by leaving that image for human description. Overall, this pipeline is designed to maximize coverage (getting at least one description for every image) while maintaining quality through careful prompting.\n\nPipeline Overview (Mermaid)\n\n\n%%| label: fig-pipeline\n%%| fig-cap: Alt-text generation, survey, and analysis pipeline (best model by consensus and cost).\nflowchart LR\n  %% Generation\n  subgraph GEN[src/generate_alt_text.py ‚Äî Generation]\n    M0[Fetch metadata JSON (METADATA_URL)]\n    M1[Select MEDIA_IDS]\n    M2[Build prompts with metadata]\n    M3[Query MODELS via OpenRouter]\n    M4[Persist raw responses under runs/&lt;timestamp&gt;/raw/*.json]\n    M5[Assemble wide table CSV/JSONL/Parquet]\n    M6[Export questions.csv for survey]\n    M0 --&gt; M1 --&gt; M2 --&gt; M3 --&gt; M4\n    M3 --&gt; M5 --&gt; M6\n  end\n\n  %% Survey\n  subgraph SUR[survey/* ‚Äî Expert Survey]\n    S0[Load questions.csv]\n    S1[Present options per image]\n    S2[Collect expert choices + comments]\n    S3[Write survey/results.csv]\n    S0 --&gt; S1 --&gt; S2 --&gt; S3\n  end\n\n  %% Analysis\n  subgraph ANA[analysis ‚Äî Model Comparison]\n    A0[Aggregate votes per model]\n    A1[Compute consensus win rate]\n    A2[Join with cost table]\n    A3[(Best model: openai/gpt-4o-mini)]\n    S3 --&gt; A0 --&gt; A1 --&gt; A2 --&gt; A3\n  end\n\n  M6 --&gt; S0"
  },
  {
    "objectID": "paper/paper.html#evaluation-strategy",
    "href": "paper/paper.html#evaluation-strategy",
    "title": "Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections",
    "section": "Evaluation Strategy",
    "text": "Evaluation Strategy\nOur evaluation of the AI-generated alt-text will address both accessibility compliance and historical accuracy in line with the research questions. We describe the planned evaluation steps below. All evaluation will be done on a representative subset of the data (approximately 100 images) due to time constraints, with the aim of scaling up later.\n\n\nRework the section above so that it complies with the ideas outlined previously."
  },
  {
    "objectID": "paper/index.html",
    "href": "paper/index.html",
    "title": "Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections",
    "section": "",
    "text": "Digital archives promised to democratize access to cultural heritage, yet a significant portion of visual historical content remains inaccessible to blind and low-vision readers. Many digitized photographs, maps, manuscripts, and other images lack descriptive alternative text (alt-text), creating an epistemic barrier to the past. This perpetuates an asymmetry in sensory access to history, where sighted people hold privileged insight into visual sources while others are excluded. Making images legible through text is more than a technical fix‚Äîit is a matter of historical justice and inclusivity in digital humanities. Even beyond vision-impaired users, rich image descriptions can aid others, such as neurodivergent readers who benefit from explicit detail that sighted users might glean implicitly (Cecilia, Moussouri, and Fraser 2023a).\nAlt-text itself is not new: the HTML alt attribute dates back to the 1990s to support accessibility. However, providing high-quality image descriptions has often been a secondary priority in scholarly communication (Cecilia, Moussouri, and Fraser 2023b). Crafting alt-text is labor-intensive and typically left to authors or curators as a final step, if done at all. The burden often falls on sighted experts to determine what information is or is not included in an image‚Äôs description, an ethical responsibility that only the content‚Äôs author can fully shoulder. Author-generated descriptions are valued for capturing contextual meaning that automated tools might miss. They can greatly enhance the accessibility, searchability, and archivability of digital scholarship. Yet in practice, many projects‚Äîespecially smaller public history initiatives‚Äîlack the resources to implement accessibility from the start. The result is that visual evidence remains ‚Äúunseen‚Äù by those who rely on assistive technologies.\nRecent advances in multimodal AI offer a potential remedy. Vision-Language Models (VLMs) such as OpenAI‚Äôs GPT-4o, Google‚Äôs Gemini 2.5, and open-source systems like Meta‚Äôs Llama 4 or Mistral‚Äôs Pixtral now claim near-human performance in image description tasks. These models can ingest an image and generate a caption or description, essentially simulating the interpretive act of a human describer. If these models could produce alt-text that is both high-quality and historically informed as well as conformant with the Web Content Accessibility Guidelines (WCAG 2.2) and the Web Accessibility Initiative (WAI) of the World Wide Web Consortium (W3C), this would dramatically reduce the human effort required to remediate large collections. Heritage institutions could then scale up accessibility by generating alt-text for thousands of images. Consequently, the ‚Äúreadership‚Äù of digital archives would expand to include those who were previously excluded.\nHowever, adopting automated captioning in a heritage context raises critical questions about truth, evidence, and authenticity. Delegating descriptive labor to machines is not a neutral technical fix; it is an act imbued with values and biases. Deciding what details to include in an image‚Äôs description is technically difficult and ethically fraught, especially for historical images depicting people or sensitive cultural content. Vision models trained on general web images may inject anachronistic terms or biases (e.g., misidentifying a 1920s street scene as ‚ÄúVictorian‚Äù), reinforce curatorial blind spots, or omit crucial context that a human historian would provide. There is also the danger of techno-ableism (Shew 2023), where blind users‚Äô needs are superficially addressed by technology without truly empowering them or respecting their perspectives. Uncritical use of AI could inadvertently recentre the sighted, algorithmic point of view rather than the lived experience of those using the alt-text.\nIn this work, we argue that AI-generated alt-text for historical collections is a pivotal test case for the entanglement of AI innovation, archival practice, and disability justice. But can a machine ‚Äúsee‚Äù history as we do? If a model can convincingly describe a photograph from 100 years ago, how does that change the way we verify and trust such descriptions? Embracing this kind of ‚Äúmachine vision‚Äù in historical scholarship may require new protocols akin to earlier paradigm shifts (for example, the move from handwritten catalog cards to MARC records, or from microfilm to digital scans). Just as those changes demanded critical awareness of how tools shape historical discovery, the use of AI-generated descriptions demands a new hermeneutic of suspicion. We must learn to critically read machine-generated metadata, much as we read any human-produced finding aid or annotation (Fickers 2022).\nThe central purpose of our study is to assess whether and how current AI models can serve as accessibility assistants in a digital history workflow, and to develop a critical framework for using them responsibly. Our approach is interdisciplinary, blending computational experimentation with qualitative, historiographically informed analysis. Concretely, we plan to experiment with state-of-the-art multimodal models to generate alt-text for a real-world public history collection, and we will evaluate the results for accessibility compliance, historical accuracy, and ethical soundness. By doing so, we aim to illuminate both the opportunities and the pitfalls of integrating AI into inclusive humanities scholarship. Each AI-generated caption is treated not just as metadata but as an interpretive act‚Äîone that can be scrutinized like any primary source.\nTo guide this inquiry, we pose the following research questions:\n\n\nRework the section above the research questions so that it complies with these ides:\n\n1. Feasibility\n\nDefine feasibility as three observables:\n\nCoverage: % images that yield a non-empty, non-refusal alt text on first pass.\nThroughput: extrapolated/theoretical images/hour.\nUnit cost: CHF per alt text (API).\n\nDemonstrate with:\n\nA small, curated set of before/after exemplars across types to show plausibility.\nA compliance heuristic pass rate: correct ‚Äúcomplex image‚Äù pattern, length bounds based on image type, banished phrases (‚ÄúBild von‚Ä¶‚Äù), presence/absence of visible text handling.\n\nNo large-N UX test needed for ‚Äúfeasibility.‚Äù Save user studies for later work. Anchor claims to logs and heuristics.\n\n2. Quality and authenticity\n\nAvoid absolute scoring for ‚Äúfactual accuracy.‚Äù Use relative preference:\n\nDesign: 4 models ‚Üí 6 pairs per image. 6 Domain experts pick the better alt text per pair (2AFC).\nEstimate model strengths with a Bradley‚ÄìTerry model. Report coefficients with CIs. Do overall and by era and type.\nReliability: Kendall‚Äôs W on ranks.\n\nTargeted objective checks is an idea not realized in this paper yet, but could be future work:\n\nMetadata consistency: forbid contradictions with known year/place/creator; count contradictions.\nText-image handling: for scans, check that visible text is mentioned or flagged for longdesc.\nHallucination audit: sample-based review for invented entities.\n\nRationale: alt texts are short; absolute ‚Äúcompleteness‚Äù is ill-posed. Relative judgments scale and are defensible.\n\n3. Ethics and governance\n\nFor a later stage of the research, do case-based audits:\n\nCurate vignettes for people images, sensitive symbols, derogatory historical text, colonial scenes, funerary objects.\nFor each, show 4 model outputs, annotate harm vectors: speculative identity, euphemism, unnecessary salience, tone, omission of slurs vs contextualization.\nDerive editorial rules: what to elide vs quote, when to defer to long description, when to avoid identity labels unless documented.\n\nOutput of this section is policy, not a number. Tie to disability language guides and CH practice, but keep examples empirical.",
    "crumbs": [
      "Home",
      "Research",
      "Paper"
    ]
  },
  {
    "objectID": "paper/index.html#dataset-for-alt-text-generation-and-evaluation",
    "href": "paper/index.html#dataset-for-alt-text-generation-and-evaluation",
    "title": "Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections",
    "section": "3.1 Dataset for Alt-Text Generation and Evaluation",
    "text": "3.1 Dataset for Alt-Text Generation and Evaluation\nFrom the available project corpus, we compiled a dataset where each entry consists of an image and its metadata (e.g.¬†title, date, description) of 100 images from the Stadt.Geschichte.Basel Research Data Platform to be used for alt-text generation. To prompt the models as described below, we use the files in JPG format, at a standardized size of 800x800 pixels for computational efficiency, and their corresponding metadata in JSON format. We intend to release the dataset of images, metadata, and model-generated descriptions as a benchmark for future research, following the conference‚Äôs emphasis on open data and reproducibility.\nThe dataset was manually curated prior to test runs of the AI-generation process and slightly adapted after first trials. We aimed to have a selection reflecting both the heterogeneity of data types and the diverse historical content that make up the Stadt.Geschichte.Basel collection, while keeping ourselves to a size that is feasible both for processing in our pipeline and for the conduction of an evaluation survey. Although the dataset was limited to the media items available on the Research Data Platform at the time of writing, we strove to create a balanced selection across data types, eras, languages, and geographical context.\nFor the alt-text evaluation survey, we created a smaller subset of 20 items from the original 100-item dataset. This survey subset was selected to maintain representativeness across the same dimensions ‚Äì type, era, language, geography ‚Äì while being manageable for expert reviewers to assess within a reasonable time frame.\n\n3.1.1 Distribution by Type\nOur dataset comprising 100 items was designed to represent the various types of media present in the overall Stadt.Geschichte.Basel collection. We categorized items into ten distinct types, and selected ten items from each of them, except for flowcharts, which are not as prevalent in our data.\n\n\n\nType\nDataset\nSurvey\n\n\n\n\nArt\n12\n0\n\n\nObject\n13\n2\n\n\nPhotograph (Archaeological Site)\n10\n2\n\n\nPhotograph (Historical Scenes)\n10\n2\n\n\nScan of Newspapers, Posters, Lists etc.\n10\n3\n\n\nDrawing (Archaeological Reconstruction)\n10\n3\n\n\nDrawing (Historical Drawing)\n10\n2\n\n\nMap\n10\n2\n\n\nDiagram (Statistics)\n10\n2\n\n\nDiagram (Flowchart, Schema etc.)\n5\n2\n\n\nTotal\n100\n20\n\n\n\n\n\n3.1.2 Distribution by Era\nWith regards to the historical eras represented in the subset, we aimed to cover the full chronological span of the Stadt.Geschichte.Basel project. Since each item is tagged with an era in the metadata, we could systematically select items across periods in a way that resembles that distribution in the whole research data set (Project Count, at the time of writing). Items from some eras, e.g.¬†Antike and 21. Jahrhundert, are less frequent in the overall collection which is reflected in a lower representation in our dataset.\n\n\n\nEra\nProject (Oct 25)\nDataset\nSurvey\n\n\n\n\nFr√ºhgeschichte\n95\n11\n3\n\n\nAntike\n8\n3\n0\n\n\nMittelalter\n134\n16\n2\n\n\nFr√ºhe Neuzeit\n159\n21\n3\n\n\n19. Jahrhundert\n162\n19\n5\n\n\n20. Jahrhundert\n194\n25\n5\n\n\n21. Jahrhundert\n28\n5\n2\n\n\nTotal\n780\n100\n20\n\n\n\n\n\n3.1.3 Distribution across Eras and Types\nOur data set covers all eras and all data types, but due to its historical nature, not all data types appear in all eras. In both our selection and survey subset, we tried to find an overall balance between data types and eras.\n\n\n\nDistribution of item types across historical eras in the selected subset of 100 items from the Stadt.Geschichte.Basel collection.\n\n\nThe survey subset accentuates the characteristics of our data set:\n\n\n\nDistribution of item types across historical eras in the selected subset of 100 items from the Stadt.Geschichte.Basel collection.\n\n\n\nfix ‚ÄúFr√ºhgeschichte‚Äù tag for one photograph\nfind better label for ‚ÄòArt‚Äô: almost all items are paintings, but a few drawings. Overlap with ‚ÄòDrawing (Historical Drawing)‚Äô.\ntranslate ‚Äòera‚Äô values to English?\n\nWe dropped Art items and the Antike era from the survey subset due to their low prevalence in our corpus and instead added other items that we deemed more interesting to evaluate within this study. This includes items with complex visual structures (e.g., maps, diagrams), items with visible text (e.g., scans of newspapers or posters) as well as items with potentially sensitive content (e.g., depictions of humans, historical content with derogatory and/or racist terminology).\n\n\n3.1.4 Language Distribution\nOur research data collection primarily contains items in German, with a small number of items in Latin, French and Dutch. We aimed to reflect this language distribution in our selection.\n\n\n\nLanguage\nProject (Oct 25)\nDataset\nSurvey\n\n\n\n\nGerman\n1573\n92\n19\n\n\nLatin\n14\n7\n1\n\n\nFrench\n5\n1\n0\n\n\nDutch\n2\n0\n0\n\n\n\nIn a similar vein, we wanted to take into account different typographic styles. However, since we are working with 800x800 pixel JPG image thumbnails, writing is not fully legible in many cases and thus only a minor factor in the selection process.\n\n\n3.1.5 Spatial Context\nWhile geospatial context is not a part of our data model, most items in the Stadt.Geschichte.Basel collection can be associated with specific locations in Basel or elsewhere. The geographical distribution of the collection items did not influence our selection process directly, but a rough categorization was done afterwards to see whether differences in geographical scope are represented in our dataset.\n\n\n\nSpatial Context\nDataset\nSurvey\n\n\n\n\nCity of Basel\n60\n14\n\n\nBasel Region/Northwestern Switzerland/Upper Rhine\n16\n2\n\n\nSwitzerland\n6\n0\n\n\nSwitzerland and Neighbouring Countries\n5\n1\n\n\nEurope\n3\n1\n\n\nWorldwide\n5\n2\n\n\nNA\n5\n0",
    "crumbs": [
      "Home",
      "Research",
      "Paper"
    ]
  },
  {
    "objectID": "paper/index.html#dataset-limitations",
    "href": "paper/index.html#dataset-limitations",
    "title": "Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections",
    "section": "3.2 Dataset Limitations",
    "text": "3.2 Dataset Limitations\nA number of limitations apply to our dataset. For instance, the process of uploading media items to the Research Data Platform is ongoing, meaning that not all items from the printed volumes are yet available online. Therefore, our selection was constrained to the items that were already accessible at the time of writing. The number of eligible items was further reduced by excluding items that are only available with placeholder images on our platform due to copyright restrictions. The validation process for the metadata used by the models is still ongoing, meaning that some metadata fields may contain inaccuracies or inconsistencies that could affect model performance. However, the reduced size of our dataset allows us to manually verify the metadata for each selected item to prevent faulty model inputs. Additionally, due to the typesetting workflow during the production of the printed volumes, some collection items had to be split up into different files. Mostly, this pertains to maps and charts where the legend is provided in a second image file, separate from the main figure. The connection between these files is made explicit in our metadata, but the models only receive one image file as input at a time, leading to some loss of information that would be visually available to a human reader. This could result in a lower description quality.",
    "crumbs": [
      "Home",
      "Research",
      "Paper"
    ]
  },
  {
    "objectID": "paper/index.html#alt-text-generation-pipeline",
    "href": "paper/index.html#alt-text-generation-pipeline",
    "title": "Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections",
    "section": "5.1 Alt-Text Generation Pipeline",
    "text": "5.1 Alt-Text Generation Pipeline\n\n\nRework the section above so that it complies with these ideas:\n\nWe systematically varied prompt roles and placement, comparing instruction blocks in the system message versus the user turn, and front-loading versus trailing constraints. Following evidence that models privilege information at the beginning or end of long contexts, we fixed normative requirements (WCAG 2.2 target, de-CH style, length limits, handling of decorative/functional/complex images) in the system prompt and kept the user message minimal and image-bound to reduce ‚Äúlost-in-the-middle‚Äù effects. The user turn injected collection-specific metadata‚Äîtitle, description, EDTF date, era, creator/publisher/source‚Äîand a concise Nutzungskontext, then the image URL. Adding this structured context markedly improved specificity, reduced refusals, and lowered hallucinations, consistent with retrieval-style findings that supplying external, task-relevant evidence boosts generation quality and faithfulness. Concretely, the prompt is a two-part template: (1) a stable system scaffold that encodes accessibility rules and output format (‚Äúonly the alt text‚Äù, max 125 chars, no ‚ÄúBild von‚Äù), and (2) a per-item user payload that lists metadata as bullet points plus the image, so the model can align linguistic content with visual features. (TODO see https://aclanthology.org/2024.tacl-1.9/)\nGeneration and Post-processing: Using this prompt, we will run each image through each of the four models, yielding up to four candidate descriptions per image. The generation process will be automated via a Python script (using an API wrapper or library for each model). We anticipate producing around 400 candidate alt-texts (4 per image for n=100 images). After generation, minimal post-processing will be applied. In particular, we will strip any extraneous phrases if a model fails to follow instructions exactly (e.g., some might prepend ‚ÄúAlt-Text:‚Äù or polite greetings, which we will remove). We will not otherwise modify the content of the AI outputs at this stage. All results will be stored along with metadata and model identifiers for evaluation.\nIf a model refuses to describe an image due to some built-in safety filter (misidentifying a historical photograph as sensitive content), we will handle those on a case-by-case basis by leaving that image for human description. Overall, this pipeline is designed to maximize coverage (getting at least one description for every image) while maintaining quality through careful prompting.\n\n5.1.1 Pipeline Overview (Mermaid)\n\n\n%%| label: fig-pipeline\n%%| fig-cap: Alt-text generation, survey, and analysis pipeline (best model by consensus and cost).\nflowchart LR\n  %% Generation\n  subgraph GEN[src/generate_alt_text.py ‚Äî Generation]\n    M0[Fetch metadata JSON (METADATA_URL)]\n    M1[Select MEDIA_IDS]\n    M2[Build prompts with metadata]\n    M3[Query MODELS via OpenRouter]\n    M4[Persist raw responses under runs/&lt;timestamp&gt;/raw/*.json]\n    M5[Assemble wide table CSV/JSONL/Parquet]\n    M6[Export questions.csv for survey]\n    M0 --&gt; M1 --&gt; M2 --&gt; M3 --&gt; M4\n    M3 --&gt; M5 --&gt; M6\n  end\n\n  %% Survey\n  subgraph SUR[survey/* ‚Äî Expert Survey]\n    S0[Load questions.csv]\n    S1[Present options per image]\n    S2[Collect expert choices + comments]\n    S3[Write survey/results.csv]\n    S0 --&gt; S1 --&gt; S2 --&gt; S3\n  end\n\n  %% Analysis\n  subgraph ANA[analysis ‚Äî Model Comparison]\n    A0[Aggregate votes per model]\n    A1[Compute consensus win rate]\n    A2[Join with cost table]\n    A3[(Best model: openai/gpt-4o-mini)]\n    S3 --&gt; A0 --&gt; A1 --&gt; A2 --&gt; A3\n  end\n\n  M6 --&gt; S0",
    "crumbs": [
      "Home",
      "Research",
      "Paper"
    ]
  },
  {
    "objectID": "paper/index.html#evaluation-strategy",
    "href": "paper/index.html#evaluation-strategy",
    "title": "Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections",
    "section": "5.2 Evaluation Strategy",
    "text": "5.2 Evaluation Strategy\nOur evaluation of the AI-generated alt-text will address both accessibility compliance and historical accuracy in line with the research questions. We describe the planned evaluation steps below. All evaluation will be done on a representative subset of the data (approximately 100 images) due to time constraints, with the aim of scaling up later.\n\n\nRework the section above so that it complies with the ideas outlined previously.",
    "crumbs": [
      "Home",
      "Research",
      "Paper"
    ]
  },
  {
    "objectID": "abstract/index.html",
    "href": "abstract/index.html",
    "title": "Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections",
    "section": "",
    "text": "Digital archives promised to democratize access to cultural heritage, yet a significant portion of visual historical content remains inaccessible to blind and low-vision readers. Many digitized photographs, maps, manuscripts, and other images lack descriptive alternative text (alt-text), creating an epistemic barrier to the past. This perpetuates an asymmetry in sensory access to history, where sighted people hold privileged insight into visual sources while others are excluded. Making images legible through text is more than a technical fix‚Äîit is a matter of historical justice and inclusivity in digital humanities. Even beyond vision-impaired users, rich image descriptions can aid others, such as neurodivergent readers who benefit from explicit detail that sighted users might glean implicitly (Cecilia, Moussouri, and Fraser 2023a).\nAlt-text itself is not new: the HTML alt attribute dates back to the 1990s to support accessibility. However, providing high-quality image descriptions has often been a secondary priority in scholarly communication (Cecilia, Moussouri, and Fraser 2023b). Crafting alt-text is labor-intensive and typically left to authors or curators as a final step, if done at all. As Conrad (Conrad 2021) observes, the burden falls on sighted experts to determine what information is or is not included in an image‚Äôs description, an ethical responsibility that only the content‚Äôs author can fully shoulder. Author-generated descriptions are valued for capturing contextual meaning that automated tools might miss. They can greatly enhance the accessibility, searchability, and archivability of digital scholarship. Yet in practice, many projects‚Äîespecially smaller public history initiatives‚Äîlack the resources to implement accessibility from the start. The result is that visual evidence remains ‚Äúunseen‚Äù by those who rely on assistive technologies.\nRecent advances in multimodal AI offer a potential remedy. Vision-Language Models (VLMs) such as OpenAI‚Äôs GPT-4o, Google‚Äôs Gemini (Vision), and open-source systems like LLaVA-Next or Mistral‚Äôs Pixtral now claim near-human performance in image description tasks. These models can ingest an image and generate a caption or description, essentially simulating the interpretive act of a human describer. If guided properly, such models could produce high-quality, historically informed, and Web Content Accessibility Guidelines (WCAG 2.2)‚Äìconformant alt-text. This would dramatically reduce the human effort required to remediate large collections, enabling heritage institutions to scale up accessibility by generating alt-text for thousands of images. In turn, the ‚Äúreadership‚Äù of digital archives would expand to include those previously left out.\nHowever, adopting automated captioning in a heritage context raises critical questions about truth, evidence, and authenticity. Delegating descriptive labor to machines is not a neutral technical fix; it is an act imbued with values and biases. Deciding what details to include in an image‚Äôs description is technically difficult and ethically fraught, especially for historical images depicting people or sensitive cultural content. Vision models trained on general web images may inject anachronistic terms or biases (e.g., misidentifying a 1920s street scene as ‚ÄúVictorian‚Äù), reinforce curatorial blind spots, or omit crucial context that a human historian would provide. There is also the danger of techno-ableism (Shew 2023), where blind users‚Äô needs are superficially addressed by technology without truly empowering them or respecting their perspectives. Uncritical use of AI could inadvertently recentre the sighted, algorithmic point of view rather than the lived experience of those using the alt-text.\nIn this work, we argue that AI-generated alt-text for historical collections is a pivotal test case for the entanglement of AI innovation, archival practice, and disability justice. But can a machine ‚Äúsee‚Äù history as we do? If a model can convincingly describe a photograph from 100 years ago, how does that change the way we verify and trust such descriptions? Embracing this kind of ‚Äúmachine vision‚Äù in historical scholarship may require new protocols akin to earlier paradigm shifts (for example, the move from handwritten catalog cards to MARC records, or from microfilm to digital scans). Just as those changes demanded critical awareness of how tools shape historical discovery, the use of AI-generated descriptions demands a new hermeneutic of suspicion. We must learn to critically read machine-generated metadata, much as we read any human-produced finding aid or annotation (Fickers 2022).\nThe central purpose of our study is to assess whether and how current AI models can serve as accessibility assistants in a digital history workflow, and to develop a critical framework for using them responsibly. Our approach is interdisciplinary, blending computational experimentation with qualitative, historiographically informed analysis. Concretely, we plan to experiment with state-of-the-art multimodal models to generate alt-text for a real-world public history collection, and we will evaluate the results for accessibility compliance, historical accuracy, and ethical soundness. By doing so, we aim to illuminate both the opportunities and the pitfalls of integrating AI into inclusive humanities scholarship. Each AI-generated caption is treated not just as metadata but as an interpretive act‚Äîone that can be scrutinized like any primary source.\nTo guide this inquiry, we pose the following research questions:\n\nFeasibility: Can current vision-language models produce useful, WCAG 2.2‚Äìcompliant alt-text for complex historical images when provided with contextual metadata? We will examine whether models can meet accessibility guidelines (providing text alternatives that convey the same information as the image) and how the inclusion of metadata influences their output. We also consider the potential usefulness of these descriptions for both blind users and sighted users who may benefit from clear explanatory captions (Cecilia, Moussouri, and Fraser 2023a).\nQuality and Authenticity: How do domain experts (e.g., historians) rate AI-generated image descriptions in terms of factual accuracy, completeness, and usefulness for understanding historical content? We will evaluate the outputs for errors such as anachronisms, misidentifications, or hallucinated details, checking them against known facts from metadata and expert knowledge.\nEthics and Governance: What are the ethical implications of using AI to generate alt-text in heritage collections, and what human oversight or policy safeguards are required for responsible use? We will identify potential harms such as biased descriptions (e.g., normative terms), and address the broader question of how much interpretive agency should be ceded to AI in a curatorial context. We will explore strategies to mitigate these risks, including human-in-the-loop editing and transparency measures.\n\nBy answering these questions, our work will provide an empirical baseline for AI-assisted accessibility in the humanities. It will also offer a reflective critique, examining AI outputs as objects of study in their own right. In the following sections, we outline our data and methodology (Section 2), present initial observations from our experiments (Section 3), and discuss implications for digital humanities practice (Section 4), before concluding with planned next steps (Section 5).",
    "crumbs": [
      "Home",
      "Research",
      "Abstract"
    ]
  },
  {
    "objectID": "abstract/index.html#alt-text-generation-pipeline",
    "href": "abstract/index.html#alt-text-generation-pipeline",
    "title": "Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections",
    "section": "Alt-Text Generation Pipeline",
    "text": "Alt-Text Generation Pipeline\nModel Selection: We have selected four state-of-the-art vision-language models (as of mid-2025) to generate image descriptions. These represent a mix of proprietary and open-source systems: (1) GPT-4o (OpenAI‚Äôs multimodal GPT-4o), (2) Google Gemini (Vision), (3) LLaVA-Next (an open-source vision-LLM based on LLaMA-2, fine-tuned for vision-chat tasks), and (4) Mistral Pixtral (a vision-language model from Mistral‚Äôs NeMo framework). We include multiple models to gauge the range of performance and to see how open models compare to the cutting-edge commercial systems. All models are capable of accepting image input and returning a text description. Where possible, we use the latest available model checkpoints or API versions.\nPrompt Design: A key feature of our pipeline is providing each model with contextual metadata alongside the image, in order to ground the generation in relevant historical facts. We designed a prompt template (in the same language as the collection, i.e., German) that injects structured metadata fields and instructs the model to follow best practices for alt-text. In essence, the prompt tells the model that it is an accessibility assistant tasked with producing an alt-text for a cultural heritage image. It includes guidelines drawn from the WCAG 2.2 and accessibility literature on how to write good alt-text. For example, the prompt directs the model not to start with redundant phrases like ‚ÄúBild von‚Ä¶‚Äù (‚Äúimage of‚Ä¶‚Äù), to be concise (typically under $$120 characters for a simple informative image), and to include any essential visual text (like signs or captions visible in the image). It also asks the model to identify the type of image and adjust the response accordingly: e.g., if the image is a complex diagram or map, the model should produce a short alt-text plus note that a longer description will be provided; if the image is merely a photograph with informative content, a 1‚Äì2 sentence description suffices; if the image is mainly text (say a scanned document or poster), the model should either transcribe it (for short text like a sign) or indicate that a full transcription is available elsewhere for longer texts. These rules were distilled from accessibility resources (Project n.d.; World Wide Web Consortium 2023) to ensure the output serves blind users properly. An example snippet of our prompt template is: ‚Äù You are an expert in writing WCAG-compliant alt-text. The image comes from a history archive with metadata. Read the metadata and analyze the image. Determine the image type (informative photo, complex diagram/map, or text image) and produce the appropriate alt-text as per the guidelines‚Ä¶‚Äú*‚Äîfollowed by the specific instructions for each case. We have found in preliminary trials that including the complete metadata (title, date, etc.) in the prompt can prevent certain errors (for instance, knowing the year of the photo helps the model avoid describing attire as‚Äùmodern‚Äù). All models are prompted with the same template structure for consistency, and all outputs are requested in German (to match the collection‚Äôs context and end-user language).\nGeneration and Post-processing: Using this prompt, we will run each image through each of the four models, yielding up to four candidate descriptions per image. The generation process will be automated via a Python script (using an API wrapper or library for each model). We anticipate producing around 6,000 candidate alt-texts (4 per image for $$1,500 images). After generation, minimal post-processing will be applied. In particular, we will strip any extraneous phrases if a model fails to follow instructions exactly (e.g., some might prepend ‚ÄúAlt-Text:‚Äù or polite greetings, which we will remove). We will not otherwise modify the content of the AI outputs at this stage. All results will be stored along with metadata and model identifiers for evaluation.\nIf a model refuses to describe an image due to some built-in safety filter (misidentifying a historical photograph as sensitive content), we will handle those on a case-by-case basis by leaving that image for human description. Overall, this pipeline is designed to maximize coverage (getting at least one description for every image) while maintaining quality through careful prompting.",
    "crumbs": [
      "Home",
      "Research",
      "Abstract"
    ]
  },
  {
    "objectID": "abstract/index.html#evaluation-strategy",
    "href": "abstract/index.html#evaluation-strategy",
    "title": "Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections",
    "section": "Evaluation Strategy",
    "text": "Evaluation Strategy\nOur evaluation of the AI-generated alt-text will address both accessibility compliance and historical accuracy in line with the research questions. We describe the planned evaluation steps below. All evaluation will be done on a representative subset of the data (approximately 100 images) due to time constraints, with the aim of scaling up later.\n(a) Accessibility and WCAG Compliance: We will assess whether the AI outputs meet established accessibility guidelines for alt-text. This involves checking each description against a checklist of best practices (e.g., does the alt-text sufficiently describe the image‚Äôs important content and function? Does it avoid unnecessary phrases like ‚Äúan image of‚Äù? If the image contains readable text or numbers, are those included or summarized in the alt-text?). We are adapting the Alt Text Checklist from the A11y Project and WCAG techniques as our evaluation rubric. Each candidate description for an image will be reviewed by at least two team members with knowledge of accessibility standards. In cases where the image is a diagram or chart, we will check that the model followed instructions (providing a short summary alt-text and indicating a longer description would be needed). For images of documents, we check that any text was appropriately handled (transcribed or deferred to full text). The outcome of this step will be a rating or rank of the candidates for each image in terms of compliance. We expect that the model prompted with metadata and guidelines will produce mostly compliant alt-text, whereas some simpler models might yield overly generic or incomplete captions. An initial pilot test supports this: for example, without metadata, an open-source model captioned a photo as ‚ÄúOld photo of a street‚Äù which misses key specifics, but with our metadata-enhanced prompt GPT-4o produced ‚ÄúSchwarzwei√ü-Fotografie einer belebten Stra√üe in Basel, 1917, mit Demonstranten, die Banner in Frakturschrift halten.‚Äù (Black-and-white photograph of a busy Basel street in 1917, with protesters holding banners in Gothic script), which is far richer and ticks more of the accessibility boxes (it mentions the context, the presence of text on banners, etc.). This step addresses the first research question by testing whether models can be guided to meet alt-text requirements. We will quantify common compliance issues and note which model outputs most often require correction.\n(b) Historical Accuracy and Usefulness: The second layer of evaluation focuses on the content accuracy and value of the descriptions from a historian‚Äôs perspective. We will conduct a blind review where domain experts (trained historians) examine the AI-generated alt-text for a given image and compare it to the known metadata or facts about that image. Each expert will be presented with the four alt-text candidates for an image and will be asked to order them by relative factual correctness‚Äîthat is, ranking the descriptions from most to least accurate in terms of representing the image content. This ranking focuses on the relative quality among the alternatives rather than absolute judgments. For example, a model might mistakenly label a horse-drawn carriage in a 1890 photo as a ‚Äúcar‚Äù (anachronistic), or it might hallucinate a ‚Äúred stamp in the corner‚Äù of a document that does not exist. Such errors are critical to catch, as they could mislead researchers. On the other hand, we will also note cases where the AI description includes details that the original metadata or caption did not mention. In preliminary tests, we observed instances of this ‚ÄúAI insight‚Äù: e.g., a model noted ‚Äúein handgezeichneter roter Umriss auf dem Stadtplan‚Äù (a hand-drawn red outline on the map) which the human catalog description had not recorded. Upon checking the image, there was indeed a red pen marking on the map, presumably added by a later hand. Discovering these additional details could be beneficial, pointing scholars to visual evidence they might otherwise overlook. Our expert reviewers will differentiate between such legitimate additions and illegitimate hallucinations. We aim to categorize common error types (misidentifications, missed context, invented details) and measure the proportion of AI-generated alt-text that is acceptable with minimal or no editing versus those that need substantial correction. We anticipate, based on prior work and initial runs, that a majority of descriptions (over 90%) will be largely correct, while a significant minority will have issues requiring human intervention. The results of this step will inform how much post-editing effort is needed when deploying these models in practice.\n(c) Ethical Review: In parallel with the above, we will perform a qualitative analysis of the AI outputs to identify any ethical or bias concerns. This involves scanning the descriptions for inappropriate language or perspective. For instance, we will check if any descriptions contain terms or tones that are outdated or offensive (e.g., describing people in a demeaning way). We are particularly attentive to ableist language: while unlikely, we want to ensure the alt-text does not include phrases like ‚Äúsuffers from blindness‚Äù or similar, which are not acceptable in modern accessibility writing (Holmes 2020). If the model describes people, we examine whether it is making unwarranted assumptions about their identity (race, gender, etc.) or appearance. One concrete example: one model output described an older photograph of a man as ‚Äúein afrikanischer Mann‚Äù (‚Äúan African man‚Äù). The image indeed depicted a Black man, but in context his nationality or ethnicity was not documented and not necessarily relevant to the image‚Äôs purpose. Including such a descriptor could be seen as othering or speculative, so our policy is to avoid it unless it is directly pertinent (Hanley et al. 2021). In our review process, any such cases will be flagged and either removed or revised. We will also consider the implications of the model‚Äôs choices of detail: what the AI focuses on can reflect implicit bias (e.g., always mentioning a woman‚Äôs appearance but not a man‚Äôs). By compiling these observations, we will derive guidelines for curators on how to handle AI-generated descriptions. The ethical review is not a separate step per se, but integrated into the human-in-the-loop oversight‚Äîno AI-generated alt-text will be added to the public collection without passing this human review stage.",
    "crumbs": [
      "Home",
      "Research",
      "Abstract"
    ]
  },
  {
    "objectID": "ARCHIVE.html",
    "href": "ARCHIVE.html",
    "title": "TODO Archive",
    "section": "",
    "text": "Archived on 2025-10-19. Completed or superseded items moved from TODO.md to keep the active list focused.\n\n\n\nUpdate project details in package.json, _quarto.yml, and index.qmd\nAdd CITATION.cff with full metadata and ORCID IDs\nFormat all files (npm run format)\nFinalize README.md to reflect repository purpose\nVerify LaTeX compilation for abstract\nEnable GitHub security alerts and branch protection\nReplace all placeholders including [INSERT CONTACT METHOD]\nClean LaTeX build files and update .gitignore\n\n\n\n\n\nFinal version submitted to CHR 2025 (abstract/paper.pdf)\n\n\n\n\n\nInitial run\nTest run\n\n\n\n\n\nsurvey/form.html\nsurvey/index.qmd\n\n\n\n\n\ngenerate_alt_text.py ‚Äî final config, tested, pending final run\n\n\n\n\n\nInitial presentation draft"
  },
  {
    "objectID": "ARCHIVE.html#repository-setup-repo-completed",
    "href": "ARCHIVE.html#repository-setup-repo-completed",
    "title": "TODO Archive",
    "section": "",
    "text": "Update project details in package.json, _quarto.yml, and index.qmd\nAdd CITATION.cff with full metadata and ORCID IDs\nFormat all files (npm run format)\nFinalize README.md to reflect repository purpose\nVerify LaTeX compilation for abstract\nEnable GitHub security alerts and branch protection\nReplace all placeholders including [INSERT CONTACT METHOD]\nClean LaTeX build files and update .gitignore"
  },
  {
    "objectID": "ARCHIVE.html#abstract-abstract",
    "href": "ARCHIVE.html#abstract-abstract",
    "title": "TODO Archive",
    "section": "",
    "text": "Final version submitted to CHR 2025 (abstract/paper.pdf)"
  },
  {
    "objectID": "ARCHIVE.html#runs-runs",
    "href": "ARCHIVE.html#runs-runs",
    "title": "TODO Archive",
    "section": "",
    "text": "Initial run\nTest run"
  },
  {
    "objectID": "ARCHIVE.html#survey-survey",
    "href": "ARCHIVE.html#survey-survey",
    "title": "TODO Archive",
    "section": "",
    "text": "survey/form.html\nsurvey/index.qmd"
  },
  {
    "objectID": "ARCHIVE.html#source-code-src",
    "href": "ARCHIVE.html#source-code-src",
    "title": "TODO Archive",
    "section": "",
    "text": "generate_alt_text.py ‚Äî final config, tested, pending final run"
  },
  {
    "objectID": "ARCHIVE.html#presentation-presentations",
    "href": "ARCHIVE.html#presentation-presentations",
    "title": "TODO Archive",
    "section": "",
    "text": "Initial presentation draft"
  },
  {
    "objectID": "TODO.html",
    "href": "TODO.html",
    "title": "CHR 2025 Project TODO",
    "section": "",
    "text": "This file tracks setup, research, and publication tasks for the CHR 2025: Seeing History Unseen repository.\nCompleted items moved to ARCHIVE.md.\n\n\n\n\n\nConnect repository to Zenodo\nAdd Zenodo DOI badge and DOI to README.md\n(Optional) Add .zenodo.json metadata file\n(Optional) Generate and commit CHANGELOG.md\n(Optional) Enable and publish GitHub Pages with Quarto\nTest all build commands (npm, make)\nUpdate from latest base template\n\n\n\n\n\n\n\n\nIncorporate summary of model cards:\n\nMistral: Pixtral 12B\nGoogle: Gemini 2.5 Flash Lite\nMeta: Llama 4 Maverick\nOpenAI: GPT-4o-mini\n\nAdd links to open-weights models on Hugging Face for pixtral and llama-maverick\nCite and integrate:\n\nhttps://arxiv.org/html/2403.09193v2\n\n\nQuestion: Do LLM‚Äìvision fusion and prompts change what visual cues VLMs rely on? arXiv\nMethod: Measure texture-vs-shape bias on cue-conflict images for VQA and captioning; run mechanistic checks across vision encoder vs LLM fusion; test prompt ‚Äústeering‚Äù toward shape/texture and low/high spatial frequencies. arXiv\nCore findings:\nVLMs inherit encoder biases only partly and are more shape-biased than standard vision-only models, though still below human levels. arXiv\nMulti-modal fusion suppresses or amplifies cues; the LLM can shift which visual evidence is used. arXiv\nPrompting can steer cue use without retraining. Steering toward texture is often easier than toward shape; achievable shape bias ranged roughly 49‚Äì72% with little accuracy loss. arXiv\n\nhttps://arxiv.org/html/2507.11543v1\n\nThe paper is a structured literature review on generative AI in computer science education. It synthesizes 52 studies (2019‚Äì2024) around three lenses‚Äîaccuracy (hallucinations, bias, error propagation, metrics), authenticity (authorship, integrity, sociotechnical context), and assessment (AI-assisted grading, fairness, limits of current metrics). It argues for human-in-the-loop, hybrid assessment models, AI literacy, and bias-mitigation frameworks; it highlights gaps such as longitudinal effects and reliable accuracy measures beyond narrow correctness.\n\nhttps://arxiv.org/html/2409.03054v1\n\nThe paper designs and tests a pipeline that injects webpage context into GPT-4V image descriptions and shows, with a 12-participant BLV user study, that context-aware descriptions are preferred and rated higher on quality, imaginability, relevance, and plausibility than context-free baselines. It also reports a technical audit for hallucinations, subjectivity, and irrelevance, and surfaces risks around trust, privacy, and person identification. This directly supports your claim that VLMs can act as accessibility assistants and offers concrete design, evaluation, and governance patterns you can adapt to GLAM workflows.\n\nEnsure consistent vocabulary for disability justice framing Reference:\n\nNYC Disability-Inclusive Terminology Guide (2021)\nUN Geneva Disability-Inclusive Language Guidelines\nStanford Disability Language Guide\n\n\n\n\n\n\nDocument model selection and changes:\n\nInitial use of allenai/molmo-7b-d abandoned due to inconsistency\nSwitched from openai/gpt-4.1-nano to openai/gpt-4o-mini for better performance\nEvaluated models with comparable costs:\n\n\n\n\n\nModel Name & ID\nInput (\\(/1M) | Output (\\)/1M)\nContext (tokens)\n\n\n\n\n\nMistral: Pixtral 12Bmistralai/pixtral-12b\n0.10\n0.10\n32 768\n\n\nGoogle: Gemini 2.5 Flash Litegoogle/gemini-2.5-flash-lite\n0.10\n0.40\n1 048 576\n\n\nMeta: Llama 4 Maverickmeta-llama/llama-4-maverick\n0.15\n0.60\n1 048 576\n\n\nOpenAI: GPT-4o-miniopenai/gpt-4o-mini\n0.15\n0.60\n128 000\n\n\n\n\n\n\n\n\nTo be completed during paper writing\n\n\n\n\n\nFinal run with full dataset\n\n\n\n\n\nsurvey/questions.csv ‚Äî update after final run\nsurvey/results.csv ‚Äî update after expert survey\n\n\n\n\n\nanalysis scripts ‚Äî completed via ranking_tests.py, analyze_survey_time.py\n\n\n\n\n\nAlign slides with final paper\nAdd examples, visualizations, and demonstrations\nFinal review and rehearsal\n\n\n\n\n\nUpdate after final paper version and publication\nReview README workflow diagram whenever src scripts change\nDocument pipeline script order and artefacts in README (2025-10-27)\n\n\n\n\n\nTBD after final version\n\n\n\n\n\nComplete user study with blind/low-vision participants\nFinalize evaluation of AI models\nRelease benchmark dataset\nDraft practical guidelines for heritage institutions\n\n\n\n\n\nUpdate to support paper writing and model comparison (2025-10-27)\n\n\n\n\n\nKeep this document current\nUse [x], [ ], and status emojis consistently\nArchive historical TODOs after publication in ARCHIVE.md"
  },
  {
    "objectID": "TODO.html#repository-setup-repo",
    "href": "TODO.html#repository-setup-repo",
    "title": "CHR 2025 Project TODO",
    "section": "",
    "text": "Connect repository to Zenodo\nAdd Zenodo DOI badge and DOI to README.md\n(Optional) Add .zenodo.json metadata file\n(Optional) Generate and commit CHANGELOG.md\n(Optional) Enable and publish GitHub Pages with Quarto\nTest all build commands (npm, make)\nUpdate from latest base template"
  },
  {
    "objectID": "TODO.html#paper-paper",
    "href": "TODO.html#paper-paper",
    "title": "CHR 2025 Project TODO",
    "section": "",
    "text": "Incorporate summary of model cards:\n\nMistral: Pixtral 12B\nGoogle: Gemini 2.5 Flash Lite\nMeta: Llama 4 Maverick\nOpenAI: GPT-4o-mini\n\nAdd links to open-weights models on Hugging Face for pixtral and llama-maverick\nCite and integrate:\n\nhttps://arxiv.org/html/2403.09193v2\n\n\nQuestion: Do LLM‚Äìvision fusion and prompts change what visual cues VLMs rely on? arXiv\nMethod: Measure texture-vs-shape bias on cue-conflict images for VQA and captioning; run mechanistic checks across vision encoder vs LLM fusion; test prompt ‚Äústeering‚Äù toward shape/texture and low/high spatial frequencies. arXiv\nCore findings:\nVLMs inherit encoder biases only partly and are more shape-biased than standard vision-only models, though still below human levels. arXiv\nMulti-modal fusion suppresses or amplifies cues; the LLM can shift which visual evidence is used. arXiv\nPrompting can steer cue use without retraining. Steering toward texture is often easier than toward shape; achievable shape bias ranged roughly 49‚Äì72% with little accuracy loss. arXiv\n\nhttps://arxiv.org/html/2507.11543v1\n\nThe paper is a structured literature review on generative AI in computer science education. It synthesizes 52 studies (2019‚Äì2024) around three lenses‚Äîaccuracy (hallucinations, bias, error propagation, metrics), authenticity (authorship, integrity, sociotechnical context), and assessment (AI-assisted grading, fairness, limits of current metrics). It argues for human-in-the-loop, hybrid assessment models, AI literacy, and bias-mitigation frameworks; it highlights gaps such as longitudinal effects and reliable accuracy measures beyond narrow correctness.\n\nhttps://arxiv.org/html/2409.03054v1\n\nThe paper designs and tests a pipeline that injects webpage context into GPT-4V image descriptions and shows, with a 12-participant BLV user study, that context-aware descriptions are preferred and rated higher on quality, imaginability, relevance, and plausibility than context-free baselines. It also reports a technical audit for hallucinations, subjectivity, and irrelevance, and surfaces risks around trust, privacy, and person identification. This directly supports your claim that VLMs can act as accessibility assistants and offers concrete design, evaluation, and governance patterns you can adapt to GLAM workflows.\n\nEnsure consistent vocabulary for disability justice framing Reference:\n\nNYC Disability-Inclusive Terminology Guide (2021)\nUN Geneva Disability-Inclusive Language Guidelines\nStanford Disability Language Guide\n\n\n\n\n\n\nDocument model selection and changes:\n\nInitial use of allenai/molmo-7b-d abandoned due to inconsistency\nSwitched from openai/gpt-4.1-nano to openai/gpt-4o-mini for better performance\nEvaluated models with comparable costs:\n\n\n\n\n\nModel Name & ID\nInput (\\(/1M) | Output (\\)/1M)\nContext (tokens)\n\n\n\n\n\nMistral: Pixtral 12Bmistralai/pixtral-12b\n0.10\n0.10\n32 768\n\n\nGoogle: Gemini 2.5 Flash Litegoogle/gemini-2.5-flash-lite\n0.10\n0.40\n1 048 576\n\n\nMeta: Llama 4 Maverickmeta-llama/llama-4-maverick\n0.15\n0.60\n1 048 576\n\n\nOpenAI: GPT-4o-miniopenai/gpt-4o-mini\n0.15\n0.60\n128 000"
  },
  {
    "objectID": "TODO.html#analysis-analysis",
    "href": "TODO.html#analysis-analysis",
    "title": "CHR 2025 Project TODO",
    "section": "",
    "text": "To be completed during paper writing"
  },
  {
    "objectID": "TODO.html#runs-runs",
    "href": "TODO.html#runs-runs",
    "title": "CHR 2025 Project TODO",
    "section": "",
    "text": "Final run with full dataset"
  },
  {
    "objectID": "TODO.html#survey-survey",
    "href": "TODO.html#survey-survey",
    "title": "CHR 2025 Project TODO",
    "section": "",
    "text": "survey/questions.csv ‚Äî update after final run\nsurvey/results.csv ‚Äî update after expert survey"
  },
  {
    "objectID": "TODO.html#source-code-src",
    "href": "TODO.html#source-code-src",
    "title": "CHR 2025 Project TODO",
    "section": "",
    "text": "analysis scripts ‚Äî completed via ranking_tests.py, analyze_survey_time.py"
  },
  {
    "objectID": "TODO.html#presentation-presentations",
    "href": "TODO.html#presentation-presentations",
    "title": "CHR 2025 Project TODO",
    "section": "",
    "text": "Align slides with final paper\nAdd examples, visualizations, and demonstrations\nFinal review and rehearsal"
  },
  {
    "objectID": "TODO.html#documentation-documentation",
    "href": "TODO.html#documentation-documentation",
    "title": "CHR 2025 Project TODO",
    "section": "",
    "text": "Update after final paper version and publication\nReview README workflow diagram whenever src scripts change\nDocument pipeline script order and artefacts in README (2025-10-27)"
  },
  {
    "objectID": "TODO.html#testing-test",
    "href": "TODO.html#testing-test",
    "title": "CHR 2025 Project TODO",
    "section": "",
    "text": "TBD after final version"
  },
  {
    "objectID": "TODO.html#research-tasks-research",
    "href": "TODO.html#research-tasks-research",
    "title": "CHR 2025 Project TODO",
    "section": "",
    "text": "Complete user study with blind/low-vision participants\nFinalize evaluation of AI models\nRelease benchmark dataset\nDraft practical guidelines for heritage institutions"
  },
  {
    "objectID": "TODO.html#agents.md",
    "href": "TODO.html#agents.md",
    "title": "CHR 2025 Project TODO",
    "section": "",
    "text": "Update to support paper writing and model comparison (2025-10-27)"
  },
  {
    "objectID": "TODO.html#notes",
    "href": "TODO.html#notes",
    "title": "CHR 2025 Project TODO",
    "section": "",
    "text": "Keep this document current\nUse [x], [ ], and status emojis consistently\nArchive historical TODOs after publication in ARCHIVE.md"
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "Contributing",
    "section": "",
    "text": "When contributing to this repository, please first discuss the change you wish to make via issue, email, or any other method with the owners of this repository before making a change.\nPlease note we have a code of conduct, please follow it in all your interactions with the project.\n\n\n\nEnsure any install or build dependencies are removed before the end of the layer when doing a build.\nUpdate the README.md with details of changes to the interface, this includes new environment variables, exposed ports, useful file locations and container parameters.\nIncrease the version numbers in any examples files and the README.md to the new version that this Pull Request would represent. The versioning scheme we use is SemVer.\nYou may merge the Pull Request in once you have the sign-off of two other developers, or if you do not have permission to do that, you may request the second reviewer to merge it for you.",
    "crumbs": [
      "Home",
      "Project",
      "Contributing"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html#pull-request-process",
    "href": "CONTRIBUTING.html#pull-request-process",
    "title": "Contributing",
    "section": "",
    "text": "Ensure any install or build dependencies are removed before the end of the layer when doing a build.\nUpdate the README.md with details of changes to the interface, this includes new environment variables, exposed ports, useful file locations and container parameters.\nIncrease the version numbers in any examples files and the README.md to the new version that this Pull Request would represent. The versioning scheme we use is SemVer.\nYou may merge the Pull Request in once you have the sign-off of two other developers, or if you do not have permission to do that, you may request the second reviewer to merge it for you.",
    "crumbs": [
      "Home",
      "Project",
      "Contributing"
    ]
  },
  {
    "objectID": "LICENSE-CCBY.html",
    "href": "LICENSE-CCBY.html",
    "title": "CHR 2025 - Seeing History Unseen",
    "section": "",
    "text": "Attribution 4.0 International\n=======================================================================\nCreative Commons Corporation (‚ÄúCreative Commons‚Äù) is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an ‚Äúas-is‚Äù basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible.\nUsing Creative Commons Public Licenses\nCreative Commons public licenses provide a standard set of terms and conditions that creators and other rights holders may use to share original works of authorship and other material subject to copyright and certain other rights specified in the public license below. The following considerations are for informational purposes only, are not exhaustive, and do not form part of our licenses.\n Considerations for licensors: Our public licenses are\n intended for use by those authorized to give the public\n permission to use material in ways otherwise restricted by\n copyright and certain other rights. Our licenses are\n irrevocable. Licensors should read and understand the terms\n and conditions of the license they choose before applying it.\n Licensors should also secure all rights necessary before\n applying our licenses so that the public can reuse the\n material as expected. Licensors should clearly mark any\n material not subject to the license. This includes other CC-\n licensed material, or material used under an exception or\n limitation to copyright. More considerations for licensors:\nwiki.creativecommons.org/Considerations_for_licensors\n\n Considerations for the public: By using one of our public\n licenses, a licensor grants the public permission to use the\n licensed material under specified terms and conditions. If\n the licensor's permission is not necessary for any reason--for\n example, because of any applicable exception or limitation to\n copyright--then that use is not regulated by the license. Our\n licenses grant only permissions under copyright and certain\n other rights that a licensor has authority to grant. Use of\n the licensed material may still be restricted for other\n reasons, including because others have copyright or other\n rights in the material. A licensor may make special requests,\n such as asking that all changes be marked or described.\n Although not required by our licenses, you are encouraged to\n respect those requests where reasonable. More_considerations\n for the public:\nwiki.creativecommons.org/Considerations_for_licensees\n=======================================================================\nCreative Commons Attribution 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution 4.0 International Public License (‚ÄúPublic License‚Äù). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\nSection 1 ‚Äì Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter‚Äôs License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\nSection 2 ‚Äì Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\n\nreproduce and Share the Licensed Material, in whole or in part; and\nproduce, reproduce, and Share Adapted Material.\n\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)\n\nnever produces Adapted Material.\n\nDownstream recipients.\n\nOffer from the Licensor ‚Äì Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nNo downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\n\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\nSection 3 ‚Äì License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\n\nretain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nindicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nindicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\n\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\nIf You Share Adapted Material You produce, the Adapter‚Äôs License You apply must not prevent recipients of the Adapted Material from complying with this Public License.\n\n\nSection 4 ‚Äì Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material; and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\nSection 5 ‚Äì Disclaimer of Warranties and Limitation of Liability.\n\nUNLESS OTHERWISE SEPARATELY UNDERTAKEN BY THE LICENSOR, TO THE EXTENT POSSIBLE, THE LICENSOR OFFERS THE LICENSED MATERIAL AS-IS AND AS-AVAILABLE, AND MAKES NO REPRESENTATIONS OR WARRANTIES OF ANY KIND CONCERNING THE LICENSED MATERIAL, WHETHER EXPRESS, IMPLIED, STATUTORY, OR OTHER. THIS INCLUDES, WITHOUT LIMITATION, WARRANTIES OF TITLE, MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT, ABSENCE OF LATENT OR OTHER DEFECTS, ACCURACY, OR THE PRESENCE OR ABSENCE OF ERRORS, WHETHER OR NOT KNOWN OR DISCOVERABLE. WHERE DISCLAIMERS OF WARRANTIES ARE NOT ALLOWED IN FULL OR IN PART, THIS DISCLAIMER MAY NOT APPLY TO YOU.\nTO THE EXTENT POSSIBLE, IN NO EVENT WILL THE LICENSOR BE LIABLE TO YOU ON ANY LEGAL THEORY (INCLUDING, WITHOUT LIMITATION, NEGLIGENCE) OR OTHERWISE FOR ANY DIRECT, SPECIAL, INDIRECT, INCIDENTAL, CONSEQUENTIAL, PUNITIVE, EXEMPLARY, OR OTHER LOSSES, COSTS, EXPENSES, OR DAMAGES ARISING OUT OF THIS PUBLIC LICENSE OR USE OF THE LICENSED MATERIAL, EVEN IF THE LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSSES, COSTS, EXPENSES, OR DAMAGES. WHERE A LIMITATION OF LIABILITY IS NOT ALLOWED IN FULL OR IN PART, THIS LIMITATION MAY NOT APPLY TO YOU.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\nSection 6 ‚Äì Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\nSection 7 ‚Äì Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\nSection 8 ‚Äì Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n=======================================================================\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the ‚ÄúLicensor.‚Äù The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark ‚ÄúCreative Commons‚Äù or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org.\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Licenses",
      "Data (CC BY 4.0)"
    ]
  },
  {
    "objectID": "LICENSE-AGPL.html",
    "href": "LICENSE-AGPL.html",
    "title": "CHR 2025 - Seeing History Unseen",
    "section": "",
    "text": "GNU AFFERO GENERAL PUBLIC LICENSE\n                   Version 3, 19 November 2007\nCopyright (C) 2007 Free Software Foundation, Inc.¬†https://fsf.org/ Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\n                        Preamble\nThe GNU Affero General Public License is a free, copyleft license for software and other kinds of works, specifically designed to ensure cooperation with the community in the case of network server software.\nThe licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, our General Public Licenses are intended to guarantee your freedom to share and change all versions of a program‚Äìto make sure it remains free software for all its users.\nWhen we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.\nDevelopers that use our General Public Licenses protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License which gives you legal permission to copy, distribute and/or modify the software.\nA secondary benefit of defending all users‚Äô freedom is that improvements made in alternate versions of the program, if they receive widespread use, become available for other developers to incorporate. Many developers of free software are heartened and encouraged by the resulting cooperation. However, in the case of software used on network servers, this result may fail to come about. The GNU General Public License permits making a modified version and letting the public access it on a server without ever releasing its source code to the public.\nThe GNU Affero General Public License is designed specifically to ensure that, in such cases, the modified source code becomes available to the community. It requires the operator of a network server to provide the source code of the modified version running there to the users of that server. Therefore, public use of a modified version, on a publicly accessible server, gives the public access to the source code of the modified version.\nAn older license, called the Affero General Public License and published by Affero, was designed to accomplish similar goals. This is a different license, not a version of the Affero GPL, but Affero has released a new version of the Affero GPL which permits relicensing under this license.\nThe precise terms and conditions for copying, distribution and modification follow.\n                   TERMS AND CONDITIONS\n\nDefinitions.\n\n‚ÄúThis License‚Äù refers to version 3 of the GNU Affero General Public License.\n‚ÄúCopyright‚Äù also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.\n‚ÄúThe Program‚Äù refers to any copyrightable work licensed under this License. Each licensee is addressed as ‚Äúyou‚Äù. ‚ÄúLicensees‚Äù and ‚Äúrecipients‚Äù may be individuals or organizations.\nTo ‚Äúmodify‚Äù a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a ‚Äúmodified version‚Äù of the earlier work or a work ‚Äúbased on‚Äù the earlier work.\nA ‚Äúcovered work‚Äù means either the unmodified Program or a work based on the Program.\nTo ‚Äúpropagate‚Äù a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.\nTo ‚Äúconvey‚Äù a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.\nAn interactive user interface displays ‚ÄúAppropriate Legal Notices‚Äù to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.\n\nSource Code.\n\nThe ‚Äúsource code‚Äù for a work means the preferred form of the work for making modifications to it. ‚ÄúObject code‚Äù means any non-source form of a work.\nA ‚ÄúStandard Interface‚Äù means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.\nThe ‚ÄúSystem Libraries‚Äù of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A ‚ÄúMajor Component‚Äù, in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.\nThe ‚ÄúCorresponding Source‚Äù for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work‚Äôs System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.\nThe Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.\nThe Corresponding Source for a work in source code form is that same work.\n\nBasic Permissions.\n\nAll rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.\nYou may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.\nConveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary.\n\nProtecting Users‚Äô Legal Rights From Anti-Circumvention Law.\n\nNo covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.\nWhen you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work‚Äôs users, your or third parties‚Äô legal rights to forbid circumvention of technological measures.\n\nConveying Verbatim Copies.\n\nYou may convey verbatim copies of the Program‚Äôs source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.\nYou may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.\n\nConveying Modified Source Versions.\n\nYou may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:\na) The work must carry prominent notices stating that you modified\nit, and giving a relevant date.\n\nb) The work must carry prominent notices stating that it is\nreleased under this License and any conditions added under section\n7.  This requirement modifies the requirement in section 4 to\n\"keep intact all notices\".\n\nc) You must license the entire work, as a whole, under this\nLicense to anyone who comes into possession of a copy.  This\nLicense will therefore apply, along with any applicable section 7\nadditional terms, to the whole of the work, and all its parts,\nregardless of how they are packaged.  This License gives no\npermission to license the work in any other way, but it does not\ninvalidate such permission if you have separately received it.\n\nd) If the work has interactive user interfaces, each must display\nAppropriate Legal Notices; however, if the Program has interactive\ninterfaces that do not display Appropriate Legal Notices, your\nwork need not make them do so.\nA compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an ‚Äúaggregate‚Äù if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation‚Äôs users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.\n\nConveying Non-Source Forms.\n\nYou may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:\na) Convey the object code in, or embodied in, a physical product\n(including a physical distribution medium), accompanied by the\nCorresponding Source fixed on a durable physical medium\ncustomarily used for software interchange.\n\nb) Convey the object code in, or embodied in, a physical product\n(including a physical distribution medium), accompanied by a\nwritten offer, valid for at least three years and valid for as\nlong as you offer spare parts or customer support for that product\nmodel, to give anyone who possesses the object code either (1) a\ncopy of the Corresponding Source for all the software in the\nproduct that is covered by this License, on a durable physical\nmedium customarily used for software interchange, for a price no\nmore than your reasonable cost of physically performing this\nconveying of source, or (2) access to copy the\nCorresponding Source from a network server at no charge.\n\nc) Convey individual copies of the object code with a copy of the\nwritten offer to provide the Corresponding Source.  This\nalternative is allowed only occasionally and noncommercially, and\nonly if you received the object code with such an offer, in accord\nwith subsection 6b.\n\nd) Convey the object code by offering access from a designated\nplace (gratis or for a charge), and offer equivalent access to the\nCorresponding Source in the same way through the same place at no\nfurther charge.  You need not require recipients to copy the\nCorresponding Source along with the object code.  If the place to\ncopy the object code is a network server, the Corresponding Source\nmay be on a different server (operated by you or a third party)\nthat supports equivalent copying facilities, provided you maintain\nclear directions next to the object code saying where to find the\nCorresponding Source.  Regardless of what server hosts the\nCorresponding Source, you remain obligated to ensure that it is\navailable for as long as needed to satisfy these requirements.\n\ne) Convey the object code using peer-to-peer transmission, provided\nyou inform other peers where the object code and Corresponding\nSource of the work are being offered to the general public at no\ncharge under subsection 6d.\nA separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.\nA ‚ÄúUser Product‚Äù is either (1) a ‚Äúconsumer product‚Äù, which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, ‚Äúnormally used‚Äù refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.\n‚ÄúInstallation Information‚Äù for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.\nIf you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).\nThe requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.\nCorresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.\n\nAdditional Terms.\n\n‚ÄúAdditional permissions‚Äù are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.\nWhen you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.\nNotwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:\na) Disclaiming warranty or limiting liability differently from the\nterms of sections 15 and 16 of this License; or\n\nb) Requiring preservation of specified reasonable legal notices or\nauthor attributions in that material or in the Appropriate Legal\nNotices displayed by works containing it; or\n\nc) Prohibiting misrepresentation of the origin of that material, or\nrequiring that modified versions of such material be marked in\nreasonable ways as different from the original version; or\n\nd) Limiting the use for publicity purposes of names of licensors or\nauthors of the material; or\n\ne) Declining to grant rights under trademark law for use of some\ntrade names, trademarks, or service marks; or\n\nf) Requiring indemnification of licensors and authors of that\nmaterial by anyone who conveys the material (or modified versions of\nit) with contractual assumptions of liability to the recipient, for\nany liability that these contractual assumptions directly impose on\nthose licensors and authors.\nAll other non-permissive additional terms are considered ‚Äúfurther restrictions‚Äù within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.\nIf you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.\nAdditional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.\n\nTermination.\n\nYou may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).\nHowever, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.\nMoreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.\nTermination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.\n\nAcceptance Not Required for Having Copies.\n\nYou are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.\n\nAutomatic Licensing of Downstream Recipients.\n\nEach time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License.\nAn ‚Äúentity transaction‚Äù is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party‚Äôs predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.\nYou may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.\n\nPatents.\n\nA ‚Äúcontributor‚Äù is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor‚Äôs ‚Äúcontributor version‚Äù.\nA contributor‚Äôs ‚Äúessential patent claims‚Äù are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, ‚Äúcontrol‚Äù includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.\nEach contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor‚Äôs essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.\nIn the following three paragraphs, a ‚Äúpatent license‚Äù is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To ‚Äúgrant‚Äù such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.\nIf you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. ‚ÄúKnowingly relying‚Äù means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient‚Äôs use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.\nIf, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.\nA patent license is ‚Äúdiscriminatory‚Äù if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.\nNothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.\n\nNo Surrender of Others‚Äô Freedom.\n\nIf conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.\n\nRemote Network Interaction; Use with the GNU General Public License.\n\nNotwithstanding any other provision of this License, if you modify the Program, your modified version must prominently offer all users interacting with it remotely through a computer network (if your version supports such interaction) an opportunity to receive the Corresponding Source of your version by providing access to the Corresponding Source from a network server at no charge, through some standard or customary means of facilitating copying of software. This Corresponding Source shall include the Corresponding Source for any work covered by version 3 of the GNU General Public License that is incorporated pursuant to the following paragraph.\nNotwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the work with which it is combined will remain governed by version 3 of the GNU General Public License.\n\nRevised Versions of this License.\n\nThe Free Software Foundation may publish revised and/or new versions of the GNU Affero General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.\nEach version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU Affero General Public License ‚Äúor any later version‚Äù applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU Affero General Public License, you may choose any version ever published by the Free Software Foundation.\nIf the Program specifies that a proxy can decide which future versions of the GNU Affero General Public License can be used, that proxy‚Äôs public statement of acceptance of a version permanently authorizes you to choose that version for the Program.\nLater license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.\n\nDisclaimer of Warranty.\n\nTHERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM ‚ÄúAS IS‚Äù WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.\n\nLimitation of Liability.\n\nIN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\nInterpretation of Sections 15 and 16.\n\nIf the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.\n                 END OF TERMS AND CONDITIONS\n\n        How to Apply These Terms to Your New Programs\nIf you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.\nTo do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the ‚Äúcopyright‚Äù line and a pointer to where the full notice is found.\n&lt;one line to give the program's name and a brief idea of what it does.&gt;\nCopyright (C) &lt;year&gt;  &lt;name of author&gt;\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Affero General Public License as published\nby the Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU Affero General Public License for more details.\n\nYou should have received a copy of the GNU Affero General Public License\nalong with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\nAlso add information on how to contact you by electronic and paper mail.\nIf your software can interact with users remotely through a computer network, you should also make sure that it provides a way for users to get its source. For example, if your program is a web application, its interface could display a ‚ÄúSource‚Äù link that leads users to an archive of the code. There are many ways you could offer source, and different solutions will be better for different programs; see section 13 for the specific requirements.\nYou should also get your employer (if you work as a programmer) or school, if any, to sign a ‚Äúcopyright disclaimer‚Äù for the program, if necessary. For more information on this, and how to apply and follow the GNU AGPL, see https://www.gnu.org/licenses/.\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Licenses",
      "Code (AGPL 3.0)"
    ]
  }
]