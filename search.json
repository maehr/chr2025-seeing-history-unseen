[
  {
    "objectID": "SECURITY.html",
    "href": "SECURITY.html",
    "title": "Security Policy",
    "section": "",
    "text": "To report a security issue, please email moritz.maehr@gmail.com with a description of the issue, the steps you took to create the issue, affected versions, and, if known, mitigations for the issue. This project follows a 90 day disclosure timeline.",
    "crumbs": [
      "Home",
      "Project",
      "Security"
    ]
  },
  {
    "objectID": "SECURITY.html#reporting-a-vulnerability",
    "href": "SECURITY.html#reporting-a-vulnerability",
    "title": "Security Policy",
    "section": "",
    "text": "To report a security issue, please email moritz.maehr@gmail.com with a description of the issue, the steps you took to create the issue, affected versions, and, if known, mitigations for the issue. This project follows a 90 day disclosure timeline.",
    "crumbs": [
      "Home",
      "Project",
      "Security"
    ]
  },
  {
    "objectID": "LICENSE-AGPL.html",
    "href": "LICENSE-AGPL.html",
    "title": "CHR 2025 - Seeing History Unseen",
    "section": "",
    "text": "GNU AFFERO GENERAL PUBLIC LICENSE\n                   Version 3, 19 November 2007\nCopyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/ Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\n                        Preamble\nThe GNU Affero General Public License is a free, copyleft license for software and other kinds of works, specifically designed to ensure cooperation with the community in the case of network server software.\nThe licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, our General Public Licenses are intended to guarantee your freedom to share and change all versions of a program–to make sure it remains free software for all its users.\nWhen we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.\nDevelopers that use our General Public Licenses protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License which gives you legal permission to copy, distribute and/or modify the software.\nA secondary benefit of defending all users’ freedom is that improvements made in alternate versions of the program, if they receive widespread use, become available for other developers to incorporate. Many developers of free software are heartened and encouraged by the resulting cooperation. However, in the case of software used on network servers, this result may fail to come about. The GNU General Public License permits making a modified version and letting the public access it on a server without ever releasing its source code to the public.\nThe GNU Affero General Public License is designed specifically to ensure that, in such cases, the modified source code becomes available to the community. It requires the operator of a network server to provide the source code of the modified version running there to the users of that server. Therefore, public use of a modified version, on a publicly accessible server, gives the public access to the source code of the modified version.\nAn older license, called the Affero General Public License and published by Affero, was designed to accomplish similar goals. This is a different license, not a version of the Affero GPL, but Affero has released a new version of the Affero GPL which permits relicensing under this license.\nThe precise terms and conditions for copying, distribution and modification follow.\n                   TERMS AND CONDITIONS\n\nDefinitions.\n\n“This License” refers to version 3 of the GNU Affero General Public License.\n“Copyright” also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.\n“The Program” refers to any copyrightable work licensed under this License. Each licensee is addressed as “you”. “Licensees” and “recipients” may be individuals or organizations.\nTo “modify” a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a “modified version” of the earlier work or a work “based on” the earlier work.\nA “covered work” means either the unmodified Program or a work based on the Program.\nTo “propagate” a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.\nTo “convey” a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.\nAn interactive user interface displays “Appropriate Legal Notices” to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.\n\nSource Code.\n\nThe “source code” for a work means the preferred form of the work for making modifications to it. “Object code” means any non-source form of a work.\nA “Standard Interface” means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.\nThe “System Libraries” of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A “Major Component”, in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.\nThe “Corresponding Source” for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work’s System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.\nThe Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.\nThe Corresponding Source for a work in source code form is that same work.\n\nBasic Permissions.\n\nAll rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.\nYou may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.\nConveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary.\n\nProtecting Users’ Legal Rights From Anti-Circumvention Law.\n\nNo covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.\nWhen you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work’s users, your or third parties’ legal rights to forbid circumvention of technological measures.\n\nConveying Verbatim Copies.\n\nYou may convey verbatim copies of the Program’s source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.\nYou may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.\n\nConveying Modified Source Versions.\n\nYou may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:\na) The work must carry prominent notices stating that you modified\nit, and giving a relevant date.\n\nb) The work must carry prominent notices stating that it is\nreleased under this License and any conditions added under section\n7.  This requirement modifies the requirement in section 4 to\n\"keep intact all notices\".\n\nc) You must license the entire work, as a whole, under this\nLicense to anyone who comes into possession of a copy.  This\nLicense will therefore apply, along with any applicable section 7\nadditional terms, to the whole of the work, and all its parts,\nregardless of how they are packaged.  This License gives no\npermission to license the work in any other way, but it does not\ninvalidate such permission if you have separately received it.\n\nd) If the work has interactive user interfaces, each must display\nAppropriate Legal Notices; however, if the Program has interactive\ninterfaces that do not display Appropriate Legal Notices, your\nwork need not make them do so.\nA compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an “aggregate” if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation’s users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.\n\nConveying Non-Source Forms.\n\nYou may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:\na) Convey the object code in, or embodied in, a physical product\n(including a physical distribution medium), accompanied by the\nCorresponding Source fixed on a durable physical medium\ncustomarily used for software interchange.\n\nb) Convey the object code in, or embodied in, a physical product\n(including a physical distribution medium), accompanied by a\nwritten offer, valid for at least three years and valid for as\nlong as you offer spare parts or customer support for that product\nmodel, to give anyone who possesses the object code either (1) a\ncopy of the Corresponding Source for all the software in the\nproduct that is covered by this License, on a durable physical\nmedium customarily used for software interchange, for a price no\nmore than your reasonable cost of physically performing this\nconveying of source, or (2) access to copy the\nCorresponding Source from a network server at no charge.\n\nc) Convey individual copies of the object code with a copy of the\nwritten offer to provide the Corresponding Source.  This\nalternative is allowed only occasionally and noncommercially, and\nonly if you received the object code with such an offer, in accord\nwith subsection 6b.\n\nd) Convey the object code by offering access from a designated\nplace (gratis or for a charge), and offer equivalent access to the\nCorresponding Source in the same way through the same place at no\nfurther charge.  You need not require recipients to copy the\nCorresponding Source along with the object code.  If the place to\ncopy the object code is a network server, the Corresponding Source\nmay be on a different server (operated by you or a third party)\nthat supports equivalent copying facilities, provided you maintain\nclear directions next to the object code saying where to find the\nCorresponding Source.  Regardless of what server hosts the\nCorresponding Source, you remain obligated to ensure that it is\navailable for as long as needed to satisfy these requirements.\n\ne) Convey the object code using peer-to-peer transmission, provided\nyou inform other peers where the object code and Corresponding\nSource of the work are being offered to the general public at no\ncharge under subsection 6d.\nA separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.\nA “User Product” is either (1) a “consumer product”, which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, “normally used” refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.\n“Installation Information” for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.\nIf you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).\nThe requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.\nCorresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.\n\nAdditional Terms.\n\n“Additional permissions” are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.\nWhen you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.\nNotwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:\na) Disclaiming warranty or limiting liability differently from the\nterms of sections 15 and 16 of this License; or\n\nb) Requiring preservation of specified reasonable legal notices or\nauthor attributions in that material or in the Appropriate Legal\nNotices displayed by works containing it; or\n\nc) Prohibiting misrepresentation of the origin of that material, or\nrequiring that modified versions of such material be marked in\nreasonable ways as different from the original version; or\n\nd) Limiting the use for publicity purposes of names of licensors or\nauthors of the material; or\n\ne) Declining to grant rights under trademark law for use of some\ntrade names, trademarks, or service marks; or\n\nf) Requiring indemnification of licensors and authors of that\nmaterial by anyone who conveys the material (or modified versions of\nit) with contractual assumptions of liability to the recipient, for\nany liability that these contractual assumptions directly impose on\nthose licensors and authors.\nAll other non-permissive additional terms are considered “further restrictions” within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.\nIf you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.\nAdditional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.\n\nTermination.\n\nYou may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).\nHowever, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.\nMoreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.\nTermination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.\n\nAcceptance Not Required for Having Copies.\n\nYou are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.\n\nAutomatic Licensing of Downstream Recipients.\n\nEach time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License.\nAn “entity transaction” is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party’s predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.\nYou may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.\n\nPatents.\n\nA “contributor” is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor’s “contributor version”.\nA contributor’s “essential patent claims” are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, “control” includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.\nEach contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor’s essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.\nIn the following three paragraphs, a “patent license” is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To “grant” such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.\nIf you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. “Knowingly relying” means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient’s use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.\nIf, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.\nA patent license is “discriminatory” if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.\nNothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.\n\nNo Surrender of Others’ Freedom.\n\nIf conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.\n\nRemote Network Interaction; Use with the GNU General Public License.\n\nNotwithstanding any other provision of this License, if you modify the Program, your modified version must prominently offer all users interacting with it remotely through a computer network (if your version supports such interaction) an opportunity to receive the Corresponding Source of your version by providing access to the Corresponding Source from a network server at no charge, through some standard or customary means of facilitating copying of software. This Corresponding Source shall include the Corresponding Source for any work covered by version 3 of the GNU General Public License that is incorporated pursuant to the following paragraph.\nNotwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the work with which it is combined will remain governed by version 3 of the GNU General Public License.\n\nRevised Versions of this License.\n\nThe Free Software Foundation may publish revised and/or new versions of the GNU Affero General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.\nEach version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU Affero General Public License “or any later version” applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU Affero General Public License, you may choose any version ever published by the Free Software Foundation.\nIf the Program specifies that a proxy can decide which future versions of the GNU Affero General Public License can be used, that proxy’s public statement of acceptance of a version permanently authorizes you to choose that version for the Program.\nLater license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.\n\nDisclaimer of Warranty.\n\nTHERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM “AS IS” WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.\n\nLimitation of Liability.\n\nIN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\nInterpretation of Sections 15 and 16.\n\nIf the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.\n                 END OF TERMS AND CONDITIONS\n\n        How to Apply These Terms to Your New Programs\nIf you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.\nTo do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the “copyright” line and a pointer to where the full notice is found.\n&lt;one line to give the program's name and a brief idea of what it does.&gt;\nCopyright (C) &lt;year&gt;  &lt;name of author&gt;\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Affero General Public License as published\nby the Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU Affero General Public License for more details.\n\nYou should have received a copy of the GNU Affero General Public License\nalong with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\nAlso add information on how to contact you by electronic and paper mail.\nIf your software can interact with users remotely through a computer network, you should also make sure that it provides a way for users to get its source. For example, if your program is a web application, its interface could display a “Source” link that leads users to an archive of the code. There are many ways you could offer source, and different solutions will be better for different programs; see section 13 for the specific requirements.\nYou should also get your employer (if you work as a programmer) or school, if any, to sign a “copyright disclaimer” for the program, if necessary. For more information on this, and how to apply and follow the GNU AGPL, see https://www.gnu.org/licenses/.\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Licenses",
      "Code (AGPL 3.0)"
    ]
  },
  {
    "objectID": "LICENSE-CCBY.html",
    "href": "LICENSE-CCBY.html",
    "title": "CHR 2025 - Seeing History Unseen",
    "section": "",
    "text": "Attribution 4.0 International\n=======================================================================\nCreative Commons Corporation (“Creative Commons”) is not a law firm and does not provide legal services or legal advice. Distribution of Creative Commons public licenses does not create a lawyer-client or other relationship. Creative Commons makes its licenses and related information available on an “as-is” basis. Creative Commons gives no warranties regarding its licenses, any material licensed under their terms and conditions, or any related information. Creative Commons disclaims all liability for damages resulting from their use to the fullest extent possible.\nUsing Creative Commons Public Licenses\nCreative Commons public licenses provide a standard set of terms and conditions that creators and other rights holders may use to share original works of authorship and other material subject to copyright and certain other rights specified in the public license below. The following considerations are for informational purposes only, are not exhaustive, and do not form part of our licenses.\n Considerations for licensors: Our public licenses are\n intended for use by those authorized to give the public\n permission to use material in ways otherwise restricted by\n copyright and certain other rights. Our licenses are\n irrevocable. Licensors should read and understand the terms\n and conditions of the license they choose before applying it.\n Licensors should also secure all rights necessary before\n applying our licenses so that the public can reuse the\n material as expected. Licensors should clearly mark any\n material not subject to the license. This includes other CC-\n licensed material, or material used under an exception or\n limitation to copyright. More considerations for licensors:\nwiki.creativecommons.org/Considerations_for_licensors\n\n Considerations for the public: By using one of our public\n licenses, a licensor grants the public permission to use the\n licensed material under specified terms and conditions. If\n the licensor's permission is not necessary for any reason--for\n example, because of any applicable exception or limitation to\n copyright--then that use is not regulated by the license. Our\n licenses grant only permissions under copyright and certain\n other rights that a licensor has authority to grant. Use of\n the licensed material may still be restricted for other\n reasons, including because others have copyright or other\n rights in the material. A licensor may make special requests,\n such as asking that all changes be marked or described.\n Although not required by our licenses, you are encouraged to\n respect those requests where reasonable. More_considerations\n for the public:\nwiki.creativecommons.org/Considerations_for_licensees\n=======================================================================\nCreative Commons Attribution 4.0 International Public License\nBy exercising the Licensed Rights (defined below), You accept and agree to be bound by the terms and conditions of this Creative Commons Attribution 4.0 International Public License (“Public License”). To the extent this Public License may be interpreted as a contract, You are granted the Licensed Rights in consideration of Your acceptance of these terms and conditions, and the Licensor grants You such rights in consideration of benefits the Licensor receives from making the Licensed Material available under these terms and conditions.\nSection 1 – Definitions.\n\nAdapted Material means material subject to Copyright and Similar Rights that is derived from or based upon the Licensed Material and in which the Licensed Material is translated, altered, arranged, transformed, or otherwise modified in a manner requiring permission under the Copyright and Similar Rights held by the Licensor. For purposes of this Public License, where the Licensed Material is a musical work, performance, or sound recording, Adapted Material is always produced where the Licensed Material is synched in timed relation with a moving image.\nAdapter’s License means the license You apply to Your Copyright and Similar Rights in Your contributions to Adapted Material in accordance with the terms and conditions of this Public License.\nCopyright and Similar Rights means copyright and/or similar rights closely related to copyright including, without limitation, performance, broadcast, sound recording, and Sui Generis Database Rights, without regard to how the rights are labeled or categorized. For purposes of this Public License, the rights specified in Section 2(b)(1)-(2) are not Copyright and Similar Rights.\nEffective Technological Measures means those measures that, in the absence of proper authority, may not be circumvented under laws fulfilling obligations under Article 11 of the WIPO Copyright Treaty adopted on December 20, 1996, and/or similar international agreements.\nExceptions and Limitations means fair use, fair dealing, and/or any other exception or limitation to Copyright and Similar Rights that applies to Your use of the Licensed Material.\nLicensed Material means the artistic or literary work, database, or other material to which the Licensor applied this Public License.\nLicensed Rights means the rights granted to You subject to the terms and conditions of this Public License, which are limited to all Copyright and Similar Rights that apply to Your use of the Licensed Material and that the Licensor has authority to license.\nLicensor means the individual(s) or entity(ies) granting rights under this Public License.\nShare means to provide material to the public by any means or process that requires permission under the Licensed Rights, such as reproduction, public display, public performance, distribution, dissemination, communication, or importation, and to make material available to the public including in ways that members of the public may access the material from a place and at a time individually chosen by them.\nSui Generis Database Rights means rights other than copyright resulting from Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, as amended and/or succeeded, as well as other essentially equivalent rights anywhere in the world.\nYou means the individual or entity exercising the Licensed Rights under this Public License. Your has a corresponding meaning.\n\nSection 2 – Scope.\n\nLicense grant.\n\nSubject to the terms and conditions of this Public License, the Licensor hereby grants You a worldwide, royalty-free, non-sublicensable, non-exclusive, irrevocable license to exercise the Licensed Rights in the Licensed Material to:\n\nreproduce and Share the Licensed Material, in whole or in part; and\nproduce, reproduce, and Share Adapted Material.\n\nExceptions and Limitations. For the avoidance of doubt, where Exceptions and Limitations apply to Your use, this Public License does not apply, and You do not need to comply with its terms and conditions.\nTerm. The term of this Public License is specified in Section 6(a).\nMedia and formats; technical modifications allowed. The Licensor authorizes You to exercise the Licensed Rights in all media and formats whether now known or hereafter created, and to make technical modifications necessary to do so. The Licensor waives and/or agrees not to assert any right or authority to forbid You from making technical modifications necessary to exercise the Licensed Rights, including technical modifications necessary to circumvent Effective Technological Measures. For purposes of this Public License, simply making modifications authorized by this Section 2(a)\n\nnever produces Adapted Material.\n\nDownstream recipients.\n\nOffer from the Licensor – Licensed Material. Every recipient of the Licensed Material automatically receives an offer from the Licensor to exercise the Licensed Rights under the terms and conditions of this Public License.\nNo downstream restrictions. You may not offer or impose any additional or different terms or conditions on, or apply any Effective Technological Measures to, the Licensed Material if doing so restricts exercise of the Licensed Rights by any recipient of the Licensed Material.\n\nNo endorsement. Nothing in this Public License constitutes or may be construed as permission to assert or imply that You are, or that Your use of the Licensed Material is, connected with, or sponsored, endorsed, or granted official status by, the Licensor or others designated to receive attribution as provided in Section 3(a)(1)(A)(i).\n\nOther rights.\n\nMoral rights, such as the right of integrity, are not licensed under this Public License, nor are publicity, privacy, and/or other similar personality rights; however, to the extent possible, the Licensor waives and/or agrees not to assert any such rights held by the Licensor to the limited extent necessary to allow You to exercise the Licensed Rights, but not otherwise.\nPatent and trademark rights are not licensed under this Public License.\nTo the extent possible, the Licensor waives any right to collect royalties from You for the exercise of the Licensed Rights, whether directly or through a collecting society under any voluntary or waivable statutory or compulsory licensing scheme. In all other cases the Licensor expressly reserves any right to collect such royalties.\n\n\nSection 3 – License Conditions.\nYour exercise of the Licensed Rights is expressly made subject to the following conditions.\n\nAttribution.\n\nIf You Share the Licensed Material (including in modified form), You must:\n\nretain the following if it is supplied by the Licensor with the Licensed Material:\n\nidentification of the creator(s) of the Licensed Material and any others designated to receive attribution, in any reasonable manner requested by the Licensor (including by pseudonym if designated);\na copyright notice;\na notice that refers to this Public License;\na notice that refers to the disclaimer of warranties;\na URI or hyperlink to the Licensed Material to the extent reasonably practicable;\n\nindicate if You modified the Licensed Material and retain an indication of any previous modifications; and\nindicate the Licensed Material is licensed under this Public License, and include the text of, or the URI or hyperlink to, this Public License.\n\nYou may satisfy the conditions in Section 3(a)(1) in any reasonable manner based on the medium, means, and context in which You Share the Licensed Material. For example, it may be reasonable to satisfy the conditions by providing a URI or hyperlink to a resource that includes the required information.\nIf requested by the Licensor, You must remove any of the information required by Section 3(a)(1)(A) to the extent reasonably practicable.\nIf You Share Adapted Material You produce, the Adapter’s License You apply must not prevent recipients of the Adapted Material from complying with this Public License.\n\n\nSection 4 – Sui Generis Database Rights.\nWhere the Licensed Rights include Sui Generis Database Rights that apply to Your use of the Licensed Material:\n\nfor the avoidance of doubt, Section 2(a)(1) grants You the right to extract, reuse, reproduce, and Share all or a substantial portion of the contents of the database;\nif You include all or a substantial portion of the database contents in a database in which You have Sui Generis Database Rights, then the database in which You have Sui Generis Database Rights (but not its individual contents) is Adapted Material; and\nYou must comply with the conditions in Section 3(a) if You Share all or a substantial portion of the contents of the database.\n\nFor the avoidance of doubt, this Section 4 supplements and does not replace Your obligations under this Public License where the Licensed Rights include other Copyright and Similar Rights.\nSection 5 – Disclaimer of Warranties and Limitation of Liability.\n\nUNLESS OTHERWISE SEPARATELY UNDERTAKEN BY THE LICENSOR, TO THE EXTENT POSSIBLE, THE LICENSOR OFFERS THE LICENSED MATERIAL AS-IS AND AS-AVAILABLE, AND MAKES NO REPRESENTATIONS OR WARRANTIES OF ANY KIND CONCERNING THE LICENSED MATERIAL, WHETHER EXPRESS, IMPLIED, STATUTORY, OR OTHER. THIS INCLUDES, WITHOUT LIMITATION, WARRANTIES OF TITLE, MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT, ABSENCE OF LATENT OR OTHER DEFECTS, ACCURACY, OR THE PRESENCE OR ABSENCE OF ERRORS, WHETHER OR NOT KNOWN OR DISCOVERABLE. WHERE DISCLAIMERS OF WARRANTIES ARE NOT ALLOWED IN FULL OR IN PART, THIS DISCLAIMER MAY NOT APPLY TO YOU.\nTO THE EXTENT POSSIBLE, IN NO EVENT WILL THE LICENSOR BE LIABLE TO YOU ON ANY LEGAL THEORY (INCLUDING, WITHOUT LIMITATION, NEGLIGENCE) OR OTHERWISE FOR ANY DIRECT, SPECIAL, INDIRECT, INCIDENTAL, CONSEQUENTIAL, PUNITIVE, EXEMPLARY, OR OTHER LOSSES, COSTS, EXPENSES, OR DAMAGES ARISING OUT OF THIS PUBLIC LICENSE OR USE OF THE LICENSED MATERIAL, EVEN IF THE LICENSOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSSES, COSTS, EXPENSES, OR DAMAGES. WHERE A LIMITATION OF LIABILITY IS NOT ALLOWED IN FULL OR IN PART, THIS LIMITATION MAY NOT APPLY TO YOU.\nThe disclaimer of warranties and limitation of liability provided above shall be interpreted in a manner that, to the extent possible, most closely approximates an absolute disclaimer and waiver of all liability.\n\nSection 6 – Term and Termination.\n\nThis Public License applies for the term of the Copyright and Similar Rights licensed here. However, if You fail to comply with this Public License, then Your rights under this Public License terminate automatically.\nWhere Your right to use the Licensed Material has terminated under Section 6(a), it reinstates:\n\nautomatically as of the date the violation is cured, provided it is cured within 30 days of Your discovery of the violation; or\nupon express reinstatement by the Licensor.\n\nFor the avoidance of doubt, this Section 6(b) does not affect any right the Licensor may have to seek remedies for Your violations of this Public License.\nFor the avoidance of doubt, the Licensor may also offer the Licensed Material under separate terms or conditions or stop distributing the Licensed Material at any time; however, doing so will not terminate this Public License.\nSections 1, 5, 6, 7, and 8 survive termination of this Public License.\n\nSection 7 – Other Terms and Conditions.\n\nThe Licensor shall not be bound by any additional or different terms or conditions communicated by You unless expressly agreed.\nAny arrangements, understandings, or agreements regarding the Licensed Material not stated herein are separate from and independent of the terms and conditions of this Public License.\n\nSection 8 – Interpretation.\n\nFor the avoidance of doubt, this Public License does not, and shall not be interpreted to, reduce, limit, restrict, or impose conditions on any use of the Licensed Material that could lawfully be made without permission under this Public License.\nTo the extent possible, if any provision of this Public License is deemed unenforceable, it shall be automatically reformed to the minimum extent necessary to make it enforceable. If the provision cannot be reformed, it shall be severed from this Public License without affecting the enforceability of the remaining terms and conditions.\nNo term or condition of this Public License will be waived and no failure to comply consented to unless expressly agreed to by the Licensor.\nNothing in this Public License constitutes or may be interpreted as a limitation upon, or waiver of, any privileges and immunities that apply to the Licensor or You, including from the legal processes of any jurisdiction or authority.\n\n=======================================================================\nCreative Commons is not a party to its public licenses. Notwithstanding, Creative Commons may elect to apply one of its public licenses to material it publishes and in those instances will be considered the “Licensor.” The text of the Creative Commons public licenses is dedicated to the public domain under the CC0 Public Domain Dedication. Except for the limited purpose of indicating that material is shared under a Creative Commons public license or as otherwise permitted by the Creative Commons policies published at creativecommons.org/policies, Creative Commons does not authorize the use of the trademark “Creative Commons” or any other trademark or logo of Creative Commons without its prior written consent including, without limitation, in connection with any unauthorized modifications to any of its public licenses or any other arrangements, understandings, or agreements concerning use of licensed material. For the avoidance of doubt, this paragraph does not form part of the public licenses.\nCreative Commons may be contacted at creativecommons.org.\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Licenses",
      "Data (CC BY 4.0)"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "Contributing",
    "section": "",
    "text": "When contributing to this repository, please first discuss the change you wish to make via issue, email, or any other method with the owners of this repository before making a change.\nPlease note we have a code of conduct, please follow it in all your interactions with the project.\n\n\n\nEnsure any install or build dependencies are removed before the end of the layer when doing a build.\nUpdate the README.md with details of changes to the interface, this includes new environment variables, exposed ports, useful file locations and container parameters.\nIncrease the version numbers in any examples files and the README.md to the new version that this Pull Request would represent. The versioning scheme we use is SemVer.\nYou may merge the Pull Request in once you have the sign-off of two other developers, or if you do not have permission to do that, you may request the second reviewer to merge it for you.",
    "crumbs": [
      "Home",
      "Project",
      "Contributing"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html#pull-request-process",
    "href": "CONTRIBUTING.html#pull-request-process",
    "title": "Contributing",
    "section": "",
    "text": "Ensure any install or build dependencies are removed before the end of the layer when doing a build.\nUpdate the README.md with details of changes to the interface, this includes new environment variables, exposed ports, useful file locations and container parameters.\nIncrease the version numbers in any examples files and the README.md to the new version that this Pull Request would represent. The versioning scheme we use is SemVer.\nYou may merge the Pull Request in once you have the sign-off of two other developers, or if you do not have permission to do that, you may request the second reviewer to merge it for you.",
    "crumbs": [
      "Home",
      "Project",
      "Contributing"
    ]
  },
  {
    "objectID": "CHANGELOG.html",
    "href": "CHANGELOG.html",
    "title": "Changelog",
    "section": "",
    "text": "All notable changes to this project will be documented in this file.\nThe format is based on Keep a Changelog, and this project adheres to Semantic Versioning.\n\n\n\n\n\nDocs for Zenodo added\nFixed various typos and replaced favicon\n\n\n\n\n\nInitial version\n\n\n\n\n\n\n\n\n…\n\n\n\n\n\n\n\n\n…",
    "crumbs": [
      "Home",
      "Project",
      "Changelog"
    ]
  },
  {
    "objectID": "CHANGELOG.html#unreleased",
    "href": "CHANGELOG.html#unreleased",
    "title": "Changelog",
    "section": "",
    "text": "Docs for Zenodo added\nFixed various typos and replaced favicon\n\n\n\n\n\nInitial version",
    "crumbs": [
      "Home",
      "Project",
      "Changelog"
    ]
  },
  {
    "objectID": "CHANGELOG.html#section",
    "href": "CHANGELOG.html#section",
    "title": "Changelog",
    "section": "",
    "text": "…",
    "crumbs": [
      "Home",
      "Project",
      "Changelog"
    ]
  },
  {
    "objectID": "CHANGELOG.html#section-1",
    "href": "CHANGELOG.html#section-1",
    "title": "Changelog",
    "section": "",
    "text": "…",
    "crumbs": [
      "Home",
      "Project",
      "Changelog"
    ]
  },
  {
    "objectID": "abstract/index.html",
    "href": "abstract/index.html",
    "title": "Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections",
    "section": "",
    "text": "Digital archives promised to democratize access to cultural heritage, yet a significant portion of visual historical content remains inaccessible to blind and low-vision readers. Many digitized photographs, maps, manuscripts, and other images lack descriptive alternative text (alt-text), creating an epistemic barrier to the past. This perpetuates an asymmetry in sensory access to history, where sighted people hold privileged insight into visual sources while others are excluded. Making images legible through text is more than a technical fix—it is a matter of historical justice and inclusivity in digital humanities. Even beyond vision-impaired users, rich image descriptions can aid others, such as neurodivergent readers who benefit from explicit detail that sighted users might glean implicitly (Cecilia, Moussouri, and Fraser 2023a).\nAlt-text itself is not new: the HTML alt attribute dates back to the 1990s to support accessibility. However, providing high-quality image descriptions has often been a secondary priority in scholarly communication (Cecilia, Moussouri, and Fraser 2023b). Crafting alt-text is labor-intensive and typically left to authors or curators as a final step, if done at all. As Conrad (Conrad 2021) observes, the burden falls on sighted experts to determine what information is or is not included in an image’s description, an ethical responsibility that only the content’s author can fully shoulder. Author-generated descriptions are valued for capturing contextual meaning that automated tools might miss. They can greatly enhance the accessibility, searchability, and archivability of digital scholarship. Yet in practice, many projects—especially smaller public history initiatives—lack the resources to implement accessibility from the start. The result is that visual evidence remains “unseen” by those who rely on assistive technologies.\nRecent advances in multimodal AI offer a potential remedy. Vision-Language Models (VLMs) such as OpenAI’s GPT-4o, Google’s Gemini (Vision), and open-source systems like LLaVA-Next or Mistral’s Pixtral now claim near-human performance in image description tasks. These models can ingest an image and generate a caption or description, essentially simulating the interpretive act of a human describer. If guided properly, such models could produce high-quality, historically informed, and Web Content Accessibility Guidelines (WCAG 2.2)–conformant alt-text. This would dramatically reduce the human effort required to remediate large collections, enabling heritage institutions to scale up accessibility by generating alt-text for thousands of images. In turn, the “readership” of digital archives would expand to include those previously left out.\nHowever, adopting automated captioning in a heritage context raises critical questions about truth, evidence, and authenticity. Delegating descriptive labor to machines is not a neutral technical fix; it is an act imbued with values and biases. Deciding what details to include in an image’s description is technically difficult and ethically fraught, especially for historical images depicting people or sensitive cultural content. Vision models trained on general web images may inject anachronistic terms or biases (e.g., misidentifying a 1920s street scene as “Victorian”), reinforce curatorial blind spots, or omit crucial context that a human historian would provide. There is also the danger of techno-ableism (Shew 2023), where blind users’ needs are superficially addressed by technology without truly empowering them or respecting their perspectives. Uncritical use of AI could inadvertently recentre the sighted, algorithmic point of view rather than the lived experience of those using the alt-text.\nIn this work, we argue that AI-generated alt-text for historical collections is a pivotal test case for the entanglement of AI innovation, archival practice, and disability justice. But can a machine “see” history as we do? If a model can convincingly describe a photograph from 100 years ago, how does that change the way we verify and trust such descriptions? Embracing this kind of “machine vision” in historical scholarship may require new protocols akin to earlier paradigm shifts (for example, the move from handwritten catalog cards to MARC records, or from microfilm to digital scans). Just as those changes demanded critical awareness of how tools shape historical discovery, the use of AI-generated descriptions demands a new hermeneutic of suspicion. We must learn to critically read machine-generated metadata, much as we read any human-produced finding aid or annotation (Fickers 2022).\nThe central purpose of our study is to assess whether and how current AI models can serve as accessibility assistants in a digital history workflow, and to develop a critical framework for using them responsibly. Our approach is interdisciplinary, blending computational experimentation with qualitative, historiographically informed analysis. Concretely, we plan to experiment with state-of-the-art multimodal models to generate alt-text for a real-world public history collection, and we will evaluate the results for accessibility compliance, historical accuracy, and ethical soundness. By doing so, we aim to illuminate both the opportunities and the pitfalls of integrating AI into inclusive humanities scholarship. Each AI-generated caption is treated not just as metadata but as an interpretive act—one that can be scrutinized like any primary source.\nTo guide this inquiry, we pose the following research questions:\n\nFeasibility: Can current vision-language models produce useful, WCAG 2.2–compliant alt-text for complex historical images when provided with contextual metadata? We will examine whether models can meet accessibility guidelines (providing text alternatives that convey the same information as the image) and how the inclusion of metadata influences their output. We also consider the potential usefulness of these descriptions for both blind users and sighted users who may benefit from clear explanatory captions (Cecilia, Moussouri, and Fraser 2023a).\nQuality and Authenticity: How do domain experts (e.g., historians) rate AI-generated image descriptions in terms of factual accuracy, completeness, and usefulness for understanding historical content? We will evaluate the outputs for errors such as anachronisms, misidentifications, or hallucinated details, checking them against known facts from metadata and expert knowledge.\nEthics and Governance: What are the ethical implications of using AI to generate alt-text in heritage collections, and what human oversight or policy safeguards are required for responsible use? We will identify potential harms such as biased descriptions (e.g., normative terms), and address the broader question of how much interpretive agency should be ceded to AI in a curatorial context. We will explore strategies to mitigate these risks, including human-in-the-loop editing and transparency measures.\n\nBy answering these questions, our work will provide an empirical baseline for AI-assisted accessibility in the humanities. It will also offer a reflective critique, examining AI outputs as objects of study in their own right. In the following sections, we outline our data and methodology (Section 2), present initial observations from our experiments (Section 3), and discuss implications for digital humanities practice (Section 4), before concluding with planned next steps (Section 5).",
    "crumbs": [
      "Home",
      "Research",
      "Abstract"
    ]
  },
  {
    "objectID": "abstract/index.html#alt-text-generation-pipeline",
    "href": "abstract/index.html#alt-text-generation-pipeline",
    "title": "Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections",
    "section": "Alt-Text Generation Pipeline",
    "text": "Alt-Text Generation Pipeline\nModel Selection: We have selected four state-of-the-art vision-language models (as of mid-2025) to generate image descriptions. These represent a mix of proprietary and open-source systems: (1) GPT-4o (OpenAI’s multimodal GPT-4o), (2) Google Gemini (Vision), (3) LLaVA-Next (an open-source vision-LLM based on LLaMA-2, fine-tuned for vision-chat tasks), and (4) Mistral Pixtral (a vision-language model from Mistral’s NeMo framework). We include multiple models to gauge the range of performance and to see how open models compare to the cutting-edge commercial systems. All models are capable of accepting image input and returning a text description. Where possible, we use the latest available model checkpoints or API versions.\nPrompt Design: A key feature of our pipeline is providing each model with contextual metadata alongside the image, in order to ground the generation in relevant historical facts. We designed a prompt template (in the same language as the collection, i.e., German) that injects structured metadata fields and instructs the model to follow best practices for alt-text. In essence, the prompt tells the model that it is an accessibility assistant tasked with producing an alt-text for a cultural heritage image. It includes guidelines drawn from the WCAG 2.2 and accessibility literature on how to write good alt-text. For example, the prompt directs the model not to start with redundant phrases like “Bild von…” (“image of…”), to be concise (typically under $$120 characters for a simple informative image), and to include any essential visual text (like signs or captions visible in the image). It also asks the model to identify the type of image and adjust the response accordingly: e.g., if the image is a complex diagram or map, the model should produce a short alt-text plus note that a longer description will be provided; if the image is merely a photograph with informative content, a 1–2 sentence description suffices; if the image is mainly text (say a scanned document or poster), the model should either transcribe it (for short text like a sign) or indicate that a full transcription is available elsewhere for longer texts. These rules were distilled from accessibility resources (Project n.d.; World Wide Web Consortium 2023) to ensure the output serves blind users properly. An example snippet of our prompt template is: ” You are an expert in writing WCAG-compliant alt-text. The image comes from a history archive with metadata. Read the metadata and analyze the image. Determine the image type (informative photo, complex diagram/map, or text image) and produce the appropriate alt-text as per the guidelines…“*—followed by the specific instructions for each case. We have found in preliminary trials that including the complete metadata (title, date, etc.) in the prompt can prevent certain errors (for instance, knowing the year of the photo helps the model avoid describing attire as”modern”). All models are prompted with the same template structure for consistency, and all outputs are requested in German (to match the collection’s context and end-user language).\nGeneration and Post-processing: Using this prompt, we will run each image through each of the four models, yielding up to four candidate descriptions per image. The generation process will be automated via a Python script (using an API wrapper or library for each model). We anticipate producing around 6,000 candidate alt-texts (4 per image for $$1,500 images). After generation, minimal post-processing will be applied. In particular, we will strip any extraneous phrases if a model fails to follow instructions exactly (e.g., some might prepend “Alt-Text:” or polite greetings, which we will remove). We will not otherwise modify the content of the AI outputs at this stage. All results will be stored along with metadata and model identifiers for evaluation.\nIf a model refuses to describe an image due to some built-in safety filter (misidentifying a historical photograph as sensitive content), we will handle those on a case-by-case basis by leaving that image for human description. Overall, this pipeline is designed to maximize coverage (getting at least one description for every image) while maintaining quality through careful prompting.",
    "crumbs": [
      "Home",
      "Research",
      "Abstract"
    ]
  },
  {
    "objectID": "abstract/index.html#evaluation-strategy",
    "href": "abstract/index.html#evaluation-strategy",
    "title": "Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections",
    "section": "Evaluation Strategy",
    "text": "Evaluation Strategy\nOur evaluation of the AI-generated alt-text will address both accessibility compliance and historical accuracy in line with the research questions. We describe the planned evaluation steps below. All evaluation will be done on a representative subset of the data (approximately 100 images) due to time constraints, with the aim of scaling up later.\n(a) Accessibility and WCAG Compliance: We will assess whether the AI outputs meet established accessibility guidelines for alt-text. This involves checking each description against a checklist of best practices (e.g., does the alt-text sufficiently describe the image’s important content and function? Does it avoid unnecessary phrases like “an image of”? If the image contains readable text or numbers, are those included or summarized in the alt-text?). We are adapting the Alt Text Checklist from the A11y Project and WCAG techniques as our evaluation rubric. Each candidate description for an image will be reviewed by at least two team members with knowledge of accessibility standards. In cases where the image is a diagram or chart, we will check that the model followed instructions (providing a short summary alt-text and indicating a longer description would be needed). For images of documents, we check that any text was appropriately handled (transcribed or deferred to full text). The outcome of this step will be a rating or rank of the candidates for each image in terms of compliance. We expect that the model prompted with metadata and guidelines will produce mostly compliant alt-text, whereas some simpler models might yield overly generic or incomplete captions. An initial pilot test supports this: for example, without metadata, an open-source model captioned a photo as “Old photo of a street” which misses key specifics, but with our metadata-enhanced prompt GPT-4o produced “Schwarzweiß-Fotografie einer belebten Straße in Basel, 1917, mit Demonstranten, die Banner in Frakturschrift halten.” (Black-and-white photograph of a busy Basel street in 1917, with protesters holding banners in Gothic script), which is far richer and ticks more of the accessibility boxes (it mentions the context, the presence of text on banners, etc.). This step addresses the first research question by testing whether models can be guided to meet alt-text requirements. We will quantify common compliance issues and note which model outputs most often require correction.\n(b) Historical Accuracy and Usefulness: The second layer of evaluation focuses on the content accuracy and value of the descriptions from a historian’s perspective. We will conduct a blind review where domain experts (trained historians) examine the AI-generated alt-text for a given image and compare it to the known metadata or facts about that image. Each expert will be presented with the four alt-text candidates for an image and will be asked to order them by relative factual correctness—that is, ranking the descriptions from most to least accurate in terms of representing the image content. This ranking focuses on the relative quality among the alternatives rather than absolute judgments. For example, a model might mistakenly label a horse-drawn carriage in a 1890 photo as a “car” (anachronistic), or it might hallucinate a “red stamp in the corner” of a document that does not exist. Such errors are critical to catch, as they could mislead researchers. On the other hand, we will also note cases where the AI description includes details that the original metadata or caption did not mention. In preliminary tests, we observed instances of this “AI insight”: e.g., a model noted “ein handgezeichneter roter Umriss auf dem Stadtplan” (a hand-drawn red outline on the map) which the human catalog description had not recorded. Upon checking the image, there was indeed a red pen marking on the map, presumably added by a later hand. Discovering these additional details could be beneficial, pointing scholars to visual evidence they might otherwise overlook. Our expert reviewers will differentiate between such legitimate additions and illegitimate hallucinations. We aim to categorize common error types (misidentifications, missed context, invented details) and measure the proportion of AI-generated alt-text that is acceptable with minimal or no editing versus those that need substantial correction. We anticipate, based on prior work and initial runs, that a majority of descriptions (over 90%) will be largely correct, while a significant minority will have issues requiring human intervention. The results of this step will inform how much post-editing effort is needed when deploying these models in practice.\n(c) Ethical Review: In parallel with the above, we will perform a qualitative analysis of the AI outputs to identify any ethical or bias concerns. This involves scanning the descriptions for inappropriate language or perspective. For instance, we will check if any descriptions contain terms or tones that are outdated or offensive (e.g., describing people in a demeaning way). We are particularly attentive to ableist language: while unlikely, we want to ensure the alt-text does not include phrases like “suffers from blindness” or similar, which are not acceptable in modern accessibility writing (Holmes 2020). If the model describes people, we examine whether it is making unwarranted assumptions about their identity (race, gender, etc.) or appearance. One concrete example: one model output described an older photograph of a man as “ein afrikanischer Mann” (“an African man”). The image indeed depicted a Black man, but in context his nationality or ethnicity was not documented and not necessarily relevant to the image’s purpose. Including such a descriptor could be seen as othering or speculative, so our policy is to avoid it unless it is directly pertinent (Hanley et al. 2021). In our review process, any such cases will be flagged and either removed or revised. We will also consider the implications of the model’s choices of detail: what the AI focuses on can reflect implicit bias (e.g., always mentioning a woman’s appearance but not a man’s). By compiling these observations, we will derive guidelines for curators on how to handle AI-generated descriptions. The ethical review is not a separate step per se, but integrated into the human-in-the-loop oversight—no AI-generated alt-text will be added to the public collection without passing this human review stage.",
    "crumbs": [
      "Home",
      "Research",
      "Abstract"
    ]
  },
  {
    "objectID": "paper/index.html",
    "href": "paper/index.html",
    "title": "Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections",
    "section": "",
    "text": "Digital archives promised to democratize access to cultural heritage, yet a significant portion of visual historical content remains inaccessible to people who are blind or have low vision. Many digitized photographs, maps, manuscripts, and other images lack descriptive alternative text (alt-text), creating an epistemic barrier to the past. This perpetuates an asymmetry in sensory access to history, where sighted people hold privileged insight into visual sources while non-sighted audiences encounter barriers to engagement. Making images legible through text is more than a technical fix—it is a matter of historical justice and inclusivity in digital humanities. Even beyond blind and low-vision users, rich image descriptions can aid others, such as neurodivergent readers who benefit from explicit detail that sighted users might glean implicitly (Cecilia, Moussouri, and Fraser 2023a).\nAlt-text itself is not new: the HTML alt attribute dates back to the 1990s to support accessibility. However, providing high-quality image descriptions has often been a secondary priority in scholarly communication (Cecilia, Moussouri, and Fraser 2023b). Crafting alt-text is labor-intensive and typically left to authors or curators as a final step, if done at all. The burden often falls on sighted domain experts (not accessibility experts) to determine what information is or is not included in an image’s description. Human-generated descriptions are valued for capturing contextual meaning and can greatly enhance the accessibility, searchability, and archivability of digital scholarship. Yet in practice, many projects—especially smaller public history initiatives—lack the resources to implement accessibility from the start. The result is that visual evidence remains “unseen” by those who rely on assistive technologies.\nRecent advances in multimodal AI offer a potential remedy. Vision-Language Models (VLMs) such as OpenAI’s GPT-4o mini, Google’s Gemini 2.5 Flash Lite, and open-weight systems like Meta’s Llama 4 Maverick or Qwen’s Qwen3 VL 8B Instruct now claim near-human performance in image description tasks. These models can ingest an image and generate a caption or description, essentially simulating the interpretive act of a human describer. If these models could produce alt-text that is both high-quality and historically informed as well as aligned with the Web Content Accessibility Guidelines (WCAG 2.2) (World Wide Web Consortium 2023), this would dramatically reduce the human effort required to remediate large collections. Heritage institutions could then scale up accessibility by generating alt-text for thousands of images, because the costs of machine captioning are negligible in comparison to a human expert. Consequently, the “readership” of digital archives would expand to include those who were previously excluded.\nHowever, adopting automated captioning in a heritage context raises critical questions about truth, evidence, and authenticity. Delegating descriptive labor to machines is not a neutral technical fix; it is an act imbued with values and biases (Bowker and Star 1999). Deciding what details to include in an image’s description is technically difficult and ethically fraught, especially for historical images depicting people or sensitive cultural content. Vision models trained on general web images may uncritically adopt source terminology, inject anachronistic biases (e.g., misidentifying a 1920s street scene as “Victorian”), reinforce curatorial blind spots, or omit crucial context that a human historian would provide. There is also the danger of techno-ableism (Shew 2023), where the needs of people who are blind are superficially addressed by technology without truly empowering them or respecting their perspectives. Uncritical use of AI could inadvertently recenter the sighted, algorithmic point of view rather than the lived experience of those using the alt-text.\nIn this work, we argue that AI-generated alt-text for historical collections is a pivotal test case for the entanglement of AI innovation, archival practice, and disability justice. But can a machine “see” history as we do? If a model can convincingly describe a photograph from 100 years ago, how does that change the way we verify and trust such descriptions? Embracing this kind of “machine vision” in historical scholarship may require new protocols akin to earlier paradigm shifts (for example, the move from handwritten catalog cards to MARC records, or from microfilm to digital scans). Just as those changes demanded critical awareness of how tools shape historical discovery, the use of AI-generated descriptions demands a new hermeneutic of suspicion. We must learn to critically read machine-generated metadata, much as we read any human-produced finding aid or annotation (Fickers 2022).\nThe central purpose of our study is to assess whether and how current AI models can serve as accessibility assistants in a digital history workflow, and to critically examine the conditions and implications of their responsible use.\nOur approach is interdisciplinary, blending computational experimentation with qualitative, historiographically informed analysis. The research design comprises the following steps:\n\nData compilation: We compile a small yet balanced dataset consisting of historical sources and research data.\nModel selection and prompt development: We conduct WCAG-aligned prompt engineering and model selection in an iterative and exploratory manner.\nGeneration and data collection: Once an optimal configuration of prompts and models has been identified, we generate candidate alternative texts (alt-text) and collect quantitative data on coverage, throughput, and unit cost.\nExpert evaluation: A group of 21 domain experts—humanities scholars with relevant disciplinary expertise—evaluate and rank the AI-generated alt-text.\nExpert review: The authors qualitatively assess a selection of the highest-ranked alt-text for factual accuracy, contextual adequacy, and bias reproductions.\nAnalysis: We perform both statistical and qualitative analyses of the data obtained in steps 3–5.\n\nBy doing so, we aim to illuminate both the opportunities and the pitfalls of integrating AI into inclusive humanities scholarship.\n\n\nTo guide this inquiry, we pose the following research questions:\n\nRQ1 Feasibility: What coverage, throughput, and unit cost can current VLMs achieve for WCAG-aligned alt-text on a heterogeneous heritage corpus, and where do they fail?\nRQ2 Relative quality: How do experts rank model outputs? What error patterns recur?\n\nBy answering these questions, our work helps to establish an empirical baseline for AI-assisted accessibility in the humanities. It also offers a reflective critique, examining AI outputs as objects of study in their own right. In the following sections, we outline our data and methodology (Section 2), present initial observations from our experiments (Section 3), and discuss implications for digital humanities practice (Section 4), before concluding with planned next steps (Section 5).",
    "crumbs": [
      "Home",
      "Research",
      "Paper"
    ]
  },
  {
    "objectID": "paper/index.html#research-questions",
    "href": "paper/index.html#research-questions",
    "title": "Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections",
    "section": "",
    "text": "To guide this inquiry, we pose the following research questions:\n\nRQ1 Feasibility: What coverage, throughput, and unit cost can current VLMs achieve for WCAG-aligned alt-text on a heterogeneous heritage corpus, and where do they fail?\nRQ2 Relative quality: How do experts rank model outputs? What error patterns recur?\n\nBy answering these questions, our work helps to establish an empirical baseline for AI-assisted accessibility in the humanities. It also offers a reflective critique, examining AI outputs as objects of study in their own right. In the following sections, we outline our data and methodology (Section 2), present initial observations from our experiments (Section 3), and discuss implications for digital humanities practice (Section 4), before concluding with planned next steps (Section 5).",
    "crumbs": [
      "Home",
      "Research",
      "Paper"
    ]
  },
  {
    "objectID": "paper/index.html#dataset-for-alt-text-generation-and-evaluation",
    "href": "paper/index.html#dataset-for-alt-text-generation-and-evaluation",
    "title": "Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections",
    "section": "2.1 Dataset for Alt Text Generation and Evaluation",
    "text": "2.1 Dataset for Alt Text Generation and Evaluation\nFor our survey, we compiled a dataset designed to represent both the heterogeneity of media types and the timeframe covered by the Stadt.Geschichte.Basel project. The project collection, published on our Open Research Data Platform, features more than 1700 media objects including metadata as of October 2025. From this corpus, we created a dataset to use for alt-text generation trials. This dataset comprises a hundred items and is released with this paper to be used for benchmarking purposes. Additionally, we created a subset of 20 items to make it more feasible to evaluate alt-text in an expert survey. For both sets, items were selected to maintain representativeness across the same dimensions while being manageable for expert reviewers to assess within a reasonable time frame. (See Section 8.1 for a more detailed description of the dataset).\nAll items were categorized into ten distinct media types (e.g. paintings, maps, scans of newspapers etc., see Section 8.1), allowing us to ensure a balanced distribution of content. Data types primarily comprise images and figures, maps and geodata, tables and statistics, and bibliographic references (Mähr 2022): Heterogeneous digitized items including historical photographs, reproductions of artifacts, city maps and architectural plans, handwritten letters and manuscripts, statistical charts, and printed ephemera (e.g., newspaper clippings, posters). We made sure to include items with complex visual structures (items that need additional information to convey their meaning, e.g., a legend for maps or diagrams), items with visible text in different languages (e.g., scans of newspapers or posters) as well as items with potentially sensitive content (e.g., content with derogatory and/or racist terminology).\nTo prompt the models as described below, we used JPG files at a standardized size of 800×800 pixels – the same resolution employed for human viewers on our online platform – and their corresponding metadata in JSON format.",
    "crumbs": [
      "Home",
      "Research",
      "Paper"
    ]
  },
  {
    "objectID": "paper/index.html#dataset-limitations",
    "href": "paper/index.html#dataset-limitations",
    "title": "Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections",
    "section": "2.2 Dataset Limitations",
    "text": "2.2 Dataset Limitations\nThe number of eligible items is reduced by excluding items that are only available with placeholder images on our platform due to copyright restrictions. Additionally, due to the typesetting workflow during the production of the printed volumes, some collection items had to be split up into different files – maps and charts where the legend is provided in a second image file, separate from the main figure. This pertains to 19 out of 100 items in our data set, respectively four out of 20 items in the survey. Connections between these segmented files are made explicit in our metadata, but the models only receive one image file as input at a time, leading to some loss of information that would be visually available to a human reader. This could result in a lower description quality.",
    "crumbs": [
      "Home",
      "Research",
      "Paper"
    ]
  },
  {
    "objectID": "paper/index.html#model-selection",
    "href": "paper/index.html#model-selection",
    "title": "Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections",
    "section": "2.3 Model Selection",
    "text": "2.3 Model Selection\nWe selected four multimodal vision-language models (VLMs) representing a balance of open-weight and proprietary systems with comparable cost and capability:1\n\nModels used for evaluation. Data reported by OpenRouter (27.10.2025).\n\n\nModel\nDeveloper\nOpenness\nContext\nLatency (s)\nInput $/M\nOutput $/M\nNotes\n\n\n\n\nGemini 2.5 Flash Lite\nGoogle\nProprietary\n1.05M\n0.42\n0.10\n0.40\nFast, low-cost; optimized for captioning.\n\n\nLlama 4 Maverick\nMeta\nOpen weights\n1.05M\n0.56\n0.15\n0.60\nMultilingual, multimodal reasoning.\n\n\nGPT-4o mini\nOpenAI\nProprietary\n128K\n0.58\n0.15\n0.60\nCompact GPT-4o variant; strong factual grounding.\n\n\nQwen3 VL 8B Instruct\nAlibaba\nOpen-weight\n131K\n1.29\n0.08\n0.50\nRobust open baseline with OCR features.\n\n\n\nSelection criteria:\n\nOpenness & diversity – two proprietary (OpenAI, Google) and two open-weight (Meta, Qwen) models.\nCost-capability parity – all models priced between $0.08–$0.15/M input and $0.40–$0.60/M output tokens, with ≥100K context windows.\nMultilingual & visual competence – explicit support for German and image understanding.\n\nThe aim was to cover diverse architectures and governance regimes while maintaining fairness in performance evaluation.",
    "crumbs": [
      "Home",
      "Research",
      "Paper"
    ]
  },
  {
    "objectID": "paper/index.html#prompt-engineering",
    "href": "paper/index.html#prompt-engineering",
    "title": "Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections",
    "section": "2.4 Prompt engineering",
    "text": "2.4 Prompt engineering\nWe systematically varied prompt roles and placement, comparing instruction blocks in the system prompt versus the user prompt, and front-loading versus trailing constraints. We used the same user and system prompts for all models in zero-shot mode. Following evidence that models privilege information in short and well structured prompts, we fixed normative requirements (WCAG 2.2 aligned, de-CH style, length limits, handling of decorative/functional/complex images) in the system prompt and kept the user prompt minimal and image-bound to reduce “lost-in-the-middle” effects (Liu et al. 2023). The user prompt injected collection-specific metadata—title, description, EDTF date, era, creator/publisher/source—and a concise description of the purpose of the alt text, then the image URL. Adding this structured context markedly improved specificity, reduced refusals, and lowered hallucinations, consistent with retrieval-style findings that supplying external, task-relevant evidence boosts generation quality and faithfulness. Recent work confirms that vision–language models can serve such accessibility roles when embedded in context-rich pipelines.\nIn particular, user studies with blind and low-vision participants demonstrate that context-aware image descriptions—those combining visual and webpage metadata—are preferred and rated higher for quality, imaginability, and plausibility than context-free baselines (Mohanbabu and Pavel 2024).\nThis supports our design choice to inject structured collection metadata into the prompt..\nThese results are consistent with findings that prompt structure and multimodal fusion can systematically shift which visual cues VLMs rely on (Gavrikov et al. 2025).\nBy anchoring metadata before the image input, we effectively steer the model toward shape- and context-based reasoning rather than shallow texture correlations—an effect analogous to prompt-based cue steering observed in vision-language bias studies.\ndef build_prompt(media: MediaObject) -&gt; str:\n    return f\"\"\"Titel: {media.title or \"Kein Titel\"}\nBeschreibung: {media.description or \"Keine Beschreibung\"}\nErsteller: {media.creator or \"Kein Ersteller\"}\nHerausgeber: {media.publisher or \"Kein Herausgeber\"}\nQuelle: {media.source or \"Keine Quelle\"}\nDatum: {media.date or \"Kein Datum\"}\nEpoche: {media.era or \"Keine Epoche\"}\"\"\".strip()\n\n\ndef build_messages(\n    prompt: str, image_url: str\n) -&gt; tuple[list[dict[str, Any]], str, str]:\n    system = \"\"\"ZIEL\n\nAlt-Texte für historische und archäologische Sammlungsbilder.\nKurz, sachlich, zugänglich. Erfassung der visuellen Essenz für Screenreader.\n\nREGELN\n\n1. Essenz statt Detail. Keine Redundanz zum Seitentext, kein „Bild von“.\n2. Zentralen Text im Bild wiedergeben oder kurz paraphrasieren.\n3. Kontext (Epoche, Ort, Gattung, Material, Datierung) nur bei Relevanz für Verständnis.\n4. Prägnante visuelle Merkmale nennen: Farbe, Haltung, Zustand, Attribute.\n5. Karten/Diagramme: zentrale Aussage oder Variablen.\n6. Sprache: neutral, präzise, faktenbasiert; keine Wertung, keine Spekulation.\n7. Umfang:\n   * Standard: 90–180 Zeichen\n   * Komplexe Karten/Tabellen: max. 400 Zeichen\n\nVERBOTE\n\n* Kein alt=, Anführungszeichen, Preambeln oder Füllwörter („zeigt“, „darstellt“).\n* Keine offensichtlichen Metadaten (z. B. Jahreszahlen aus Beschriftung).\n* Keine Bewertungen, Hypothesen oder Stilkommentare.\n* Keine Emojis oder emotionalen Begriffe.\n\nHEURISTIKEN\n\nPorträt: Person (Name, falls bekannt), Epoche, Pose oder Attribut, ggf. Funktion.\nObjekt: Gattung, Material, Datierung, auffällige Besonderheit.\nDokument: Typ, Sprache/Schrift, Datierung, Kernaussage.\nKarte: Gebiet, Zeitraum, Zweck, Hauptvariablen.\nEreignisfoto: Wer, was, wo, situativer Kontext.\nPlakat/Cover: Titel, Zweck, zentrale Schlagzeile.\n\nFALLBACK\n\nUnklarer Inhalt: generische, aber sinnvolle Essenz aus Metadaten.\n\nQUELLEN\n\nNur visuelle Analyse (Bildinhalt) und übergebene Metadaten. Keine externen Kontexte.\"\"\".strip()\n    return (\n        [\n            {\"role\": \"system\", \"content\": system},\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": prompt},\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n                ],\n            },\n        ],\n        system,\n        prompt,\n    )",
    "crumbs": [
      "Home",
      "Research",
      "Paper"
    ]
  },
  {
    "objectID": "paper/index.html#alt-text-generation-and-post-processing",
    "href": "paper/index.html#alt-text-generation-and-post-processing",
    "title": "Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections",
    "section": "2.5 Alt Text Generation and Post-processing",
    "text": "2.5 Alt Text Generation and Post-processing\nUsing the carefully engineered system and user prompts, we ran each image through each of the four models, yielding four candidate descriptions per image. The generation process was automated via a Python script using OpenRouter as an API wrapper. We produced 80 candidate alt-texts (4 per image for n=20 images in our survey). After generation, no post-processing was applied. All results were stored along with metadata and model identifiers for evaluation.\nNo model refused to describe an image due to some built-in safety filter (labelling a historical photograph as sensitive content). Otherwise we would have handled those on a case-by-case basis by leaving that image for human description. Overall, this pipeline is designed to be simple, and maximize coverage (getting at least one description for every image) while maintaining quality through careful prompting.",
    "crumbs": [
      "Home",
      "Research",
      "Paper"
    ]
  },
  {
    "objectID": "paper/index.html#survey",
    "href": "paper/index.html#survey",
    "title": "Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections",
    "section": "2.6 Survey",
    "text": "2.6 Survey\nTwenty-one humanities scholars ranked, per image, four model-generated descriptions from best (1) to worst (4) under WCAG-intended criteria for alt text. Raters were asked to consider: (a) concise rendering of the core visual content; (b) avoidance of redundant phrases (e.g., “image of”); (c) prioritisation of salient visual features (persons, objects, actions, visible text); and (d) context inclusion only when it improves comprehension. While factual accuracy, completeness, and absence of bias were not primary ranking dimensions, they may have been factored in implicitly.",
    "crumbs": [
      "Home",
      "Research",
      "Paper"
    ]
  },
  {
    "objectID": "paper/index.html#close-reading",
    "href": "paper/index.html#close-reading",
    "title": "Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections",
    "section": "2.7 Close reading",
    "text": "2.7 Close reading\nTo check for these dimensions, the authors conducted a qualitative close reading of a selection of the generated alt-text. This analysis specifically targeted outputs that had received the highest rankings from the expert panel.\n\nFactual accuracy (Did the generated description contain any incorrect identifications of people, objects, or actions?)\nContextual adequacy (Did the generated description include any incorrect or misleading historical context?)\nBias reproduction (Did the model reproduce sensitive, derogatory, or racist terminology from the source material?)\n\nThis allowed us to investigate whether an alt-text could be ranked highly for WCAG alignment while simultaneously being factually incorrect or ethically problematic.",
    "crumbs": [
      "Home",
      "Research",
      "Paper"
    ]
  },
  {
    "objectID": "paper/index.html#rq1-feasibility-coverage-throughput-and-unit-cost",
    "href": "paper/index.html#rq1-feasibility-coverage-throughput-and-unit-cost",
    "title": "Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections",
    "section": "3.1 RQ1 Feasibility: Coverage, Throughput, and Unit Cost",
    "text": "3.1 RQ1 Feasibility: Coverage, Throughput, and Unit Cost\nTo address the feasibility of automatic alt-text generation at corpus scale, we compared four state-of-the-art vision–language models (VLMs): Google Gemini 2.5 Flash Lite, Meta Llama 4 Maverick, OpenAI GPT-4o mini, and Qwen 3 VL 8B Instruct.\nEach model generated alt-text descriptions for 20 representative heritage images selected for diversity of content, medium, and metadata completeness.\n\n3.1.1 Coverage and reliability\nAll models returned non-empty outputs for all 20 prompts, yielding 100 % coverage and no failed responses.\nThis demonstrates that current VLMs can reliably produce textual descriptions even for heterogeneous heritage data without the need for fallback mechanisms.\n\n\n3.1.2 Throughput and latency\nProcessing speed ranged between 0.24 – 0.43 items/s, corresponding to median latencies of 2 – 4 s per item. Models were accessed via OpenRouter.ai with the following providers: Google – gemini-2.5-flash-lite, OpenAI – gpt-4o-mini, Together – llama-4-maverick, and Alibaba – Qwen 3 VL 8B.\nQwen 3 VL 8B achieved the fastest throughput and lowest latency, while OpenAI GPT-4o mini was slower but consistent.\nAll models showed moderate response-time variability (≈ 1.7 – 10 s).\n\n\n3.1.3 Cost efficiency\nUnit generation costs differed by two orders of magnitude, reflecting API pricing rather than architectural complexity. According to OpenRouter.ai cost reports, costs per item ranged from $1.8 × 10⁻⁴ (Qwen) to $3.6 × 10⁻³ (OpenAI).\n\nFeasibility metrics for alt text generation (n = 20).\n\n\n\n\n\n\n\n\n\nModel\nCoverage (%)\nThroughput (items/s)\nMedian Latency (s)\nMean Cost (USD/item)\n\n\n\n\nGoogle Gemini 2.5 Flash Lite\n100\n0.31\n2.72\n0.000215\n\n\nMeta Llama 4 Maverick\n100\n0.41\n2.38\n0.000395\n\n\nOpenAI GPT-4o Mini\n100\n0.24\n4.00\n0.003625\n\n\nQwen 3 VL 8B Instruct\n100\n0.43\n2.27\n0.000182\n\n\n\n\n\n3.1.4 Summary of RQ1\nAll models achieved complete coverage, acceptable latency, and minimal cost, confirming the technical and economic feasibility of automated alt text generation for large, heterogeneous cultural collections.\nFailures were not due to empty outputs but to qualitative weaknesses, which are examined under RQ2.",
    "crumbs": [
      "Home",
      "Research",
      "Paper"
    ]
  },
  {
    "objectID": "paper/index.html#rq2-relative-quality-expert-ranking-and-qualitative-assessment",
    "href": "paper/index.html#rq2-relative-quality-expert-ranking-and-qualitative-assessment",
    "title": "Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections",
    "section": "3.2 RQ2 Relative Quality: Expert Ranking and Qualitative Assessment",
    "text": "3.2 RQ2 Relative Quality: Expert Ranking and Qualitative Assessment\n\n3.2.1 Quantitative ranking analysis\nWithin each task, all models were directly compared; task-level median ranks were analyzed across the 20 tasks using the Friedman test for repeated measures, followed by pairwise Wilcoxon signed-rank tests with Holm–Bonferroni correction.\nAgreement across tasks was quantified with Kendall’s W.\n\\[\n\\chi^2(3, N = 20) = 6.02, \\quad p = 0.11, \\quad W = 0.0085.\n\\]\nThe results indicate no statistically significant difference among models (\\(p &gt; 0.05\\)) and very low inter-task agreement (\\(W \\approx 0.01\\)), implying that relative rankings varied substantially by task. Pairwise Wilcoxon comparisons (Section 8.4) showed no significant differences after correction (\\(p_{\\text{Holm}} &gt; 0.5\\)); unadjusted \\(p\\)-values suggested weak, non-significant trends favoring OpenAI GPT-4o Mini and Qwen 3 VL 8B over Google Gemini and Meta Llama.\n\n\n3.2.2 Descriptive patterns\nTwenty-one human experts each rated four alternative texts for 20 images, yielding a total of 420 individual ratings:\n\n\n\nModel\nRank 1\nRank 2\nRank 3\nRank 4\n\n\n\n\nGoogle Gemini 2.5 Flash Lite\n86\n84\n113\n137\n\n\nMeta Llama 4 Maverick\n88\n114\n110\n108\n\n\nOpenAI GPT-4o Mini\n132\n101\n84\n103\n\n\nQwen 3 VL 8B Instruct\n114\n121\n113\n72\n\n\n\nOpenAI and Qwen outputs received more first-place and fewer last-place rankings, but overlapping rank distributions Section 8.3 indicate that these tendencies remain descriptive rather than inferentially significant.\n\n\n3.2.3 Qualitative evaluation of top-rated outputs\nA manual close reading inspection of hand-picked alt texts with highest mean rank scores revealed that even those outputs deemed to be the ‘best’ were not free from substantive and ethical shortcomings. In fact, all models produced at least one error. Some are easy to catch in a manual review (factually wrong descriptions), others require expert domain knowledge (reproduction of stereotypes).\n\n3.2.3.1 Example for Factually Wrong Text\n\n\n\nFaltblatt der Gruppe ‹Freiräume für Frauen› (FFF) (m92410)\n\n\n\nBest-ranked alt-text: «Faltblatt der Gruppe ‹Freiräume für Frauen› (FFF) von 1992. Zwei Personen gehen eine Treppe hinunter, Frau mit Zigarette, Mann mit Sonnenbrille und Händen in den Hosentaschen» (Google Gemini 2.5 Flash Lite)\nTranslation: “Leaflet of the ‘Freiräume für Frauen’ (FFF) group from 1992. Two people walk down a staircase, a woman with a cigarette, a man with sunglasses and hands in his pockets”\nShortcoming: This description is factually incorrect. The two people in the photograph are clearly walking up the staircase, not down.\n\n\n\n3.2.3.2 Example for Reproduction of Stereotypes\n\n\n\n‹Gegen den Eisenbahnhandel›, anonyme antisemitische Karikatur in der Extraausgabe der Basler ‹Allgemeinen Schweizer Zeitung› vom 14. Februar 1898 (m91960)\n\n\n\nBest-ranked alt-text: «Antisemitische Karikatur zur Eisenbahnverstaatlichung 1898. Links: Wehrhafte Helvetia zwingt knienden Geldverleiher nieder. Rechts: Kniende Helvetia bittet reichen Juden um Geld. Kontrastdarstellung zum Volksentscheid über die Eisenbahnverstaatlichung» (Meta Llama 4 Maverick)\nTranslation: “Antisemitic caricature on the nationalization of the railways 1898. Left: Defensive Helvetia forces a kneeling moneylender down. Right: Kneeling Helvetia asks a rich Jew for money. Contrasting depiction of the referendum on the nationalization of the railways”\nShortcoming: The text uncritically reproduces the term “reichen Juden” (rich Jew). This is an antisemitic stereotype likely drawn from the item’s metadata or biased training data, not a neutral description of the visual content.\n\n\n\n3.2.3.3 Example for Selective Description\n\n\n\nZuschauerzahlen von FC Basel und Konzerten im St. Jakob-Stadion, 1960–2022 (m88415_1)\n\n\n\nBest ranked alt-text: «Balkendiagramm: Zuschauerzahlen von FC Basel (blau) und Konzerten (gelb) im St. Jakob-Stadion, 1960–2022. Höchstwerte bei Fußballspielen ab 2001 im neuen St. Jakob-Park, Einbruch 2020 durch Corona-Pandemie» (Meta Llama 4 Maverick)\nTranslation: “Bar chart: Audience numbers for FC Basel (blue) and concerts (yellow) at St. Jakob-Stadion, 1960–2022. Peak values for football matches from 2001 in the new St. Jakob-Park, slump in 2020 due to the Corona pandemic”\nShortcoming: The description is selective and unbalanced. It provides a detailed interpretation of the trends for the football matches (blue bars) but does not give information or interpretation about neither the concert attendance (yellow bars), nor the number of football matches (green circles), omitting a lot of the chart’s comparative data.\n\n\n\n\n3.2.4 Interpretation\nThese examples for erroneous alt texts that still were ranked best in our survey illustrate that high quantitative rankings do not imply factual accuracy or ethical adequacy as illustrated by a close reading. Even when linguistically fluent and stylistically polished, VLM-generated alt texts may introduce epistemic distortions or perpetuate historical bias.\nQuantitatively, no model achieved a statistically distinct performance profile; qualitatively, all exhibited systematic error patterns—misrecognition, omission, and uncritical reproduction of harmful source language.\nThis combination highlights the limits of rank-based evaluation alone: expert preference captures relative quality but not factual or ethical soundness.",
    "crumbs": [
      "Home",
      "Research",
      "Paper"
    ]
  },
  {
    "objectID": "paper/index.html#synthesis",
    "href": "paper/index.html#synthesis",
    "title": "Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections",
    "section": "3.3 Synthesis",
    "text": "3.3 Synthesis\n\nRQ1 Feasibility: All four VLMs achieved full coverage, low latency, and negligible cost, confirming the operational viability of automated WCAG-aligned alt text generation for heritage corpora.\nRQ2 Relative quality: Expert rankings showed no statistically significant hierarchy among models (\\(p = 0.11\\), \\(W \\approx 0.01\\)), and qualitative inspection exposed factual inaccuracies, biased reproduction, and selective omissions even in top-rated outputs.\n\nOverall, current VLMs can populate heritage databases at scale but require expert review and critical post-editing to ensure factual precision, ethical compliance, and contextual adequacy. Automated alt text workflows should therefore combine model ensembles with targeted human oversight to meet both accessibility and historiographical standards.",
    "crumbs": [
      "Home",
      "Research",
      "Paper"
    ]
  },
  {
    "objectID": "paper/index.html#sec-appendix-data",
    "href": "paper/index.html#sec-appendix-data",
    "title": "Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections",
    "section": "8.1 Dataset description",
    "text": "8.1 Dataset description\n\nIn both our selection of 100-item and the 20-item survey subset, we tried to find an overall balance between all data types and eras that make up the Stadt.Geschichte.Basel collection. Due to its historical nature, not all data types appear in all eras, and the smaller size of the survey subset accentuates these constraints. We dropped Painting items and the Antiquity era from the survey subset due to their low prevalence in our corpus.",
    "crumbs": [
      "Home",
      "Research",
      "Paper"
    ]
  },
  {
    "objectID": "paper/index.html#system-performance-analysis",
    "href": "paper/index.html#system-performance-analysis",
    "title": "Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections",
    "section": "8.2 System Performance Analysis",
    "text": "8.2 System Performance Analysis\n\n\n\n\nThroughput, Latency, and Cost by Model",
    "crumbs": [
      "Home",
      "Research",
      "Paper"
    ]
  },
  {
    "objectID": "paper/index.html#sec-appendix-rank-distribution",
    "href": "paper/index.html#sec-appendix-rank-distribution",
    "title": "Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections",
    "section": "8.3 Rank Distributions and Aggregate Performance",
    "text": "8.3 Rank Distributions and Aggregate Performance\n\n\n\n\nCounts of Ranks per Model (All Ratings)\n\n\n\n\n\nRank Distributions per Model (Task-Level Medians; Lower = Better)\n\n\n\nRank counts per object and model.\n\n\n\n\n\n\n\n\n\n\nobjectid\nmodel\ncount_rank_1\ncount_rank_2\ncount_rank_3\ncount_rank_4\n\n\n\n\nm12965\ngoogle/gemini-2.5-flash-lite\n0\n9\n8\n4\n\n\nm12965\nmeta-llama/llama-4-maverick\n11\n0\n4\n6\n\n\nm12965\nopenai/gpt-4o-mini\n7\n5\n4\n5\n\n\nm12965\nqwen/qwen3-vl-8b-instruct\n3\n7\n5\n6\n\n\nm13176\ngoogle/gemini-2.5-flash-lite\n2\n4\n5\n10\n\n\nm13176\nmeta-llama/llama-4-maverick\n4\n4\n10\n3\n\n\nm13176\nopenai/gpt-4o-mini\n6\n6\n2\n7\n\n\nm13176\nqwen/qwen3-vl-8b-instruct\n9\n7\n4\n1\n\n\nm15298_1\ngoogle/gemini-2.5-flash-lite\n6\n1\n6\n8\n\n\nm15298_1\nmeta-llama/llama-4-maverick\n2\n11\n7\n1\n\n\nm15298_1\nopenai/gpt-4o-mini\n6\n2\n4\n9\n\n\nm15298_1\nqwen/qwen3-vl-8b-instruct\n7\n7\n4\n3\n\n\nm20435\ngoogle/gemini-2.5-flash-lite\n5\n5\n8\n3\n\n\nm20435\nmeta-llama/llama-4-maverick\n2\n6\n4\n9\n\n\nm20435\nopenai/gpt-4o-mini\n3\n4\n6\n8\n\n\nm20435\nqwen/qwen3-vl-8b-instruct\n11\n6\n3\n1\n\n\nm22924\ngoogle/gemini-2.5-flash-lite\n5\n3\n4\n9\n\n\nm22924\nmeta-llama/llama-4-maverick\n7\n3\n5\n6\n\n\nm22924\nopenai/gpt-4o-mini\n3\n9\n6\n3\n\n\nm22924\nqwen/qwen3-vl-8b-instruct\n6\n6\n6\n3\n\n\nm28635\ngoogle/gemini-2.5-flash-lite\n2\n6\n7\n6\n\n\nm28635\nmeta-llama/llama-4-maverick\n6\n9\n2\n4\n\n\nm28635\nopenai/gpt-4o-mini\n13\n3\n4\n1\n\n\nm28635\nqwen/qwen3-vl-8b-instruct\n0\n3\n8\n10\n\n\nm29084\ngoogle/gemini-2.5-flash-lite\n2\n9\n10\n0\n\n\nm29084\nmeta-llama/llama-4-maverick\n2\n2\n2\n15\n\n\nm29084\nopenai/gpt-4o-mini\n7\n5\n3\n6\n\n\nm29084\nqwen/qwen3-vl-8b-instruct\n10\n5\n6\n0\n\n\nm34620\ngoogle/gemini-2.5-flash-lite\n3\n8\n9\n1\n\n\nm34620\nmeta-llama/llama-4-maverick\n5\n5\n4\n7\n\n\nm34620\nopenai/gpt-4o-mini\n4\n5\n3\n9\n\n\nm34620\nqwen/qwen3-vl-8b-instruct\n9\n3\n5\n4\n\n\nm37030_1\ngoogle/gemini-2.5-flash-lite\n7\n6\n5\n3\n\n\nm37030_1\nmeta-llama/llama-4-maverick\n3\n7\n4\n7\n\n\nm37030_1\nopenai/gpt-4o-mini\n9\n7\n3\n2\n\n\nm37030_1\nqwen/qwen3-vl-8b-instruct\n2\n1\n9\n9\n\n\nm37716\ngoogle/gemini-2.5-flash-lite\n3\n2\n7\n9\n\n\nm37716\nmeta-llama/llama-4-maverick\n3\n4\n8\n6\n\n\nm37716\nopenai/gpt-4o-mini\n8\n8\n2\n3\n\n\nm37716\nqwen/qwen3-vl-8b-instruct\n7\n7\n4\n3\n\n\nm39198_1\ngoogle/gemini-2.5-flash-lite\n3\n2\n1\n15\n\n\nm39198_1\nmeta-llama/llama-4-maverick\n4\n5\n9\n3\n\n\nm39198_1\nopenai/gpt-4o-mini\n10\n4\n4\n3\n\n\nm39198_1\nqwen/qwen3-vl-8b-instruct\n4\n10\n7\n0\n\n\nm82972\ngoogle/gemini-2.5-flash-lite\n11\n5\n3\n2\n\n\nm82972\nmeta-llama/llama-4-maverick\n2\n9\n7\n3\n\n\nm82972\nopenai/gpt-4o-mini\n5\n5\n4\n7\n\n\nm82972\nqwen/qwen3-vl-8b-instruct\n3\n2\n7\n9\n\n\nm88415_1\ngoogle/gemini-2.5-flash-lite\n1\n3\n1\n16\n\n\nm88415_1\nmeta-llama/llama-4-maverick\n5\n9\n7\n0\n\n\nm88415_1\nopenai/gpt-4o-mini\n9\n3\n5\n4\n\n\nm88415_1\nqwen/qwen3-vl-8b-instruct\n6\n6\n8\n1\n\n\nm91000_1\ngoogle/gemini-2.5-flash-lite\n1\n3\n6\n11\n\n\nm91000_1\nmeta-llama/llama-4-maverick\n7\n7\n6\n1\n\n\nm91000_1\nopenai/gpt-4o-mini\n5\n6\n2\n8\n\n\nm91000_1\nqwen/qwen3-vl-8b-instruct\n8\n5\n7\n1\n\n\nm91960\ngoogle/gemini-2.5-flash-lite\n2\n4\n8\n7\n\n\nm91960\nmeta-llama/llama-4-maverick\n10\n6\n4\n1\n\n\nm91960\nopenai/gpt-4o-mini\n4\n1\n6\n10\n\n\nm91960\nqwen/qwen3-vl-8b-instruct\n5\n10\n3\n3\n\n\nm92357\ngoogle/gemini-2.5-flash-lite\n8\n3\n5\n5\n\n\nm92357\nmeta-llama/llama-4-maverick\n3\n11\n3\n4\n\n\nm92357\nopenai/gpt-4o-mini\n5\n4\n7\n5\n\n\nm92357\nqwen/qwen3-vl-8b-instruct\n5\n3\n6\n7\n\n\nm92410\ngoogle/gemini-2.5-flash-lite\n14\n0\n3\n4\n\n\nm92410\nmeta-llama/llama-4-maverick\n3\n6\n6\n6\n\n\nm92410\nopenai/gpt-4o-mini\n3\n11\n4\n3\n\n\nm92410\nqwen/qwen3-vl-8b-instruct\n1\n4\n8\n8\n\n\nm94271\ngoogle/gemini-2.5-flash-lite\n9\n3\n5\n4\n\n\nm94271\nmeta-llama/llama-4-maverick\n2\n3\n4\n12\n\n\nm94271\nopenai/gpt-4o-mini\n9\n4\n6\n2\n\n\nm94271\nqwen/qwen3-vl-8b-instruct\n1\n11\n6\n3\n\n\nm94775\ngoogle/gemini-2.5-flash-lite\n0\n2\n2\n17\n\n\nm94775\nmeta-llama/llama-4-maverick\n4\n5\n9\n3\n\n\nm94775\nopenai/gpt-4o-mini\n7\n5\n8\n1\n\n\nm94775\nqwen/qwen3-vl-8b-instruct\n10\n9\n2\n0\n\n\nm95804\ngoogle/gemini-2.5-flash-lite\n2\n6\n10\n3\n\n\nm95804\nmeta-llama/llama-4-maverick\n3\n2\n5\n11\n\n\nm95804\nopenai/gpt-4o-mini\n9\n4\n1\n7\n\n\nm95804\nqwen/qwen3-vl-8b-instruct\n7\n9\n5\n0\n\n\n\n\nRanked Friedman and Kendall’s W Test Summary\n\n\nStatistic\nValue\n\n\n\n\nFriedman χ²\n6.0191\n\n\np-value\n0.1107\n\n\nKendall’s W (observed)\n0.0085\n\n\nKendall’s W (from Friedman)\n0.1003\n\n\nNumber of tasks\n20\n\n\nNumber of unique raters\n21\n\n\nTotal submissions\n420",
    "crumbs": [
      "Home",
      "Research",
      "Paper"
    ]
  },
  {
    "objectID": "paper/index.html#sec-appendix-pairwise-comparison",
    "href": "paper/index.html#sec-appendix-pairwise-comparison",
    "title": "Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections",
    "section": "8.4 Pairwise Comparison of Models",
    "text": "8.4 Pairwise Comparison of Models\n\n\n\n\nPairwise Adjusted p-values (Holm) — Task-Level Inference\n\n\n\nPairwise Wilcoxon Signed-Rank Tests between Models with Holm Adjustment\n\n\n\n\n\n\n\n\n\nmodel_a\nmodel_b\nstatistic\npvalue\np_adjusted_holm\n\n\n\n\ngoogle/gemini-2.5-flash-lite\nmeta-llama/llama-4-maverick\n100.0\n0.864524\n1.0\n\n\ngoogle/gemini-2.5-flash-lite\nopenai/gpt-4o-mini\n59.5\n0.088574\n0.513834\n\n\ngoogle/gemini-2.5-flash-lite\nqwen/qwen3-vl-8b-instruct\n60.5\n0.095709\n0.513834\n\n\nmeta-llama/llama-4-maverick\nopenai/gpt-4o-mini\n60.0\n0.085639\n0.513834\n\n\nmeta-llama/llama-4-maverick\nqwen/qwen3-vl-8b-instruct\n61.5\n0.103028\n0.513834\n\n\nopenai/gpt-4o-mini\nqwen/qwen3-vl-8b-instruct\n105.0\n1.0\n1.0",
    "crumbs": [
      "Home",
      "Research",
      "Paper"
    ]
  },
  {
    "objectID": "paper/index.html#reproducibility-and-data-availability",
    "href": "paper/index.html#reproducibility-and-data-availability",
    "title": "Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections",
    "section": "8.5 Reproducibility and Data Availability",
    "text": "8.5 Reproducibility and Data Availability\n\nAll code, datasets, and analysis artefacts supporting this study are openly available under open licenses at:\nRepository: https://github.com/maehr/chr2025-seeing-history-unseen\nPersistent record: DOI 10.5281/zenodo.17639517\nThe repository provides a complete, executable research pipeline for the CHR 2025 paper “Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections.”\nKey components:\n\nsrc/ — source code for data generation, cleaning, and statistical analysis\nruns/ — timestamped outputs of alt-text generation runs, including raw API responses\ndata/processed/ — anonymised survey and ranking data used for evaluation\nanalysis/ — statistical summaries, CSVs, and figures referenced in this appendix\npaper/images/ — figure assets for the manuscript\n\nReference run: runs/20251021_233530/ — canonical example with subsample configuration (20 media objects × 4 models). All tables and plots in this appendix derive from this run and subsequent survey analyses.\nA pre-configured GitHub Codespace enables fully containerised reproduction without local setup. All scripts print output paths and runtime logs to ensure transparent traceability.",
    "crumbs": [
      "Home",
      "Research",
      "Paper"
    ]
  },
  {
    "objectID": "paper/index.html#fair-and-care-compliance",
    "href": "paper/index.html#fair-and-care-compliance",
    "title": "Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections",
    "section": "8.6 FAIR and CARE Compliance",
    "text": "8.6 FAIR and CARE Compliance\n\nThe project adheres to the FAIR (Findable, Accessible, Interoperable, Reusable) and CARE (Collective Benefit, Authority to Control, Responsibility, Ethics) principles for open humanities data.\n\n\n\n\n\n\n\nPrinciple\nImplementation in this project\n\n\n\n\nFindable (FAIR)\nRepository indexed on GitHub and Zenodo with persistent DOI; structured metadata and semantic filenames.\n\n\nAccessible\nPublicly accessible under AGPL-3.0 (code) and CC BY 4.0 (data, documentation). No authentication barriers.\n\n\nInteroperable\nMachine-readable CSV, JSONL, and Parquet formats; consistent column schemas; human- and machine-readable metadata.\n\n\nReusable\nVersion-controlled pipeline, deterministic random seeds, explicit dependencies, and complete provenance logs.\n\n\nCollective Benefit (CARE)\nFocus on accessibility and inclusion in digital heritage; results aim to improve equitable access to cultural data.\n\n\nAuthority to Control\nNo personal or culturally sensitive material; contributors retain authorship and citation credit.\n\n\nResponsibility\nTransparent methodological reporting and ethical safeguards for AI-assisted heritage interpretation.\n\n\nEthics\nEvaluation limited to non-personal, publicly available heritage materials; compliance with institutional research ethics guidelines.\n\n\n\nTogether, these practices ensure that the entire workflow—from model evaluation to figure generation—is transparent, reproducible, and reusable across digital humanities and accessibility research contexts.",
    "crumbs": [
      "Home",
      "Research",
      "Paper"
    ]
  },
  {
    "objectID": "paper/index.html#footnotes",
    "href": "paper/index.html#footnotes",
    "title": "Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCheaper models such as Mistral Pixtral 12B, AllenAI Molmo 7B-D, and OpenAI GPT-4.1 Nano were tested but excluded due to consistently empty or nonsensical outputs.↩︎",
    "crumbs": [
      "Home",
      "Research",
      "Paper"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.\n\n\n\nExamples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\n\n\n\nCommunity leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.\n\n\n\nThis Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.\n\n\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at moritz.maehr@gmail.com. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.\n\n\n\nCommunity leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.\n\n\n\n\nThis Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla’s code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.",
    "crumbs": [
      "Home",
      "Project",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-pledge",
    "href": "CODE_OF_CONDUCT.html#our-pledge",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.",
    "crumbs": [
      "Home",
      "Project",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-standards",
    "href": "CODE_OF_CONDUCT.html#our-standards",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Examples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting",
    "crumbs": [
      "Home",
      "Project",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "href": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.",
    "crumbs": [
      "Home",
      "Project",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#scope",
    "href": "CODE_OF_CONDUCT.html#scope",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.",
    "crumbs": [
      "Home",
      "Project",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement",
    "href": "CODE_OF_CONDUCT.html#enforcement",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at moritz.maehr@gmail.com. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.",
    "crumbs": [
      "Home",
      "Project",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "href": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.",
    "crumbs": [
      "Home",
      "Project",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#attribution",
    "href": "CODE_OF_CONDUCT.html#attribution",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla’s code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.",
    "crumbs": [
      "Home",
      "Project",
      "Code of Conduct"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CHR 2025 Conference Materials",
    "section": "",
    "text": "This repository contains the abstract and presentation materials for the CHR 2025 conference paper “Seeing History Unseen: Evaluating Vision-Language Models for WCAG-Compliant Alt-Text in Digital Heritage Collections” by Moritz Mähr (University of Bern and Basel) and Moritz Twente (University of Basel).\n    \n\n\n\nThis repository hosts the conference materials for CHR 2025 (Conference on Computational Humanities Research) including:\n\nAbstract: A LaTeX document containing the complete abstract for our paper\nPresentation Materials: Slides and supporting materials for the conference presentation\nDocumentation: Supporting documentation and setup instructions\n\n\n\n\nOur research explores the feasibility, accuracy, and ethics of using state-of-the-art vision-language models to generate WCAG-compliant alt-text for heterogeneous digital heritage collections. We combine computational experiments with qualitative evaluation to develop a framework for responsible AI-assisted accessibility in the humanities.\n\n\n\nFeasibility: Can current vision-language models produce useful, WCAG 2.2–compliant alt-text for complex historical images when provided with contextual metadata?\nQuality and Authenticity: How do domain experts rate AI-generated image descriptions in terms of factual accuracy, completeness, and usefulness for understanding historical content?\nEthics and Governance: What are the ethical implications of using AI to generate alt-text in heritage collections, and what human oversight or policy safeguards are required for responsible use?\n\n\n\n\n\n\nabstract/: Contains the LaTeX source, class files, and bibliography for the conference abstract\npresentation/: Will contain presentation slides and supporting materials\nsrc/: Alt-text generation and survey analysis pipeline\n\ngenerate_alt_text.py: Batch alt-text generator that writes timestamped outputs to runs/\nclean_survey_data.py: Removes excluded submissions and email addresses from raw Formspree exports\nprocess_survey_rankings.py: Expands cleaned submissions into per-object model rankings (survey_rankings.csv)\nprocess_best_answers.py: Aggregates consensus winners and texts per object (best_answers.csv)\nanalyze_survey_time.py: Summarises completion times across objects and raters\nranking_tests.py: Runs Friedman/Wilcoxon tests and produces comparison plots under analysis/\nviz_dataset.py: Creates figure assets for the manuscript (paper/images/fig_type_era_*.png)\nplayground.ipynb: Interactive Jupyter notebook for experimenting with the pipeline\n\nruns/: Output directory for generated alt-text results, including raw API responses and CSV/JSONL/Parquet tables\ndata/: Data directories for raw and cleaned datasets\n\n\n\n\nWe recommend using GitHub Codespaces for a reproducible setup.\n\n\n\n\n\n\nUse this template for your project in a new repository on your GitHub account.\n\n\n\nClick the green &lt;&gt; Code button at the top right of this repository.\nSelect the “Codespaces” tab and click “Create codespace on main”. GitHub will now build a container that includes:\n\n✅ Node.js (via npm)\n✅ Python with uv\n✅ R with renv\n✅ Quarto\n\n\n\n\nOnce the Codespace is ready, open a terminal and preview the documentation:\nuv run quarto preview\n\n\n\n\n\nNote: All dependencies (Node.js, Python, R, Quarto) are pre-installed in the Codespace.\n\n\n\n👩‍💻 Advanced Local Installation\n\n\n\n\nNode.js\nR and Rtools (on Windows)\nuv (Python manager)\nQuarto\n\n\nNote: uv installs and manages the correct Python version automatically.\n\n\n\n\n# 1. Install Node.js dependencies\nnpm install\nnpm run prepare\n\n# 2. Setup Python environment\nuv sync\n\n# 3. Setup R environment\nRscript -e 'install.packages(\"renv\"); renv::restore()'\n\n# 4. Preview documentation\nuv run quarto preview",
    "crumbs": [
      "Home",
      "Project",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#about-this-repository",
    "href": "index.html#about-this-repository",
    "title": "CHR 2025 Conference Materials",
    "section": "",
    "text": "This repository hosts the conference materials for CHR 2025 (Conference on Computational Humanities Research) including:\n\nAbstract: A LaTeX document containing the complete abstract for our paper\nPresentation Materials: Slides and supporting materials for the conference presentation\nDocumentation: Supporting documentation and setup instructions",
    "crumbs": [
      "Home",
      "Project",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#research-overview",
    "href": "index.html#research-overview",
    "title": "CHR 2025 Conference Materials",
    "section": "",
    "text": "Our research explores the feasibility, accuracy, and ethics of using state-of-the-art vision-language models to generate WCAG-compliant alt-text for heterogeneous digital heritage collections. We combine computational experiments with qualitative evaluation to develop a framework for responsible AI-assisted accessibility in the humanities.\n\n\n\nFeasibility: Can current vision-language models produce useful, WCAG 2.2–compliant alt-text for complex historical images when provided with contextual metadata?\nQuality and Authenticity: How do domain experts rate AI-generated image descriptions in terms of factual accuracy, completeness, and usefulness for understanding historical content?\nEthics and Governance: What are the ethical implications of using AI to generate alt-text in heritage collections, and what human oversight or policy safeguards are required for responsible use?",
    "crumbs": [
      "Home",
      "Project",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#repository-structure",
    "href": "index.html#repository-structure",
    "title": "CHR 2025 Conference Materials",
    "section": "",
    "text": "abstract/: Contains the LaTeX source, class files, and bibliography for the conference abstract\npresentation/: Will contain presentation slides and supporting materials\nsrc/: Alt-text generation and survey analysis pipeline\n\ngenerate_alt_text.py: Batch alt-text generator that writes timestamped outputs to runs/\nclean_survey_data.py: Removes excluded submissions and email addresses from raw Formspree exports\nprocess_survey_rankings.py: Expands cleaned submissions into per-object model rankings (survey_rankings.csv)\nprocess_best_answers.py: Aggregates consensus winners and texts per object (best_answers.csv)\nanalyze_survey_time.py: Summarises completion times across objects and raters\nranking_tests.py: Runs Friedman/Wilcoxon tests and produces comparison plots under analysis/\nviz_dataset.py: Creates figure assets for the manuscript (paper/images/fig_type_era_*.png)\nplayground.ipynb: Interactive Jupyter notebook for experimenting with the pipeline\n\nruns/: Output directory for generated alt-text results, including raw API responses and CSV/JSONL/Parquet tables\ndata/: Data directories for raw and cleaned datasets",
    "crumbs": [
      "Home",
      "Project",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "CHR 2025 Conference Materials",
    "section": "",
    "text": "We recommend using GitHub Codespaces for a reproducible setup.",
    "crumbs": [
      "Home",
      "Project",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "CHR 2025 Conference Materials",
    "section": "",
    "text": "Use this template for your project in a new repository on your GitHub account.\n\n\n\nClick the green &lt;&gt; Code button at the top right of this repository.\nSelect the “Codespaces” tab and click “Create codespace on main”. GitHub will now build a container that includes:\n\n✅ Node.js (via npm)\n✅ Python with uv\n✅ R with renv\n✅ Quarto\n\n\n\n\nOnce the Codespace is ready, open a terminal and preview the documentation:\nuv run quarto preview\n\n\n\n\n\nNote: All dependencies (Node.js, Python, R, Quarto) are pre-installed in the Codespace.\n\n\n\n👩‍💻 Advanced Local Installation\n\n\n\n\nNode.js\nR and Rtools (on Windows)\nuv (Python manager)\nQuarto\n\n\nNote: uv installs and manages the correct Python version automatically.\n\n\n\n\n# 1. Install Node.js dependencies\nnpm install\nnpm run prepare\n\n# 2. Setup Python environment\nuv sync\n\n# 3. Setup R environment\nRscript -e 'install.packages(\"renv\"); renv::restore()'\n\n# 4. Preview documentation\nuv run quarto preview",
    "crumbs": [
      "Home",
      "Project",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#use",
    "href": "index.html#use",
    "title": "CHR 2025 Conference Materials",
    "section": "Use",
    "text": "Use\n\nBuilding the Abstract\nTo build the LaTeX abstract:\ncd abstract\nmake paper\n\n\nDevelopment Commands\nCheck that all files are properly formatted:\nnpm run check\nFormat all files:\nnpm run format\nRun the wizard to write meaningful commit messages:\nnpm run commit\nGenerate a changelog:\nnpm run changelog\n\n\nAlt-Text Generation Pipeline\nThe repository includes a focused Python pipeline for generating WCAG-compliant alternative texts using OpenRouter-compatible vision-language models. The pipeline supports systematic evaluation of VLM performance on alt-text generation tasks for digital heritage collections.\n\nFeatures\n\nAutomated alt-text generation using multiple VLM models in parallel\nWCAG 2.2 compliance with structured prompts based on accessibility guidelines\nMetadata integration from remote sources with provenance tracking\nWide-format output with model responses in CSV, JSONL, and Parquet formats\nRaw API response storage for reproducibility and analysis\nInteractive playground via Jupyter notebook for experimentation\n\n\n\nQuick Start\n\nInstall Python dependencies with uv:\n\nuv sync\n\nSet up your OpenRouter API key in a .env file:\n\ncp example.env .env\n# Edit .env to add your OPENROUTER_API_KEY\n\nRun the alt-text generation pipeline:\n\nuv run python src/generate_alt_text.py\nThis will:\n\nFetch metadata from the configured URL\nGenerate alt-text for specified media IDs using all configured models\nSave results in runs/YYYYmmdd_HHMMSS/ including:\n\nmetadata.json: Copy of fetched metadata for provenance\nalt_text_runs_*.csv: Wide-format table with all model responses\nalt_text_runs_*.jsonl: Same data in JSONL format\nalt_text_runs_*.parquet: Same data in Parquet format (if available)\nraw/*.json: Individual raw API responses from each model\nmanifest.json: Run metadata including configuration and file paths\n\n\n\nExperiment interactively with the Jupyter notebook:\n\nuv run jupyter notebook src/playground.ipynb\n\n\nSurvey workflow\n\nGenerate or refresh survey/questions.csv from the latest run outputs and publish it in the Formspree survey.\nInvite human experts to complete the ranking survey—model comparison only works with real judgments.\nAfter submissions close, run the survey analysis scripts in sequence:\n\n# Clean and anonymise raw Formspree export\nuv run python src/clean_survey_data.py\n\n# Expand per-object rankings for each rater\nuv run python src/process_survey_rankings.py\n\n# Aggregate consensus winners and example texts\nuv run python src/process_best_answers.py\n\n# Summarise completion times for quality checks\nuv run python src/analyze_survey_time.py\n\n# Required: statistical tests, tables, and plots\nuv run python src/ranking_tests.py\nAll scripts write to data/processed/ and analysis/.\n\n\nConfiguration\nEdit src/generate_alt_text.py to customize:\n\nMODELS: List of OpenRouter model identifiers to use\nMEDIA_IDS: List of media object IDs to process\nMETADATA_URL: URL to fetch media metadata JSON\n\nCurrent models configured in generate_alt_text.py:\n\ngoogle/gemini-2.5-flash-lite\nqwen/qwen3-vl-8b-instruct\nopenai/gpt-4o-mini\nmeta-llama/llama-4-maverick\n\n\n\n\nScript workflow and artefacts\n\n\n\n\n\nflowchart TD\n  A[generate_alt_text.py&lt;br/&gt;Fetch metadata + call models] --&gt;|runs/&lt;timestamp&gt;/*| B{Survey prep}\n  B --&gt; C[clean_survey_data.py&lt;br/&gt;Sanitise Formspree export]\n  C --&gt; D[process_survey_rankings.py&lt;br/&gt;Expand per object + rater ranks]\n  D --&gt; E[process_best_answers.py&lt;br/&gt;Consensus winner per object]\n  D --&gt; F[analyze_survey_time.py&lt;br/&gt;Timing summaries]\n  D --&gt; G[ranking_tests.py&lt;br/&gt;Statistical tests + plots]\n  A --&gt; H[viz_dataset.py&lt;br/&gt;Paper figures]\n\n  E --&gt; I[analysis/&lt;br/&gt;CSVs + plots]\n  F --&gt; I\n  G --&gt; I\n  H --&gt; J[paper/images/fig_type_era_*.png]\n\n\n\n\n\n\n\n\nOutputs by directory\n\nruns/&lt;timestamp&gt;/ — generate_alt_text.py writes manifest.json, raw/*.json (per model × object), cached images/*.jpg, and timestamped tables (alt_text_runs_*_{wide,long}.csv|parquet|jsonl, optional prompts CSV).\ndata/raw/ — manual Formspree exports (e.g., formspree_*_export.json).\ndata/processed/ — clean_survey_data.py, process_survey_rankings.py, and process_best_answers.py materialise processed_survey_submissions.json, survey_rankings.csv, and best_answers.csv.\nanalysis/ — analyze_survey_time.py and ranking_tests.py produce time_stats_by_{object,submission}.csv, rank_counts_*.csv, statistical summaries, and comparison plots (rank_distributions_boxplot.png, pairwise_pvalues_heatmap.png, etc.).\npaper/images/ — viz_dataset.py renders figure assets such as fig_type_era_full.png and fig_type_era_subset.png.\n\nEach script prints the paths it writes; check those logs for exact filenames when running new experiments.\n\n\nReference run (2025-10-21 subsample)\nUse runs/20251021_233530/ as the canonical example of a recent full pipeline execution.\n\nConfiguration: mode=\"subsample\" across 20 media IDs and four models (google/gemini-2.5-flash-lite, qwen/qwen3-vl-8b-instruct, openai/gpt-4o-mini, meta-llama/llama-4-maverick).\nRuntime: 244 seconds wall time; no errors recorded in run.log.\nArtefacts:\n\nalt_text_runs_20251021_233933_wide.{csv,parquet} — pivoted responses (one row per media object with model-specific columns).\nalt_text_runs_20251021_233933_long.{csv,parquet,jsonl} — long format table with 80 model/object rows.\nalt_text_runs_20251021_233933_prompts.csv — per-item prompt, system, and image URL trace.\nraw/*.json — individual API responses (model × object).\nimages/*.jpg — thumbnails cached during the run.\nmanifest.json — reproducibility metadata (models, media IDs, durations, output pointers).\n\n\nMirror this structure when staging new runs for survey generation or reporting.",
    "crumbs": [
      "Home",
      "Project",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#support",
    "href": "index.html#support",
    "title": "CHR 2025 Conference Materials",
    "section": "Support",
    "text": "Support\nThis project is maintained by @maehr. Please understand that we can’t provide individual support via email. We also believe that help is much more valuable when it’s shared publicly, so more people can benefit from it.\n\n\n\nType\nPlatforms\n\n\n\n\n🚨 Bug Reports\nGitHub Issue Tracker\n\n\n📚 Docs Issue\nGitHub Issue Tracker\n\n\n🎁 Feature Requests\nGitHub Issue Tracker\n\n\n🛡 Report a security vulnerability\nSee SECURITY.md\n\n\n💬 General Questions\nGitHub Discussions",
    "crumbs": [
      "Home",
      "Project",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#roadmap",
    "href": "index.html#roadmap",
    "title": "CHR 2025 Conference Materials",
    "section": "Roadmap",
    "text": "Roadmap\n\nComplete the conference abstract and presentation preparation\nCreate presentation slides for CHR 2025\nFinalize user study design and implementation\nPublish dataset and benchmark for future research",
    "crumbs": [
      "Home",
      "Project",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "CHR 2025 Conference Materials",
    "section": "Contributing",
    "text": "Contributing\nPlease see CONTRIBUTING.md for details on our code of conduct and the process for submitting pull requests.",
    "crumbs": [
      "Home",
      "Project",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#authors-and-credits",
    "href": "index.html#authors-and-credits",
    "title": "CHR 2025 Conference Materials",
    "section": "Authors and credits",
    "text": "Authors and credits\n\nMoritz Mähr - University of Bern & Basel - maehr\nMoritz Twente - University of Basel - moritztwente\n\nSee also the list of contributors who contributed to this project.",
    "crumbs": [
      "Home",
      "Project",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "CHR 2025 Conference Materials",
    "section": "License",
    "text": "License\nThe abstract, presentation materials, and documentation in this repository are released under the Creative Commons Attribution 4.0 International (CC BY 4.0) License - see the LICENSE-CCBY file for details. By using these materials, you agree to give appropriate credit to the original author(s) and to indicate if any modifications have been made.\nAny code in this repository is released under the GNU Affero General Public License v3.0 - see the LICENSE-AGPL file for details.",
    "crumbs": [
      "Home",
      "Project",
      "Overview"
    ]
  },
  {
    "objectID": "AGENTS.html",
    "href": "AGENTS.html",
    "title": "Repository Guidelines",
    "section": "",
    "text": "Authoring sources live in:\n\npaper/ — manuscript (paper.md), wrapper (index.qmd), bibliography.bib, and images/\nsrc/ — Python alt-text pipeline (generate_alt_text.py) writing timestamped outputs to runs/\nabstract/, presentation/ — ancillary materials; published HTML in _site/\n\n\n\n\n\ngenerate_alt_text.py — fetch metadata, call configured models, and write timestamped runs under runs/&lt;timestamp&gt;/ (raw JSON, prompt tables, manifests, cached images).\nHuman experts complete the ranking survey (Formspree) using the generated prompts/questions.\nclean_survey_data.py — strip excluded submissions/emails from data/raw/formspree_*_export.json, saving data/processed/processed_survey_submissions.json.\nprocess_survey_rankings.py — expand per-object rankings into data/processed/survey_rankings.csv with elapsed time per rater.\nprocess_best_answers.py — derive consensus winners and export data/processed/best_answers.csv with exemplar texts.\nanalyze_survey_time.py — summarise completion durations and write analysis/time_stats_by_{object,submission}.csv.\nranking_tests.py — required step that runs Friedman/Wilcoxon tests, generates CSV summaries, and saves plots (boxplots, heatmaps) in analysis/.\nviz_dataset.py — build manuscript figures at paper/images/fig_type_era_{full,subset}.png.\n\nSee README.md for the mermaid workflow diagram and an artefact directory table.\n\n\n\n\nEdit paper/paper.md; paper/index.qmd includes it for rendering.\nPreview: quarto preview (live reload).\nCitations: add BibTeX to paper/bibliography.bib and cite with [@key]. Example: “…explicit detail [@cecilia2023b]”.\nFigures: place under paper/images/ and reference with labels. Example: ![Model comparison.](images/fig_models.png){#fig:models width=60%}.\nTables: use Markdown tables with captions. Example: Table: Model costs {#tbl:models} then reference as [Table @tbl:models].\nFrom TODO to paper: items under TODO.md → “Paper/Literature and Context” and “Methodology Updates” should become citations and a maintained @tbl:models in paper.md. Keep model IDs and costs aligned with src/generate_alt_text.py and recent runs.\n\n\n\n\n\nnpm install && uv sync — install Node and Python toolchains.\nquarto preview — preview the website; add paper/ to focus on the manuscript.\nuv run python src/generate_alt_text.py — execute the batch job (requires .env). [DO NOT EXECUTE UNLESS EXPLICITLY AUTHORIZED]\nnpm run check / npm run format — Prettier verify/fix.\nuv run ruff check / uv run ruff fix — Ruff verify/fix.\n\n\n\n\n\nPrettier for Markdown/Quarto/JSON; Ruff for Python (PEP 8, typed).\nUse snake_case filenames; figures fig_&lt;short-key&gt;.png; labels #fig:key, #tbl:key.\nCommit notebooks with cleared outputs (jupyter nbconvert --clear-output).\n\n\n\n\n\nUse npm run commit (Conventional Commits); reference issues with #NNN.\nPRs: describe scope, note affected sections of paper.md, and attach before/after renders when changing figures/tables. Ensure npm run check passes.\n\n\n\n\n\nReference run artifacts via relative paths (e.g., runs/2025.../manifest.json); avoid committing large intermediates.\nConfigure secrets in .env (e.g., OPENROUTER_API_KEY); never commit credentials."
  },
  {
    "objectID": "AGENTS.html#project-structure-module-organization",
    "href": "AGENTS.html#project-structure-module-organization",
    "title": "Repository Guidelines",
    "section": "",
    "text": "Authoring sources live in:\n\npaper/ — manuscript (paper.md), wrapper (index.qmd), bibliography.bib, and images/\nsrc/ — Python alt-text pipeline (generate_alt_text.py) writing timestamped outputs to runs/\nabstract/, presentation/ — ancillary materials; published HTML in _site/"
  },
  {
    "objectID": "AGENTS.html#pipeline-workflow-src",
    "href": "AGENTS.html#pipeline-workflow-src",
    "title": "Repository Guidelines",
    "section": "",
    "text": "generate_alt_text.py — fetch metadata, call configured models, and write timestamped runs under runs/&lt;timestamp&gt;/ (raw JSON, prompt tables, manifests, cached images).\nHuman experts complete the ranking survey (Formspree) using the generated prompts/questions.\nclean_survey_data.py — strip excluded submissions/emails from data/raw/formspree_*_export.json, saving data/processed/processed_survey_submissions.json.\nprocess_survey_rankings.py — expand per-object rankings into data/processed/survey_rankings.csv with elapsed time per rater.\nprocess_best_answers.py — derive consensus winners and export data/processed/best_answers.csv with exemplar texts.\nanalyze_survey_time.py — summarise completion durations and write analysis/time_stats_by_{object,submission}.csv.\nranking_tests.py — required step that runs Friedman/Wilcoxon tests, generates CSV summaries, and saves plots (boxplots, heatmaps) in analysis/.\nviz_dataset.py — build manuscript figures at paper/images/fig_type_era_{full,subset}.png.\n\nSee README.md for the mermaid workflow diagram and an artefact directory table."
  },
  {
    "objectID": "AGENTS.html#paper-authoring-workflow",
    "href": "AGENTS.html#paper-authoring-workflow",
    "title": "Repository Guidelines",
    "section": "",
    "text": "Edit paper/paper.md; paper/index.qmd includes it for rendering.\nPreview: quarto preview (live reload).\nCitations: add BibTeX to paper/bibliography.bib and cite with [@key]. Example: “…explicit detail [@cecilia2023b]”.\nFigures: place under paper/images/ and reference with labels. Example: ![Model comparison.](images/fig_models.png){#fig:models width=60%}.\nTables: use Markdown tables with captions. Example: Table: Model costs {#tbl:models} then reference as [Table @tbl:models].\nFrom TODO to paper: items under TODO.md → “Paper/Literature and Context” and “Methodology Updates” should become citations and a maintained @tbl:models in paper.md. Keep model IDs and costs aligned with src/generate_alt_text.py and recent runs."
  },
  {
    "objectID": "AGENTS.html#build-test-and-development-commands",
    "href": "AGENTS.html#build-test-and-development-commands",
    "title": "Repository Guidelines",
    "section": "",
    "text": "npm install && uv sync — install Node and Python toolchains.\nquarto preview — preview the website; add paper/ to focus on the manuscript.\nuv run python src/generate_alt_text.py — execute the batch job (requires .env). [DO NOT EXECUTE UNLESS EXPLICITLY AUTHORIZED]\nnpm run check / npm run format — Prettier verify/fix.\nuv run ruff check / uv run ruff fix — Ruff verify/fix."
  },
  {
    "objectID": "AGENTS.html#coding-style-naming-conventions",
    "href": "AGENTS.html#coding-style-naming-conventions",
    "title": "Repository Guidelines",
    "section": "",
    "text": "Prettier for Markdown/Quarto/JSON; Ruff for Python (PEP 8, typed).\nUse snake_case filenames; figures fig_&lt;short-key&gt;.png; labels #fig:key, #tbl:key.\nCommit notebooks with cleared outputs (jupyter nbconvert --clear-output)."
  },
  {
    "objectID": "AGENTS.html#commit-pull-request-guidelines",
    "href": "AGENTS.html#commit-pull-request-guidelines",
    "title": "Repository Guidelines",
    "section": "",
    "text": "Use npm run commit (Conventional Commits); reference issues with #NNN.\nPRs: describe scope, note affected sections of paper.md, and attach before/after renders when changing figures/tables. Ensure npm run check passes."
  },
  {
    "objectID": "AGENTS.html#reproducibility-security",
    "href": "AGENTS.html#reproducibility-security",
    "title": "Repository Guidelines",
    "section": "",
    "text": "Reference run artifacts via relative paths (e.g., runs/2025.../manifest.json); avoid committing large intermediates.\nConfigure secrets in .env (e.g., OPENROUTER_API_KEY); never commit credentials."
  },
  {
    "objectID": "survey/index.html",
    "href": "survey/index.html",
    "title": "CHR 2025 - Seeing History Unseen",
    "section": "",
    "text": "WarningSurvey Closed\n\n\n\nThank you for your interest! The survey period has ended, and we are no longer accepting responses. We appreciate everyone who took the time to participate.\n\n\n\n\n\n    \n        \n            Studie: Vergleich von automatisch erzeugten Alternativtexten\n\n            Liebe Teilnehmerin, lieber Teilnehmer,\n            \n                im Rahmen des Projekts Stadt.Geschichte.Basel stellen wir historische Quellen und\n                Forschungsdaten Open Access zur Verfügung. Ziel ist es, diese digitale Infrastruktur\n                möglichst barrierearm zu gestalten, damit auch sehbehinderte und blinde Personen Zugang zu\n                den visuellen Beständen erhalten.\n            \n            \n                Das Verfassen von Alternativtexten (Alt-Texten) für Bilder ist jedoch sehr aufwendig. Da uns\n                die personellen Mittel für eine vollständige manuelle Erschliessung fehlen, untersuchen wir\n                in dieser Vorstudie, ob multimodale KI-Modelle (Vision-Language Models) bei dieser Aufgabe\n                unterstützen können.\n            \n\n            Ziel der Studie\n            \n                Wir testen, wie gut verschiedene KI-Systeme kurze Beschreibungen von historischen Objekten\n                erzeugen, die als barrierefreie Alternativtexte verwendet werden können. Dazu wurden 20\n                Bilder ausgewählt und mit ihren Metadaten an vier verschiedene Modelle übergeben. Die\n                Auswahl bildet die unterschiedlichen in der Sammlung von Stadt.Geschichte.Basel abgedeckten\n                Epochen und Medientypen (Fotografien, Karten, Drucke, Objekte etc.) repräsentativ ab.\n            \n            \n                Ihre Einschätzung hilft uns zu ermitteln, welches Modell sich am besten für den Einsatz in\n                der historischen Forschung eignet.\n            \n            \n                Weitere Fragen der Studie sind, welche Kosten damit verbunden sind, und wie algorithmische\n                Verfahren verantwortungsvoll in geisteswissenschaftliche Arbeitsprozesse integriert werden\n                können. Die Ergebnisse werden auf der Computational Humanities Research Conference (CHR\n                2025) vom 9.–12. Dezember 2025 am Luxembourg Centre for Contemporary and Digital History\n                (C²DH) vorgestellt.\n            \n\n            \n\n            Ablauf und Bewertungskriterien\n            \n                Die Umfrage umfasst 20 Objekte und dauert etwa 10 bis 15 Minuten (ca. 30\n                bis 45 Sekunden pro Bild).\n            \n            \n                Auf jeder Seite sehen Sie ein Sammlungsbild und vier kurze Beschreibungen. Ordnen Sie die\n                Beschreibungen von beste Beschreibung nach\n                schlechteste Beschreibung\n                per Drag & Drop. Alle Texte wurden automatisch generiert und können Fehler enthalten.\n            \n\n            \n                Bewertungsregeln (nach WCAG-Intention):\n                \n                    den Kerninhalt sachlich und knapp wiedergeben,\n                    keine redundanten Angaben wie „Bild von …“ enthalten,\n                    \n                        wesentliche visuelle Merkmale (z. B. Personen, Objekte, Handlungen, Schrift)\n                        priorisieren,\n                    \n                    und nur dann Kontext liefern, wenn er zum Verständnis beiträgt.\n                    \n                        Weiterführende Links:\n                        7 Regeln für gute Alt-Texte\n                        (nur lesen, falls Unklarheiten bestehen)\n                    \n                \n                \n                    Tastatur: ↑/↓ verschiebt fokussierten Eintrag; 1–4 setzt direkt auf\n                    Position; Tab für Fokus.\n                \n                \n                    Fokus-Indikator sichtbar. Elemente sind per ARIA ausgezeichnet.\n                \n            \n            \n                Wir danken Ihnen herzlich für Ihre Zeit und Ihre fachliche Einschätzung. Sie leisten damit\n                einen wichtigen Beitrag zu inklusiver, nachhaltiger und kritischer digitaler\n                Geschichtsforschung.\n            \n            \n                Mit freundlichen Grüssen,\n                Dr. Moritz Mähr und Moritz Twente\n                Universität Bern / Universität Basel\n                    Kontakt: moritz.maehr@unibas.ch /\n                    moritz.twente@unibas.ch\n            \n\n            \n\n            Einwilligung und Start\n            \n                Ihre Teilnahme ist freiwillig. Die personenbezogene Angabe (E-Mail-Adresse) dient\n                ausschliesslich möglichen Rückfragen und wird nicht in die Forschungsdatenbank aufgenommen.\n            \n\n            \n                \n                    E-Mail*\n                    (nur für eventuelle Rückfragen)\n                \n                \n            \n\n            \n                \n                \n                    Ich willige in die Verarbeitung meiner Daten für diese Studie ein.\n                \n            \n\n            \n                Hinweis: Einige Objekte enthalten veraltete, vorurteilsbeladene oder diskriminierende\n                Darstellungen. Als historische Quellen werden diese Inhalte unverändert präsentiert.\n            \n\n            Start\n            \n        \n    \n\n    \n        \n            \n\n            \n                \n                    \n                    \n                \n\n                \n                    \n                    \n                    \n                        Verschiebe Elemente per Drag und Drop oder nutze Pfeiltasten. Beste Beschreibung nach\n                        oben.\n                    \n                    \n\n                    \n                        \n                            Zurück\n                        \n                        \n                            Weiter\n                        \n                    \n                \n            \n        \n    \n\n    \n        \n            Abschluss\n            Übermittlung…\n            Antworten senden\n            \n        \n    \n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Data & Survey",
      "Survey"
    ]
  }
]